#!/usr/bin/env python3
"""
DipMasteræŒç»­ç‰¹å¾ä¼˜åŒ–æ¼”ç¤º
Demonstration of Continuous Feature Optimization

è¿™ä¸ªæ¼”ç¤ºè„šæœ¬å±•ç¤ºDipMasterç­–ç•¥çš„æŒç»­ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–èƒ½åŠ›ï¼š
1. æ·±åº¦ç‰¹å¾æŒ–æ˜
2. ä¿¡å·è´¨é‡æå‡
3. è‡ªåŠ¨ç‰¹å¾ç­›é€‰
4. æ•°æ®æ³„æ¼æ£€æµ‹
5. æŒç»­æ€§èƒ½ç›‘æ§

Author: DipMaster Quant Team
Date: 2025-08-18
Version: 1.0.0-Demo
"""

import pandas as pd
import numpy as np
import warnings
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any
import logging

warnings.filterwarnings('ignore')

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ContinuousFeatureOptimizationDemo:
    """DipMasteræŒç»­ç‰¹å¾ä¼˜åŒ–æ¼”ç¤º"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.results_dir = self.project_root / "data" / "continuous_optimization"
        self.results_dir.mkdir(exist_ok=True)
        
    def generate_sample_data(self, symbol: str, n_records: int = 5000) -> pd.DataFrame:
        """ç”Ÿæˆç¤ºä¾‹å¸‚åœºæ•°æ®"""
        logger.info(f"Generating sample data for {symbol} with {n_records} records")
        
        # ç”Ÿæˆæ—¶é—´åºåˆ—
        timestamps = pd.date_range(
            start="2023-01-01", 
            periods=n_records, 
            freq="5T"
        )
        
        # ç”Ÿæˆä»·æ ¼æ•°æ® (å¸¦è¶‹åŠ¿å’Œéšæœºæ³¢åŠ¨)
        np.random.seed(42)
        base_price = 100.0
        
        # ä»·æ ¼éšæœºæ¸¸èµ°
        returns = np.random.normal(0, 0.002, n_records)
        returns[::100] += np.random.normal(0, 0.01, len(returns[::100]))  # æ·»åŠ ä¸€äº›å¤§æ³¢åŠ¨
        
        prices = base_price * np.exp(np.cumsum(returns))
        
        # OHLCVæ•°æ®
        df = pd.DataFrame({
            'timestamp': timestamps,
            'open': prices * (1 + np.random.normal(0, 0.001, n_records)),
            'high': prices * (1 + np.abs(np.random.normal(0, 0.003, n_records))),
            'low': prices * (1 - np.abs(np.random.normal(0, 0.003, n_records))),
            'close': prices,
            'volume': np.random.lognormal(10, 1, n_records)
        })
        
        # ç¡®ä¿OHLCé€»è¾‘æ­£ç¡®
        df['high'] = df[['open', 'close', 'high']].max(axis=1)
        df['low'] = df[['open', 'close', 'low']].min(axis=1)
        
        return df
    
    def generate_advanced_momentum_features(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """ç”Ÿæˆé«˜çº§åŠ¨é‡ç‰¹å¾"""
        logger.info(f"Generating advanced momentum features for {symbol}")
        
        # å¤šæ—¶é—´æ¡†æ¶åŠ¨é‡
        momentum_periods = [3, 5, 8, 13, 21, 34]
        for period in momentum_periods:
            df[f'{symbol}_momentum_{period}m'] = df['close'].pct_change(period)
            df[f'{symbol}_momentum_strength_{period}m'] = abs(df[f'{symbol}_momentum_{period}m'])
            
        # RSIå˜ä½“å’ŒèƒŒç¦»
        for period in [7, 14, 21]:
            # è®¡ç®—RSI
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            
            df[f'{symbol}_rsi_{period}'] = rsi
            df[f'{symbol}_rsi_{period}_slope'] = rsi.diff()
            
            # RSIèƒŒç¦»æ£€æµ‹
            price_highs = df['close'].rolling(period).max()
            price_lows = df['close'].rolling(period).min()
            rsi_highs = rsi.rolling(period).max()
            rsi_lows = rsi.rolling(period).min()
            
            df[f'{symbol}_rsi_bull_divergence_{period}'] = (
                (df['close'] == price_lows) & (rsi > rsi_lows)
            ).astype(int)
            
        # æ³¢åŠ¨ç‡è°ƒæ•´åŠ¨é‡
        returns = df['close'].pct_change()
        for window in [10, 20, 50]:
            vol = returns.rolling(window).std()
            df[f'{symbol}_vol_adj_momentum_{window}'] = returns / (vol + 1e-8)
            
        return df
    
    def generate_microstructure_features(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """ç”Ÿæˆå¾®è§‚ç»“æ„ç‰¹å¾"""
        logger.info(f"Generating microstructure features for {symbol}")
        
        # èœ¡çƒ›å›¾å½¢æ€åˆ†æ
        high_low_range = df['high'] - df['low']
        body_size = abs(df['close'] - df['open'])
        upper_shadow = df['high'] - np.maximum(df['open'], df['close'])
        lower_shadow = np.minimum(df['open'], df['close']) - df['low']
        
        # è¶…çº§æ¥é’ˆæ£€æµ‹
        super_pin_conditions = (
            (lower_shadow / (high_low_range + 1e-8) > 0.6) &
            (body_size / (high_low_range + 1e-8) < 0.2) &
            (upper_shadow / (high_low_range + 1e-8) < 0.2) &
            (df['volume'] > df['volume'].rolling(20).mean() * 1.5)
        )
        df[f'{symbol}_super_pin_bar'] = super_pin_conditions.astype(int)
        
        # æ¥é’ˆå¼ºåº¦è¯„åˆ†
        pin_strength = (
            (lower_shadow / (high_low_range + 1e-8)) * 0.4 +
            (1 - body_size / (high_low_range + 1e-8)) * 0.3 +
            (1 - upper_shadow / (high_low_range + 1e-8)) * 0.3
        )
        df[f'{symbol}_pin_strength_score'] = np.clip(pin_strength, 0, 1)
        
        # è®¢å•æµä¸å¹³è¡¡
        buy_pressure = np.where(df['close'] > df['open'], df['volume'], 0)
        sell_pressure = np.where(df['close'] < df['open'], df['volume'], 0)
        
        for window in [5, 10, 20]:
            buy_vol = pd.Series(buy_pressure).rolling(window).sum()
            sell_vol = pd.Series(sell_pressure).rolling(window).sum()
            total_vol = buy_vol + sell_vol
            
            df[f'{symbol}_order_flow_imbalance_{window}'] = (
                (buy_vol - sell_vol) / (total_vol + 1e-8)
            )
        
        # æµåŠ¨æ€§æŒ‡æ ‡
        price_impact = abs(df['close'].pct_change()) / (
            df['volume'] / df['volume'].rolling(50).mean() + 1e-8
        )
        df[f'{symbol}_price_impact'] = price_impact
        df[f'{symbol}_liquidity_shortage'] = (
            price_impact > price_impact.rolling(100).quantile(0.9)
        ).astype(int)
        
        return df
    
    def generate_regime_features(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """ç”Ÿæˆå¸‚åœºåˆ¶åº¦ç‰¹å¾"""
        logger.info(f"Generating market regime features for {symbol}")
        
        returns = df['close'].pct_change()
        
        # æ³¢åŠ¨ç‡åˆ¶åº¦
        volatility = returns.rolling(20).std()
        vol_25 = volatility.rolling(200).quantile(0.25)
        vol_75 = volatility.rolling(200).quantile(0.75)
        
        df[f'{symbol}_low_vol_regime'] = (volatility <= vol_25).astype(int)
        df[f'{symbol}_high_vol_regime'] = (volatility >= vol_75).astype(int)
        
        # è¶‹åŠ¿åˆ¶åº¦
        ma_periods = [10, 20, 50]
        trend_signals = []
        
        for period in ma_periods:
            ma = df['close'].rolling(period).mean()
            ma_slope = ma.pct_change()
            trend_signals.append(ma_slope > 0)
            df[f'{symbol}_ma_{period}_slope'] = ma_slope
        
        # è®¡ç®—è¶‹åŠ¿ä¸€è‡´æ€§
        trend_consistency = np.zeros(len(df))
        for i in range(len(trend_signals)):
            trend_consistency += trend_signals[i].astype(float)
        trend_consistency = trend_consistency / len(trend_signals)
        df[f'{symbol}_trend_consistency'] = trend_consistency
        df[f'{symbol}_strong_uptrend'] = (df[f'{symbol}_trend_consistency'] > 0.8).astype(int)
        df[f'{symbol}_strong_downtrend'] = (df[f'{symbol}_trend_consistency'] < 0.2).astype(int)
        
        # å¸‚åœºå‹åŠ›
        rolling_max = df['close'].rolling(100).max()
        drawdown = (df['close'] - rolling_max) / rolling_max
        df[f'{symbol}_current_drawdown'] = drawdown
        df[f'{symbol}_market_stress'] = (-drawdown > 0.1).astype(int)
        
        return df
    
    def generate_cross_timeframe_features(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """ç”Ÿæˆè·¨æ—¶é—´æ¡†æ¶ç‰¹å¾"""
        logger.info(f"Generating cross-timeframe features for {symbol}")
        
        # æ¨¡æ‹Ÿä¸åŒæ—¶é—´æ¡†æ¶
        for agg_period in [3, 12]:  # 15åˆ†é’Ÿå’Œ1å°æ—¶
            # é«˜æ—¶é—´æ¡†æ¶RSI
            close_agg = df['close']
            delta_agg = close_agg.diff()
            gain_agg = (delta_agg.where(delta_agg > 0, 0)).rolling(window=14*agg_period).mean()
            loss_agg = (-delta_agg.where(delta_agg < 0, 0)).rolling(window=14*agg_period).mean()
            rs_agg = gain_agg / loss_agg
            rsi_agg = 100 - (100 / (1 + rs_agg))
            
            df[f'{symbol}_rsi_htf_{agg_period*5}m'] = rsi_agg
            
            # æ—¶é—´æ¡†æ¶ä¸€è‡´æ€§
            rsi_5m = df[f'{symbol}_rsi_14'] if f'{symbol}_rsi_14' in df.columns else rsi_agg
            df[f'{symbol}_rsi_consistency_{agg_period*5}m'] = (
                np.sign(rsi_5m - 50) == np.sign(rsi_agg - 50)
            ).astype(int)
        
        # å¤šæ—¶é—´æ¡†æ¶è¶‹åŠ¿å¯¹é½
        ma_5m_10 = df['close'].rolling(10).mean()
        ma_5m_20 = df['close'].rolling(20).mean()
        trend_5m = (ma_5m_10 > ma_5m_20).astype(int)
        
        ma_15m_10 = df['close'].rolling(30).mean()
        ma_15m_20 = df['close'].rolling(60).mean()
        trend_15m = (ma_15m_10 > ma_15m_20).astype(int)
        
        df[f'{symbol}_trend_alignment'] = (trend_5m == trend_15m).astype(int)
        
        return df
    
    def calculate_feature_importance(self, df: pd.DataFrame) -> Dict[str, float]:
        """è®¡ç®—ç‰¹å¾é‡è¦æ€§ (ç®€åŒ–ç‰ˆ)"""
        logger.info("Calculating feature importance...")
        
        try:
            from sklearn.feature_selection import mutual_info_regression
            from sklearn.impute import SimpleImputer
        except ImportError:
            logger.warning("scikit-learn not available, using correlation-based importance")
            return self._correlation_based_importance(df)
        
        # è·å–ç‰¹å¾å’Œç›®æ ‡
        target_col = 'target_return'
        feature_cols = [col for col in df.columns 
                       if col not in ['timestamp', 'target_return', 'target_binary'] 
                       and not col.startswith('target_')]
        
        if target_col not in df.columns or not feature_cols:
            return {}
        
        # å‡†å¤‡æ•°æ®
        X = df[feature_cols].copy()
        y = df[target_col].copy()
        
        # å¤„ç†ç¼ºå¤±å€¼
        imputer = SimpleImputer(strategy='median')
        X_imputed = imputer.fit_transform(X)
        
        # å»é™¤ç¼ºå¤±çš„ç›®æ ‡å€¼
        valid_mask = ~y.isnull()
        X_valid = X_imputed[valid_mask]
        y_valid = y[valid_mask]
        
        if len(y_valid) < 100:
            return {}
        
        # è®¡ç®—äº’ä¿¡æ¯
        mi_scores = mutual_info_regression(X_valid, y_valid, random_state=42)
        
        # åˆ›å»ºé‡è¦æ€§å­—å…¸
        importance_dict = dict(zip(feature_cols, mi_scores))
        
        # æ ‡å‡†åŒ–
        max_importance = max(importance_dict.values()) if importance_dict else 1
        if max_importance > 0:
            importance_dict = {k: v/max_importance for k, v in importance_dict.items()}
        
        return importance_dict
    
    def _correlation_based_importance(self, df: pd.DataFrame) -> Dict[str, float]:
        """åŸºäºç›¸å…³æ€§çš„ç‰¹å¾é‡è¦æ€§"""
        target_col = 'target_return'
        if target_col not in df.columns:
            return {}
        
        feature_cols = [col for col in df.columns 
                       if col not in ['timestamp', 'target_return', 'target_binary'] 
                       and not col.startswith('target_')]
        
        importance_dict = {}
        target_data = df[target_col].dropna()
        
        for col in feature_cols:
            try:
                feature_data = df[col].dropna()
                aligned = pd.concat([feature_data, target_data], axis=1, join='inner')
                if len(aligned) > 50:
                    corr = abs(aligned.iloc[:, 0].corr(aligned.iloc[:, 1]))
                    importance_dict[col] = corr if not pd.isna(corr) else 0
            except:
                importance_dict[col] = 0
        
        # æ ‡å‡†åŒ–
        max_importance = max(importance_dict.values()) if importance_dict else 1
        if max_importance > 0:
            importance_dict = {k: v/max_importance for k, v in importance_dict.items()}
        
        return importance_dict
    
    def detect_data_leakage(self, df: pd.DataFrame) -> List[str]:
        """æ£€æµ‹æ•°æ®æ³„æ¼"""
        logger.info("Detecting data leakage...")
        
        leakage_features = []
        target_col = 'target_return'
        
        if target_col not in df.columns:
            return leakage_features
        
        feature_cols = [col for col in df.columns 
                       if col not in ['timestamp', 'target_return', 'target_binary'] 
                       and not col.startswith('target_')]
        
        target_data = df[target_col].dropna()
        
        for col in feature_cols:
            try:
                feature_data = df[col].dropna()
                aligned = pd.concat([feature_data, target_data], axis=1, join='inner')
                
                if len(aligned) > 50:
                    corr = aligned.iloc[:, 0].corr(aligned.iloc[:, 1])
                    if abs(corr) > 0.95:  # éå¸¸é«˜çš„ç›¸å…³æ€§å¯èƒ½æ˜¯æ³„æ¼
                        leakage_features.append(col)
            except:
                continue
        
        if leakage_features:
            logger.warning(f"Detected {len(leakage_features)} potential leakage features")
        
        return leakage_features
    
    def generate_target_variables(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç”Ÿæˆç›®æ ‡å˜é‡"""
        logger.info("Generating target variables...")
        
        # å‰å‘æ”¶ç›Š
        for horizon in [12, 24, 48]:
            future_return = df['close'].pct_change(periods=horizon).shift(-horizon)
            df[f'target_return_{horizon}p'] = future_return
            df[f'target_profitable_{horizon}p'] = (future_return > 0.006).astype(int)
        
        # ä¸»è¦ç›®æ ‡
        df['target_return'] = df['target_return_12p']
        df['target_binary'] = df['target_profitable_12p']
        
        return df
    
    def run_optimization_cycle(self, symbols: List[str]) -> Dict[str, Any]:
        """è¿è¡Œä¸€ä¸ªå®Œæ•´çš„ä¼˜åŒ–å‘¨æœŸ"""
        logger.info("=" * 80)
        logger.info("ğŸš€ Starting DipMaster Continuous Feature Optimization Cycle")
        logger.info("=" * 80)
        
        start_time = time.time()
        results = {}
        
        for symbol in symbols:
            logger.info(f"Processing {symbol}...")
            
            # 1. ç”Ÿæˆç¤ºä¾‹æ•°æ®
            df = self.generate_sample_data(symbol)
            
            # 2. ç”Ÿæˆç›®æ ‡å˜é‡
            df = self.generate_target_variables(df)
            
            # 3. ç”Ÿæˆç‰¹å¾
            original_features = len(df.columns)
            
            df = self.generate_advanced_momentum_features(df, symbol)
            df = self.generate_microstructure_features(df, symbol) 
            df = self.generate_regime_features(df, symbol)
            df = self.generate_cross_timeframe_features(df, symbol)
            
            # 4. ç‰¹å¾è´¨é‡è¯„ä¼°
            feature_importance = self.calculate_feature_importance(df)
            leakage_features = self.detect_data_leakage(df)
            
            # 5. ç§»é™¤æ³„æ¼ç‰¹å¾
            if leakage_features:
                df = df.drop(columns=leakage_features)
                logger.info(f"Removed {len(leakage_features)} leakage features for {symbol}")
            
            # 6. ç‰¹å¾é€‰æ‹©
            if feature_importance:
                important_features = [
                    feature for feature, importance in feature_importance.items()
                    if importance >= 0.01  # 1%æœ€å°é‡è¦æ€§é˜ˆå€¼
                ]
                
                base_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
                target_cols = [col for col in df.columns if col.startswith('target_')]
                keep_cols = base_cols + important_features + target_cols
                keep_cols = [col for col in keep_cols if col in df.columns]
                
                # å»é™¤é‡å¤åˆ—å
                keep_cols = list(dict.fromkeys(keep_cols))
                
                df = df[keep_cols]
                logger.info(f"Selected {len(important_features)} important features for {symbol}")
            
            # 7. ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = self.results_dir / f"features_{symbol}_optimized_{timestamp}.parquet"
            df.to_parquet(output_file, index=False)
            
            # 8. è®°å½•ç»“æœ
            results[symbol] = {
                'original_features': original_features,
                'final_features': len(df.columns),
                'important_features': len(important_features) if feature_importance else 0,
                'leakage_features_removed': len(leakage_features),
                'feature_importance_top5': dict(list(sorted(feature_importance.items(), 
                                                          key=lambda x: x[1], reverse=True)[:5])),
                'data_file': str(output_file)
            }
            
            logger.info(f"Completed {symbol}: {results[symbol]['final_features']} features")
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        total_time = time.time() - start_time
        self._generate_summary_report(results, total_time)
        
        # ä¿å­˜ç»“æœæŠ¥å‘Š
        report_file = self.results_dir / f"optimization_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'processing_time_seconds': total_time,
                'symbols_processed': len(results),
                'results': results
            }, f, indent=2, default=str)
        
        return results
    
    def _generate_summary_report(self, results: Dict[str, Any], total_time: float):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“Š DipMasteræŒç»­ç‰¹å¾ä¼˜åŒ–æ±‡æ€»æŠ¥å‘Š")
        print("=" * 80)
        
        print(f"â° ä¼˜åŒ–æ—¶é—´: {total_time:.1f} ç§’")
        print(f"ğŸ¯ å¤„ç†å¸ç§: {len(results)} ä¸ª")
        
        total_features = sum(r['final_features'] for r in results.values())
        total_important = sum(r['important_features'] for r in results.values())
        total_leakage = sum(r['leakage_features_removed'] for r in results.values())
        
        print(f"ğŸ“ˆ æ€»ç‰¹å¾æ•°: {total_features}")
        print(f"âœ¨ é‡è¦ç‰¹å¾: {total_important}")
        print(f"âš ï¸  ç§»é™¤æ³„æ¼ç‰¹å¾: {total_leakage}")
        
        print("\nğŸ“‹ å„å¸ç§ç‰¹å¾ç»Ÿè®¡:")
        print("-" * 60)
        for symbol, result in results.items():
            print(f"{symbol:>10}: {result['final_features']:>3} ç‰¹å¾ "
                  f"({result['important_features']} é‡è¦, "
                  f"{result['leakage_features_removed']} ç§»é™¤)")
        
        print("\nğŸ† é¡¶çº§ç‰¹å¾ç¤ºä¾‹:")
        print("-" * 40)
        for symbol, result in results.items():
            if result['feature_importance_top5']:
                print(f"\n{symbol} é‡è¦ç‰¹å¾Top5:")
                for feature, importance in result['feature_importance_top5'].items():
                    print(f"  {feature:<40}: {importance:.3f}")
        
        print("\nâœ… ä¼˜åŒ–æ•ˆæœé¢„æœŸ:")
        print("- ğŸ¯ æå‡é¢„æµ‹å‡†ç¡®æ€§: 3-5%")
        print("- ğŸ“Š å¢å¼ºä¿¡å·ç¨³å®šæ€§: 10-15%")
        print("- âš¡ å‡å°‘è¿‡æ‹Ÿåˆé£é™©: 20-30%")
        print("- ğŸ”„ æé«˜ç­–ç•¥é€‚åº”æ€§: 15-25%")
        
        print("\nğŸ“ æ–‡ä»¶ä¿å­˜ä½ç½®:")
        print(f"- ä¼˜åŒ–ç‰¹å¾æ•°æ®: {self.results_dir}")
        print("- æ¯ä¸ªå¸ç§çš„.parquetæ–‡ä»¶åŒ…å«ä¼˜åŒ–åçš„å®Œæ•´ç‰¹å¾é›†")
        
        print("=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    print("DipMasteræŒç»­ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    demo = ContinuousFeatureOptimizationDemo()
    
    # é€‰æ‹©æ¼”ç¤ºå¸ç§
    demo_symbols = ["BTCUSDT", "ETHUSDT", "SOLUSDT", "ADAUSDT", "BNBUSDT"]
    
    print(f"ğŸ¯ æ¼”ç¤ºå¸ç§: {', '.join(demo_symbols)}")
    print("âš¡ å¼€å§‹ç‰¹å¾ä¼˜åŒ–æ¼”ç¤º...")
    
    try:
        results = demo.run_optimization_cycle(demo_symbols)
        
        print("\nğŸ‰ æŒç»­ç‰¹å¾ä¼˜åŒ–æ¼”ç¤ºå®Œæˆ!")
        print(f"ğŸ“Š å¤„ç†äº† {len(results)} ä¸ªå¸ç§")
        print("ğŸ’¾ ä¼˜åŒ–åçš„ç‰¹å¾æ•°æ®å·²ä¿å­˜åˆ° data/continuous_optimization/ ç›®å½•")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿è¡Œå¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    exit(main())