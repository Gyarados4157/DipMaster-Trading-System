#!/usr/bin/env python3
"""
DipMasteræŒç»­æ¨¡å‹è®­ç»ƒä¸»ç¨‹åº
æ•´åˆæ‰€æœ‰ä¼˜åŒ–ç»„ä»¶ï¼Œå®ç°æŒç»­è®­ç»ƒå’ŒéªŒè¯å¾ªç¯
ç›®æ ‡ï¼šè¾¾åˆ°èƒœç‡85%+, å¤æ™®æ¯”ç‡>1.5, æœ€å¤§å›æ’¤<3%, å¹´åŒ–æ”¶ç›Š>15%
"""

import os
import sys
import time
import json
import asyncio
import numpy as np
import pandas as pd
import warnings
import schedule
from datetime import datetime, timedelta
from pathlib import Path
import logging
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from ml.continuous_training_system import ContinuousTrainingSystem, TrainingConfig
from validation.enhanced_time_series_validator import EnhancedTimeSeriesValidator, ValidationConfig
from core.signal_optimization_engine import SignalOptimizationEngine, SignalConfig
from ml.realistic_backtester import RealisticBacktester, TradingCosts, BacktestConfig

warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('continuous_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ContinuousModelTrainingOrchestrator:
    """æŒç»­æ¨¡å‹è®­ç»ƒç¼–æ’å™¨"""
    
    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–ç¼–æ’å™¨"""
        self.config = self._load_config(config_path)
        self.is_running = False
        self.iteration_count = 0
        self.best_performance = {}
        self.performance_history = []
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("results/continuous_training")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç›®æ ‡æ€§èƒ½æŒ‡æ ‡
        self.target_metrics = {
            'win_rate': 0.85,      # 85%èƒœç‡
            'sharpe_ratio': 1.5,   # å¤æ™®æ¯”ç‡>1.5
            'max_drawdown': 0.03,  # æœ€å¤§å›æ’¤<3%
            'annual_return': 0.15  # å¹´åŒ–æ”¶ç›Š>15%
        }
        
        logger.info("ğŸš€ DipMasteræŒç»­è®­ç»ƒç¼–æ’å™¨å·²åˆå§‹åŒ–")
    
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "training_interval_hours": 2,
            "data_dir": "data/continuous_optimization",
            "max_iterations": 100,
            "early_stopping_patience": 10,
            "performance_threshold": {
                "min_improvement": 0.01,
                "lookback_iterations": 5
            }
        }
        
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r') as f:
                file_config = json.load(f)
                default_config.update(file_config)
        
        return default_config
    
    def _initialize_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        # è®­ç»ƒç³»ç»Ÿé…ç½®
        training_config = TrainingConfig(
            target_win_rate=self.target_metrics['win_rate'],
            target_sharpe=self.target_metrics['sharpe_ratio'],
            target_max_drawdown=self.target_metrics['max_drawdown'],
            target_annual_return=self.target_metrics['annual_return']
        )
        
        # éªŒè¯ç³»ç»Ÿé…ç½®
        validation_config = ValidationConfig(
            n_splits=5,
            embargo_hours=2,
            walk_forward_steps=10
        )
        
        # ä¿¡å·ä¼˜åŒ–é…ç½®
        signal_config = SignalConfig(
            base_threshold=0.5,
            min_signal_strength=0.6,
            max_daily_signals=10
        )
        
        # å›æµ‹é…ç½®
        costs = TradingCosts(
            maker_fee=0.0010,
            taker_fee=0.0010,
            slippage_base=0.0005
        )
        backtest_config = BacktestConfig(
            initial_capital=10000,
            max_position_size=0.1,
            stop_loss_ratio=0.05,
            take_profit_ratio=0.02
        )
        
        # åˆ›å»ºç»„ä»¶å®ä¾‹
        self.training_system = ContinuousTrainingSystem(training_config)
        self.validator = EnhancedTimeSeriesValidator(validation_config)
        self.signal_optimizer = SignalOptimizationEngine(signal_config)
        self.backtester = RealisticBacktester(costs, backtest_config)
        
        logger.info("âœ… æ‰€æœ‰ç»„ä»¶å·²åˆå§‹åŒ–")
    
    def run_single_iteration(self) -> Dict:
        """è¿è¡Œå•æ¬¡è®­ç»ƒè¿­ä»£"""
        iteration_start_time = datetime.now()
        self.iteration_count += 1
        
        logger.info(f"\n{'='*50}")
        logger.info(f"ğŸ”„ å¼€å§‹ç¬¬ {self.iteration_count} æ¬¡è®­ç»ƒè¿­ä»£")
        logger.info(f"{'='*50}")
        
        try:
            # 1. åŠ è½½æœ€æ–°æ•°æ®
            logger.info("ğŸ“Š åŠ è½½æœ€æ–°ç‰¹å¾æ•°æ®...")
            datasets = self.training_system.load_multi_symbol_data(
                self.config["data_dir"]
            )
            
            if not datasets:
                logger.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
                return {'success': False, 'error': 'No data available'}
            
            # 2. å¯¹æ¯ä¸ªå¸ç§è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯
            symbol_results = {}
            
            for symbol, data in datasets.items():
                logger.info(f"\nğŸª™ å¤„ç† {symbol}...")
                
                try:
                    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
                    X, y_return, y_binary = self.training_system.prepare_features_and_labels(data)
                    
                    if len(X) < 1000:
                        logger.warning(f"âš ï¸ {symbol} æ•°æ®ä¸è¶³ ({len(X)} æ ·æœ¬)")
                        continue
                    
                    # 2.1 è®­ç»ƒé›†æˆæ¨¡å‹
                    logger.info(f"ğŸ¤– è®­ç»ƒ {symbol} é›†æˆæ¨¡å‹...")
                    training_result = self.training_system.train_ensemble_model(X, y_return, y_binary)
                    
                    # 2.2 å¢å¼ºéªŒè¯
                    logger.info(f"ğŸ” æ‰§è¡Œ {symbol} å¢å¼ºéªŒè¯...")
                    
                    # åˆ›å»ºæ¨¡å‹å·¥å‚å‡½æ•°
                    def create_model_factory():
                        import lightgbm as lgb
                        return lambda: lgb.LGBMClassifier(
                            objective='binary',
                            metric='binary_logloss',
                            boosting_type='gbdt',
                            num_leaves=31,
                            learning_rate=0.05,
                            verbose=-1
                        )
                    
                    model_factory = create_model_factory()
                    validation_result = self.validator.comprehensive_validation(
                        X, y_binary, model_factory
                    )
                    
                    # 2.3 ä¿¡å·ä¼˜åŒ–
                    logger.info(f"âš¡ ä¼˜åŒ– {symbol} ä¿¡å·...")
                    
                    predictions = training_result['test_data']['predictions']
                    market_data = data.copy()
                    
                    optimized_signals = self.signal_optimizer.generate_optimized_signals(
                        predictions, X, market_data
                    )
                    
                    # 2.4 ç°å®åŒ–å›æµ‹
                    logger.info(f"ğŸ“ˆ æ‰§è¡Œ {symbol} ç°å®åŒ–å›æµ‹...")
                    
                    if optimized_signals:
                        # åˆ›å»ºä¿¡å·DataFrame
                        signals_df = pd.DataFrame(optimized_signals)
                        
                        # å‡†å¤‡å¸‚åœºæ•°æ®
                        market_data_dict = {symbol: market_data}
                        
                        # è¿è¡Œå›æµ‹
                        backtest_result = self.backtester.run_backtest(
                            signals_df, market_data_dict
                        )
                    else:
                        backtest_result = {'error': 'No signals generated'}
                    
                    # 2.5 è¯„ä¼°æ€§èƒ½
                    symbol_performance = self._evaluate_symbol_performance(
                        symbol, training_result, validation_result, backtest_result
                    )
                    
                    symbol_results[symbol] = {
                        'training_result': training_result,
                        'validation_result': validation_result,
                        'backtest_result': backtest_result,
                        'performance': symbol_performance,
                        'signals_count': len(optimized_signals) if optimized_signals else 0
                    }
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                    if symbol_performance.get('targets_achieved', False):
                        logger.info(f"ğŸ‰ {symbol} è¾¾åˆ°æ‰€æœ‰ç›®æ ‡æŒ‡æ ‡!")
                        self._save_champion_model(symbol, symbol_results[symbol])
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç† {symbol} æ—¶å‡ºé”™: {e}")
                    continue
            
            # 3. ç”Ÿæˆè¿­ä»£æ€»ç»“
            iteration_summary = self._generate_iteration_summary(
                symbol_results, iteration_start_time
            )
            
            # 4. ä¿å­˜ç»“æœ
            self._save_iteration_results(iteration_summary)
            
            # 5. æ£€æŸ¥å…¨å±€ç›®æ ‡è¾¾æˆ
            if self._check_global_targets_achieved(symbol_results):
                logger.info("ğŸ† å…¨å±€ç›®æ ‡è¾¾æˆ!")
                iteration_summary['global_success'] = True
                return iteration_summary
            
            logger.info(f"âœ… ç¬¬ {self.iteration_count} æ¬¡è¿­ä»£å®Œæˆ")
            return iteration_summary
            
        except Exception as e:
            logger.error(f"âŒ è¿­ä»£ {self.iteration_count} å¤±è´¥: {e}")
            return {
                'success': False, 
                'error': str(e),
                'iteration': self.iteration_count
            }
    
    def _evaluate_symbol_performance(self, symbol: str, training_result: Dict, 
                                   validation_result: Dict, backtest_result: Dict) -> Dict:
        """è¯„ä¼°å¸ç§æ€§èƒ½"""
        
        performance = {
            'symbol': symbol,
            'timestamp': datetime.now().isoformat(),
            'targets_achieved': False
        }
        
        # ä»å›æµ‹ç»“æœä¸­æå–æ€§èƒ½æŒ‡æ ‡
        if 'performance_metrics' in backtest_result and 'error' not in backtest_result:
            metrics = backtest_result['performance_metrics']
            
            performance.update({
                'win_rate': metrics.get('win_rate', 0),
                'sharpe_ratio': metrics.get('sharpe_ratio', 0),
                'max_drawdown': abs(metrics.get('max_drawdown', 0)),
                'annual_return': metrics.get('annual_return', 0),
                'total_trades': metrics.get('total_trades', 0),
                'profit_factor': metrics.get('profit_factor', 0),
                'total_return': metrics.get('total_return', 0)
            })
            
            # æ£€æŸ¥ç›®æ ‡è¾¾æˆæƒ…å†µ
            targets_met = {
                'win_rate': performance['win_rate'] >= self.target_metrics['win_rate'],
                'sharpe_ratio': performance['sharpe_ratio'] >= self.target_metrics['sharpe_ratio'],
                'max_drawdown': performance['max_drawdown'] <= self.target_metrics['max_drawdown'],
                'annual_return': performance['annual_return'] >= self.target_metrics['annual_return']
            }
            
            performance['targets_met'] = targets_met
            performance['targets_achieved'] = all(targets_met.values())
            
        else:
            performance['error'] = backtest_result.get('error', 'Unknown backtest error')
        
        # æ·»åŠ éªŒè¯ç»“æœ
        if 'error' not in validation_result:
            performance['validation_stable'] = validation_result.get(
                'stability_analysis', {}
            ).get('overall_stability_score', 0) > 0.7
            
        return performance
    
    def _generate_iteration_summary(self, symbol_results: Dict, start_time: datetime) -> Dict:
        """ç”Ÿæˆè¿­ä»£æ€»ç»“"""
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        summary = {
            'iteration': self.iteration_count,
            'timestamp': datetime.now().isoformat(),
            'processing_time_seconds': processing_time,
            'symbols_processed': len(symbol_results),
            'results': symbol_results,
            'performance_summary': {},
            'targets_achieved_count': 0
        }
        
        # ç»Ÿè®¡æ€§èƒ½
        if symbol_results:
            performances = []
            targets_achieved = 0
            
            for symbol, result in symbol_results.items():
                perf = result.get('performance', {})
                if 'error' not in perf:
                    performances.append(perf)
                    if perf.get('targets_achieved', False):
                        targets_achieved += 1
            
            if performances:
                summary['performance_summary'] = {
                    'avg_win_rate': np.mean([p.get('win_rate', 0) for p in performances]),
                    'avg_sharpe_ratio': np.mean([p.get('sharpe_ratio', 0) for p in performances]),
                    'avg_max_drawdown': np.mean([p.get('max_drawdown', 0) for p in performances]),
                    'avg_annual_return': np.mean([p.get('annual_return', 0) for p in performances]),
                    'best_performer': max(performances, key=lambda x: x.get('sharpe_ratio', 0))['symbol'] if performances else None
                }
                
            summary['targets_achieved_count'] = targets_achieved
        
        # æ›´æ–°æ€§èƒ½å†å²
        self.performance_history.append(summary)
        
        return summary
    
    def _save_champion_model(self, symbol: str, result: Dict):
        """ä¿å­˜å† å†›æ¨¡å‹"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        champion_dir = self.output_dir / f"champion_models/{symbol}_{timestamp}"
        champion_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        if 'training_result' in result and 'models' in result['training_result']:
            import joblib
            
            models = result['training_result']['models']
            for model_name, model in models.items():
                model_path = champion_dir / f"{model_name}_model.pkl"
                joblib.dump(model, model_path)
        
        # ä¿å­˜ç»“æœæ‘˜è¦
        summary_path = champion_dir / "champion_summary.json"
        with open(summary_path, 'w') as f:
            # æ¸…ç†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
            clean_result = self._clean_for_json(result)
            json.dump(clean_result, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ† å† å†›æ¨¡å‹å·²ä¿å­˜: {champion_dir}")
    
    def _save_iteration_results(self, summary: Dict):
        """ä¿å­˜è¿­ä»£ç»“æœ"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_path = self.output_dir / f"iteration_{self.iteration_count}_{timestamp}.json"
        
        with open(results_path, 'w') as f:
            clean_summary = self._clean_for_json(summary)
            json.dump(clean_summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ è¿­ä»£ç»“æœå·²ä¿å­˜: {results_path}")
    
    def _clean_for_json(self, obj):
        """æ¸…ç†å¯¹è±¡ä»¥ä¾¿JSONåºåˆ—åŒ–"""
        if isinstance(obj, dict):
            return {k: self._clean_for_json(v) for k, v in obj.items() 
                   if k not in ['models', 'scalers', 'test_data', 'fold_results']}
        elif isinstance(obj, list):
            return [self._clean_for_json(item) for item in obj]
        elif isinstance(obj, (np.ndarray, pd.Series, pd.DataFrame)):
            return f"<{type(obj).__name__} shape={getattr(obj, 'shape', len(obj))}>"
        elif isinstance(obj, (np.int64, np.float64, np.int32, np.float32)):
            return obj.item()
        elif hasattr(obj, '__dict__'):
            return str(obj)
        else:
            return obj
    
    def _check_global_targets_achieved(self, symbol_results: Dict) -> bool:
        """æ£€æŸ¥å…¨å±€ç›®æ ‡æ˜¯å¦è¾¾æˆ"""
        
        targets_achieved_count = 0
        total_symbols = len(symbol_results)
        
        for symbol, result in symbol_results.items():
            perf = result.get('performance', {})
            if perf.get('targets_achieved', False):
                targets_achieved_count += 1
        
        # å¦‚æœè‡³å°‘50%çš„å¸ç§è¾¾åˆ°ç›®æ ‡ï¼Œæˆ–æœ‰ä»»ä½•å¸ç§è¡¨ç°ä¼˜å¼‚
        min_required = max(1, total_symbols // 2)
        
        if targets_achieved_count >= min_required:
            return True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¡¨ç°ç‰¹åˆ«ä¼˜å¼‚çš„å¸ç§
        for symbol, result in symbol_results.items():
            perf = result.get('performance', {})
            if (perf.get('win_rate', 0) > 0.9 and 
                perf.get('sharpe_ratio', 0) > 2.0):
                logger.info(f"ğŸŒŸ å‘ç°è¶…çº§è¡¨ç°è€…: {symbol}")
                return True
        
        return False
    
    def run_continuous_loop(self):
        """è¿è¡ŒæŒç»­è®­ç»ƒå¾ªç¯"""
        logger.info("ğŸš€ å¯åŠ¨æŒç»­è®­ç»ƒå¾ªç¯...")
        logger.info(f"ğŸ¯ ç›®æ ‡æŒ‡æ ‡: èƒœç‡â‰¥{self.target_metrics['win_rate']:.0%}, "
                   f"å¤æ™®â‰¥{self.target_metrics['sharpe_ratio']}, "
                   f"å›æ’¤â‰¤{self.target_metrics['max_drawdown']:.0%}, "
                   f"å¹´åŒ–â‰¥{self.target_metrics['annual_return']:.0%}")
        
        self.is_running = True
        consecutive_failures = 0
        max_failures = 5
        
        try:
            while self.is_running:
                
                # æ£€æŸ¥æœ€å¤§è¿­ä»£æ•°
                if self.iteration_count >= self.config.get("max_iterations", 100):
                    logger.info("ğŸ”š è¾¾åˆ°æœ€å¤§è¿­ä»£æ•°ï¼Œåœæ­¢è®­ç»ƒ")
                    break
                
                # è¿è¡Œå•æ¬¡è¿­ä»£
                result = self.run_single_iteration()
                
                if result.get('success', True):
                    consecutive_failures = 0
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å…¨å±€ç›®æ ‡
                    if result.get('global_success', False):
                        logger.info("ğŸ‰ è¾¾åˆ°å…¨å±€ç›®æ ‡ï¼Œè®­ç»ƒæˆåŠŸå®Œæˆ!")
                        self._generate_final_report()
                        break
                    
                    # æ£€æŸ¥æ—©åœæ¡ä»¶
                    if self._should_early_stop():
                        logger.info("â¹ï¸ æ»¡è¶³æ—©åœæ¡ä»¶ï¼Œåœæ­¢è®­ç»ƒ")
                        break
                    
                else:
                    consecutive_failures += 1
                    logger.warning(f"âš ï¸ è¿ç»­å¤±è´¥æ¬¡æ•°: {consecutive_failures}/{max_failures}")
                    
                    if consecutive_failures >= max_failures:
                        logger.error("âŒ è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œåœæ­¢è®­ç»ƒ")
                        break
                
                # ç­‰å¾…ä¸‹æ¬¡è¿­ä»£
                wait_hours = self.config.get("training_interval_hours", 2)
                logger.info(f"â° ç­‰å¾… {wait_hours} å°æ—¶åç»§ç»­ä¸‹æ¬¡è¿­ä»£...")
                
                # ä½¿ç”¨éé˜»å¡ç­‰å¾…ï¼Œä»¥ä¾¿å¯ä»¥ä¼˜é›…åœæ­¢
                for _ in range(wait_hours * 60):  # åˆ†é’Ÿçº§æ£€æŸ¥
                    if not self.is_running:
                        break
                    time.sleep(60)  # ç­‰å¾…1åˆ†é’Ÿ
        
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¼˜é›…åœæ­¢...")
        except Exception as e:
            logger.error(f"âŒ æŒç»­è®­ç»ƒå¾ªç¯å‡ºé”™: {e}")
        finally:
            self.is_running = False
            logger.info("ğŸ“‹ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
            self._generate_final_report()
    
    def _should_early_stop(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ"""
        
        patience = self.config.get("early_stopping_patience", 10)
        min_improvement = self.config.get("performance_threshold", {}).get("min_improvement", 0.01)
        
        if len(self.performance_history) < patience:
            return False
        
        # æ£€æŸ¥æœ€è¿‘å‡ æ¬¡è¿­ä»£æ˜¯å¦æœ‰æ”¹å–„
        recent_performances = self.performance_history[-patience:]
        
        # ä½¿ç”¨å¹³å‡å¤æ™®æ¯”ç‡ä½œä¸ºä¸»è¦æŒ‡æ ‡
        recent_sharpe_ratios = [
            p.get('performance_summary', {}).get('avg_sharpe_ratio', 0)
            for p in recent_performances
        ]
        
        if not recent_sharpe_ratios:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æŒç»­æ”¹å–„
        best_recent = max(recent_sharpe_ratios)
        if len(self.performance_history) > patience:
            historical_best = max(
                p.get('performance_summary', {}).get('avg_sharpe_ratio', 0)
                for p in self.performance_history[:-patience]
            )
            
            improvement = best_recent - historical_best
            if improvement < min_improvement:
                logger.info(f"ğŸ“‰ æ€§èƒ½æ”¹å–„ä¸è¶³: {improvement:.4f} < {min_improvement}")
                return True
        
        return False
    
    def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆè®­ç»ƒæŠ¥å‘Š"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_path = self.output_dir / f"final_training_report_{timestamp}.json"
        
        # æŸ¥æ‰¾æ‰€æœ‰å† å†›æ¨¡å‹
        champion_models_dir = self.output_dir / "champion_models"
        champion_models = []
        
        if champion_models_dir.exists():
            for champion_dir in champion_models_dir.iterdir():
                if champion_dir.is_dir():
                    summary_file = champion_dir / "champion_summary.json"
                    if summary_file.exists():
                        try:
                            with open(summary_file, 'r') as f:
                                champion_data = json.load(f)
                                champion_data['model_path'] = str(champion_dir)
                                champion_models.append(champion_data)
                        except Exception as e:
                            logger.warning(f"æ— æ³•è¯»å–å† å†›æ¨¡å‹æ‘˜è¦: {e}")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        final_report = {
            'training_summary': {
                'start_time': self.performance_history[0]['timestamp'] if self.performance_history else None,
                'end_time': datetime.now().isoformat(),
                'total_iterations': self.iteration_count,
                'champion_models_found': len(champion_models),
                'target_metrics': self.target_metrics
            },
            'champion_models': champion_models,
            'performance_evolution': [
                {
                    'iteration': p['iteration'],
                    'timestamp': p['timestamp'],
                    'avg_performance': p.get('performance_summary', {}),
                    'targets_achieved_count': p.get('targets_achieved_count', 0)
                }
                for p in self.performance_history
            ],
            'final_recommendations': self._generate_final_recommendations(champion_models)
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(report_path, 'w') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_path = self._generate_html_report(final_report, timestamp)
        
        logger.info(f"ğŸ“Š æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ:")
        logger.info(f"  JSON: {report_path}")
        logger.info(f"  HTML: {html_path}")
        
        return final_report
    
    def _generate_final_recommendations(self, champion_models: List[Dict]) -> List[str]:
        """ç”Ÿæˆæœ€ç»ˆå»ºè®®"""
        recommendations = []
        
        if not champion_models:
            recommendations.append("âŒ æœªæ‰¾åˆ°è¾¾æ ‡çš„å† å†›æ¨¡å‹ï¼Œå»ºè®®ï¼š")
            recommendations.append("  1. å¢åŠ è®­ç»ƒæ•°æ®é‡æˆ–æ”¹å–„æ•°æ®è´¨é‡")
            recommendations.append("  2. è°ƒæ•´ç‰¹å¾å·¥ç¨‹ç­–ç•¥")
            recommendations.append("  3. å°è¯•ä¸åŒçš„æ¨¡å‹æ¶æ„")
            recommendations.append("  4. ä¼˜åŒ–é£é™©ç®¡ç†å‚æ•°")
        else:
            recommendations.append(f"âœ… å‘ç° {len(champion_models)} ä¸ªå† å†›æ¨¡å‹:")
            
            # åˆ†ææœ€ä½³è¡¨ç°è€…
            best_model = max(champion_models, 
                           key=lambda x: x.get('performance', {}).get('sharpe_ratio', 0))
            
            recommendations.append(f"ğŸ† æœ€ä½³æ¨¡å‹è¡¨ç°:")
            perf = best_model.get('performance', {})
            recommendations.append(f"  - èƒœç‡: {perf.get('win_rate', 0):.1%}")
            recommendations.append(f"  - å¤æ™®æ¯”ç‡: {perf.get('sharpe_ratio', 0):.2f}")
            recommendations.append(f"  - æœ€å¤§å›æ’¤: {perf.get('max_drawdown', 0):.1%}")
            recommendations.append(f"  - å¹´åŒ–æ”¶ç›Š: {perf.get('annual_return', 0):.1%}")
            
            recommendations.append("ğŸ’¡ éƒ¨ç½²å»ºè®®:")
            recommendations.append("  1. ä½¿ç”¨å† å†›æ¨¡å‹è¿›è¡Œçº¸é¢äº¤æ˜“éªŒè¯")
            recommendations.append("  2. å®æ–½ä¸¥æ ¼çš„é£é™©æ§åˆ¶")
            recommendations.append("  3. ç›‘æ§æ¨¡å‹è¡¨ç°å¹¶å®šæœŸé‡è®­ç»ƒ")
            recommendations.append("  4. è€ƒè™‘å¤šæ¨¡å‹é›†æˆç­–ç•¥")
        
        return recommendations
    
    def _generate_html_report(self, report_data: Dict, timestamp: str) -> str:
        """ç”ŸæˆHTMLæ ¼å¼çš„æœ€ç»ˆæŠ¥å‘Š"""
        
        html_path = self.output_dir / f"final_training_report_{timestamp}.html"
        
        # åˆ›å»ºæ€§èƒ½æ¼”åŒ–å›¾è¡¨æ•°æ®
        performance_data = report_data.get('performance_evolution', [])
        iterations = [p['iteration'] for p in performance_data]
        avg_win_rates = [p.get('avg_performance', {}).get('avg_win_rate', 0) for p in performance_data]
        avg_sharpe_ratios = [p.get('avg_performance', {}).get('avg_sharpe_ratio', 0) for p in performance_data]
        
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DipMasteræŒç»­è®­ç»ƒæœ€ç»ˆæŠ¥å‘Š</title>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            text-align: center;
            margin-bottom: 30px;
        }}
        .summary-cards {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .card {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        .card h3 {{
            margin-top: 0;
            color: #2c3e50;
        }}
        .metric {{
            display: flex;
            justify-content: space-between;
            margin: 10px 0;
        }}
        .metric-label {{
            font-weight: 600;
        }}
        .metric-value {{
            color: #27ae60;
        }}
        .chart-container {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin-bottom: 30px;
        }}
        .champion-models {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin-bottom: 30px;
        }}
        .champion-model {{
            border-left: 4px solid #27ae60;
            padding: 15px;
            margin: 15px 0;
            background: #f8fff8;
        }}
        .recommendations {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        .recommendations ul {{
            list-style-type: none;
            padding-left: 0;
        }}
        .recommendations li {{
            margin: 8px 0;
            padding: 5px 0;
        }}
        .success {{ color: #27ae60; }}
        .warning {{ color: #f39c12; }}
        .error {{ color: #e74c3c; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ¯ DipMasteræŒç»­è®­ç»ƒæœ€ç»ˆæŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="summary-cards">
        <div class="card">
            <h3>ğŸ“Š è®­ç»ƒç»Ÿè®¡</h3>
            <div class="metric">
                <span class="metric-label">æ€»è¿­ä»£æ¬¡æ•°:</span>
                <span class="metric-value">{report_data['training_summary']['total_iterations']}</span>
            </div>
            <div class="metric">
                <span class="metric-label">å† å†›æ¨¡å‹æ•°:</span>
                <span class="metric-value">{report_data['training_summary']['champion_models_found']}</span>
            </div>
            <div class="metric">
                <span class="metric-label">è®­ç»ƒçŠ¶æ€:</span>
                <span class="metric-value {'success' if report_data['training_summary']['champion_models_found'] > 0 else 'warning'}">
                    {'âœ… æˆåŠŸ' if report_data['training_summary']['champion_models_found'] > 0 else 'âš ï¸ éƒ¨åˆ†æˆåŠŸ'}
                </span>
            </div>
        </div>
        
        <div class="card">
            <h3>ğŸ¯ ç›®æ ‡æŒ‡æ ‡</h3>
            <div class="metric">
                <span class="metric-label">ç›®æ ‡èƒœç‡:</span>
                <span class="metric-value">{report_data['training_summary']['target_metrics']['win_rate']:.0%}</span>
            </div>
            <div class="metric">
                <span class="metric-label">ç›®æ ‡å¤æ™®æ¯”ç‡:</span>
                <span class="metric-value">{report_data['training_summary']['target_metrics']['sharpe_ratio']}</span>
            </div>
            <div class="metric">
                <span class="metric-label">æœ€å¤§å›æ’¤é™åˆ¶:</span>
                <span class="metric-value">{report_data['training_summary']['target_metrics']['max_drawdown']:.0%}</span>
            </div>
            <div class="metric">
                <span class="metric-label">ç›®æ ‡å¹´åŒ–æ”¶ç›Š:</span>
                <span class="metric-value">{report_data['training_summary']['target_metrics']['annual_return']:.0%}</span>
            </div>
        </div>
    </div>
    
    <div class="chart-container">
        <h3>ğŸ“ˆ æ€§èƒ½æ¼”åŒ–è¶‹åŠ¿</h3>
        <div id="performanceChart"></div>
    </div>
    
    <div class="champion-models">
        <h3>ğŸ† å† å†›æ¨¡å‹åˆ—è¡¨</h3>
        {self._generate_champion_models_html(report_data.get('champion_models', []))}
    </div>
    
    <div class="recommendations">
        <h3>ğŸ’¡ æœ€ç»ˆå»ºè®®</h3>
        <ul>
            {''.join(f'<li>{rec}</li>' for rec in report_data.get('final_recommendations', []))}
        </ul>
    </div>
    
    <script>
        // åˆ›å»ºæ€§èƒ½æ¼”åŒ–å›¾è¡¨
        var trace1 = {{
            x: {iterations},
            y: {avg_win_rates},
            type: 'scatter',
            mode: 'lines+markers',
            name: 'å¹³å‡èƒœç‡',
            yaxis: 'y'
        }};
        
        var trace2 = {{
            x: {iterations},
            y: {avg_sharpe_ratios},
            type: 'scatter',
            mode: 'lines+markers',
            name: 'å¹³å‡å¤æ™®æ¯”ç‡',
            yaxis: 'y2'
        }};
        
        var layout = {{
            title: 'è®­ç»ƒè¿­ä»£æ€§èƒ½è¶‹åŠ¿',
            xaxis: {{ title: 'è¿­ä»£æ¬¡æ•°' }},
            yaxis: {{ 
                title: 'èƒœç‡',
                side: 'left'
            }},
            yaxis2: {{
                title: 'å¤æ™®æ¯”ç‡',
                side: 'right',
                overlaying: 'y'
            }}
        }};
        
        Plotly.newPlot('performanceChart', [trace1, trace2], layout);
    </script>
</body>
</html>
        """
        
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return str(html_path)
    
    def _generate_champion_models_html(self, champion_models: List[Dict]) -> str:
        """ç”Ÿæˆå† å†›æ¨¡å‹HTML"""
        if not champion_models:
            return "<p>æš‚æ— å† å†›æ¨¡å‹è¾¾åˆ°ç›®æ ‡æŒ‡æ ‡ã€‚</p>"
        
        html = ""
        for i, model in enumerate(champion_models):
            perf = model.get('performance', {})
            
            html += f"""
            <div class="champion-model">
                <h4>ğŸ¥‡ å† å†›æ¨¡å‹ #{i+1}: {perf.get('symbol', 'Unknown')}</h4>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 15px;">
                    <div class="metric">
                        <span class="metric-label">èƒœç‡:</span>
                        <span class="metric-value">{perf.get('win_rate', 0):.1%}</span>
                    </div>
                    <div class="metric">
                        <span class="metric-label">å¤æ™®æ¯”ç‡:</span>
                        <span class="metric-value">{perf.get('sharpe_ratio', 0):.2f}</span>
                    </div>
                    <div class="metric">
                        <span class="metric-label">æœ€å¤§å›æ’¤:</span>
                        <span class="metric-value">{perf.get('max_drawdown', 0):.1%}</span>
                    </div>
                    <div class="metric">
                        <span class="metric-label">å¹´åŒ–æ”¶ç›Š:</span>
                        <span class="metric-value">{perf.get('annual_return', 0):.1%}</span>
                    </div>
                    <div class="metric">
                        <span class="metric-label">äº¤æ˜“æ¬¡æ•°:</span>
                        <span class="metric-value">{perf.get('total_trades', 0)}</span>
                    </div>
                    <div class="metric">
                        <span class="metric-label">ç›ˆäºæ¯”:</span>
                        <span class="metric-value">{perf.get('profit_factor', 0):.2f}</span>
                    </div>
                </div>
                <p><strong>æ¨¡å‹è·¯å¾„:</strong> {model.get('model_path', 'N/A')}</p>
            </div>
            """
        
        return html
    
    def stop(self):
        """åœæ­¢æŒç»­è®­ç»ƒ"""
        logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·...")
        self.is_running = False

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='DipMasteræŒç»­æ¨¡å‹è®­ç»ƒç³»ç»Ÿ')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--single-run', action='store_true', help='åªè¿è¡Œä¸€æ¬¡è¿­ä»£')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç¼–æ’å™¨
    orchestrator = ContinuousModelTrainingOrchestrator(args.config)
    
    try:
        if args.single_run:
            # å•æ¬¡è¿è¡Œæ¨¡å¼
            logger.info("ğŸ”„ å•æ¬¡è¿­ä»£æ¨¡å¼")
            result = orchestrator.run_single_iteration()
            
            if result.get('global_success', False):
                logger.info("ğŸ‰ å•æ¬¡è¿è¡ŒæˆåŠŸè¾¾åˆ°ç›®æ ‡!")
            else:
                logger.info("ğŸ“Š å•æ¬¡è¿è¡Œå®Œæˆï¼Œå¯æŸ¥çœ‹ç»“æœè¿›è¡Œåˆ†æ")
        
        else:
            # æŒç»­è¿è¡Œæ¨¡å¼
            logger.info("ğŸ” æŒç»­è®­ç»ƒæ¨¡å¼")
            orchestrator.run_continuous_loop()
    
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        orchestrator.stop()
    
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        raise
    
    finally:
        logger.info("ğŸ‘‹ ç¨‹åºé€€å‡º")

if __name__ == "__main__":
    main()