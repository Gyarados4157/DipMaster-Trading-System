#!/usr/bin/env python3
"""
DipMaster Top30 Expansion Data Downloader
ä¸º30å¸ç§ç­–ç•¥æ‰©å±•ä¸‹è½½é¢å¤–5ä¸ªå¸ç§çš„æ•°æ®

æ–°å¢å¸ç§: SHIBUSDT, DOGEUSDT, TONUSDT, PEPEUSDT, INJUSDT
"""

import pandas as pd
import numpy as np
import ccxt
from datetime import datetime, timedelta
import time
import logging
import json
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class Top30ExpansionDataDownloader:
    def __init__(self):
        self.exchange = ccxt.binance({
            'rateLimit': 1200,
            'enableRateLimit': True,
        })
        
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        
        # æ–°å¢å¸ç§åˆ—è¡¨
        self.new_symbols = ['SHIBUSDT', 'DOGEUSDT', 'TONUSDT', 'PEPEUSDT', 'INJUSDT']
        self.timeframes = ['5m', '15m', '1h']
        self.days_back = 90
        
        # æ•°æ®å­˜å‚¨è·¯å¾„
        self.data_dir = Path('data/enhanced_market_data')
        self.data_dir.mkdir(exist_ok=True)
        
    def download_symbol_data(self, symbol: str, timeframe: str, days: int = 90) -> pd.DataFrame:
        """ä¸‹è½½å•ä¸ªå¸ç§çš„æ•°æ®"""
        try:
            self.logger.info(f"Downloading {symbol} {timeframe} data for {days} days...")
            
            # è®¡ç®—å¼€å§‹æ—¶é—´
            end_time = datetime.now()
            start_time = end_time - timedelta(days=days)
            
            # è½¬æ¢ä¸ºæ¯«ç§’æ—¶é—´æˆ³
            since = int(start_time.timestamp() * 1000)
            
            # ä¸‹è½½æ•°æ®
            all_data = []
            while since < int(end_time.timestamp() * 1000):
                try:
                    ohlcv = self.exchange.fetch_ohlcv(symbol, timeframe, since, limit=1000)
                    if not ohlcv:
                        break
                    
                    all_data.extend(ohlcv)
                    since = ohlcv[-1][0] + 1
                    time.sleep(0.1)  # é™é€Ÿ
                    
                except Exception as e:
                    self.logger.error(f"Error downloading {symbol} {timeframe}: {e}")
                    time.sleep(2)
                    continue
            
            if not all_data:
                self.logger.error(f"No data downloaded for {symbol} {timeframe}")
                return pd.DataFrame()
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(all_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            
            # æ•°æ®æ¸…æ´—
            df = df.drop_duplicates('timestamp').sort_values('timestamp').reset_index(drop=True)
            
            # åŸºæœ¬éªŒè¯
            df = df.dropna()
            if len(df) < 100:
                self.logger.warning(f"Insufficient data for {symbol} {timeframe}: {len(df)} rows")
                return pd.DataFrame()
            
            self.logger.info(f"Downloaded {len(df)} rows for {symbol} {timeframe}")
            return df
            
        except Exception as e:
            self.logger.error(f"Failed to download {symbol} {timeframe}: {e}")
            return pd.DataFrame()
    
    def validate_data_quality(self, df: pd.DataFrame, symbol: str, timeframe: str) -> dict:
        """éªŒè¯æ•°æ®è´¨é‡"""
        if df.empty:
            return {'quality_score': 0, 'issues': ['No data']}
        
        issues = []
        quality_metrics = {}
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        total_rows = len(df)
        null_count = df.isnull().sum().sum()
        quality_metrics['completeness'] = 1 - (null_count / (total_rows * len(df.columns)))
        
        if quality_metrics['completeness'] < 0.98:
            issues.append(f"Data completeness: {quality_metrics['completeness']:.2%}")
        
        # æ£€æŸ¥æ—¶é—´åºåˆ—è¿ç»­æ€§
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        time_diff = df['timestamp'].diff().dt.total_seconds()
        expected_interval = {'5m': 300, '15m': 900, '1h': 3600}[timeframe]
        
        irregular_intervals = (time_diff != expected_interval).sum()
        quality_metrics['time_regularity'] = 1 - (irregular_intervals / total_rows)
        
        if quality_metrics['time_regularity'] < 0.95:
            issues.append(f"Time irregularity: {irregular_intervals} gaps")
        
        # æ£€æŸ¥ä»·æ ¼å¼‚å¸¸
        price_cols = ['open', 'high', 'low', 'close']
        for col in price_cols:
            if col in df.columns:
                # æ£€æŸ¥é›¶å€¼æˆ–è´Ÿå€¼
                invalid_prices = (df[col] <= 0).sum()
                if invalid_prices > 0:
                    issues.append(f"Invalid {col} prices: {invalid_prices}")
                
                # æ£€æŸ¥æç«¯å˜åŒ–
                pct_changes = df[col].pct_change().abs()
                extreme_changes = (pct_changes > 0.5).sum()
                if extreme_changes > total_rows * 0.001:  # è¶…è¿‡0.1%
                    issues.append(f"Extreme {col} changes: {extreme_changes}")
        
        # æ£€æŸ¥OHLCé€»è¾‘
        if all(col in df.columns for col in price_cols):
            ohlc_violations = (
                (df['high'] < df['open']) | 
                (df['high'] < df['close']) | 
                (df['low'] > df['open']) | 
                (df['low'] > df['close'])
            ).sum()
            
            if ohlc_violations > 0:
                issues.append(f"OHLC logic violations: {ohlc_violations}")
        
        # æ£€æŸ¥æˆäº¤é‡
        if 'volume' in df.columns:
            zero_volume = (df['volume'] <= 0).sum()
            if zero_volume > total_rows * 0.05:  # è¶…è¿‡5%
                issues.append(f"Zero volume periods: {zero_volume}")
        
        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
        quality_score = np.mean([
            quality_metrics.get('completeness', 0),
            quality_metrics.get('time_regularity', 0),
            1 - (len(issues) * 0.1)  # æ¯ä¸ªé—®é¢˜æ‰£10%
        ])
        
        return {
            'quality_score': max(0, quality_score),
            'issues': issues,
            'metrics': quality_metrics,
            'row_count': total_rows
        }
    
    def save_data(self, df: pd.DataFrame, symbol: str, timeframe: str) -> bool:
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            if df.empty:
                return False
            
            # æ„å»ºæ–‡ä»¶å
            filename = f"{symbol}_{timeframe}_{self.days_back}days.parquet"
            filepath = self.data_dir / filename
            
            # ä¿å­˜æ•°æ®
            df.to_parquet(filepath, index=False)
            
            # åˆ›å»ºå…ƒæ•°æ®
            metadata = {
                'symbol': symbol,
                'timeframe': timeframe,
                'days_back': self.days_back,
                'total_rows': len(df),
                'start_date': df['timestamp'].min().isoformat(),
                'end_date': df['timestamp'].max().isoformat(),
                'download_date': datetime.now().isoformat(),
                'file_size_mb': filepath.stat().st_size / (1024 * 1024)
            }
            
            metadata_file = filepath.with_suffix('.parquet_metadata.json')
            with open(metadata_file, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            self.logger.info(f"Saved {filename} ({len(df)} rows)")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to save {symbol} {timeframe}: {e}")
            return False
    
    def download_all_new_symbols(self) -> dict:
        """ä¸‹è½½æ‰€æœ‰æ–°å¸ç§æ•°æ®"""
        results = {
            'downloaded': [],
            'failed': [],
            'quality_scores': {},
            'total_files': 0,
            'summary': {}
        }
        
        for symbol in self.new_symbols:
            symbol_results = {'symbol': symbol, 'timeframes': {}}
            
            for timeframe in self.timeframes:
                try:
                    # ä¸‹è½½æ•°æ®
                    df = self.download_symbol_data(symbol, timeframe, self.days_back)
                    
                    if not df.empty:
                        # éªŒè¯è´¨é‡
                        quality_result = self.validate_data_quality(df, symbol, timeframe)
                        
                        # ä¿å­˜æ•°æ®
                        if quality_result['quality_score'] > 0.8:  # è´¨é‡é˜ˆå€¼
                            if self.save_data(df, symbol, timeframe):
                                symbol_results['timeframes'][timeframe] = {
                                    'status': 'success',
                                    'rows': len(df),
                                    'quality_score': quality_result['quality_score']
                                }
                                results['total_files'] += 1
                            else:
                                symbol_results['timeframes'][timeframe] = {
                                    'status': 'save_failed',
                                    'error': 'Failed to save file'
                                }
                        else:
                            symbol_results['timeframes'][timeframe] = {
                                'status': 'quality_failed',
                                'quality_score': quality_result['quality_score'],
                                'issues': quality_result['issues']
                            }
                    else:
                        symbol_results['timeframes'][timeframe] = {
                            'status': 'download_failed',
                            'error': 'No data returned'
                        }
                        
                except Exception as e:
                    symbol_results['timeframes'][timeframe] = {
                        'status': 'error',
                        'error': str(e)
                    }
                
                # ä¼‘æ¯ä¸€ä¸‹é¿å…é™é€Ÿ
                time.sleep(1)
            
            # åˆ¤æ–­æ•´ä½“æˆåŠŸæˆ–å¤±è´¥
            success_count = sum(1 for tf_result in symbol_results['timeframes'].values() 
                              if tf_result.get('status') == 'success')
            
            if success_count >= 2:  # è‡³å°‘2ä¸ªæ—¶é—´æ¡†æ¶æˆåŠŸ
                results['downloaded'].append(symbol)
            else:
                results['failed'].append(symbol)
            
            results['quality_scores'][symbol] = symbol_results
        
        # ç”Ÿæˆæ‘˜è¦
        results['summary'] = {
            'total_symbols': len(self.new_symbols),
            'successful_symbols': len(results['downloaded']),
            'failed_symbols': len(results['failed']),
            'total_files_created': results['total_files'],
            'success_rate': len(results['downloaded']) / len(self.new_symbols)
        }
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    print("DipMaster Top30 Expansion Data Downloader")
    print("=" * 50)
    
    downloader = Top30ExpansionDataDownloader()
    
    print(f"æ–°å¢å¸ç§: {', '.join(downloader.new_symbols)}")
    print(f"æ—¶é—´æ¡†æ¶: {', '.join(downloader.timeframes)}")
    print(f"å†å²æ•°æ®: {downloader.days_back} å¤©")
    print()
    
    # å¼€å§‹ä¸‹è½½
    print("å¼€å§‹ä¸‹è½½æ•°æ®...")
    start_time = time.time()
    
    results = downloader.download_all_new_symbols()
    
    end_time = time.time()
    
    # æ‰“å°ç»“æœ
    print(f"\nä¸‹è½½å®Œæˆ! è€—æ—¶: {end_time - start_time:.1f} ç§’")
    print(f"æˆåŠŸä¸‹è½½: {results['summary']['successful_symbols']}/{results['summary']['total_symbols']} å¸ç§")
    print(f"åˆ›å»ºæ–‡ä»¶: {results['summary']['total_files']} ä¸ª")
    print(f"æˆåŠŸç‡: {results['summary']['success_rate']:.1%}")
    
    if results['downloaded']:
        print(f"\nâœ… æˆåŠŸå¸ç§: {', '.join(results['downloaded'])}")
    
    if results['failed']:
        print(f"\nâŒ å¤±è´¥å¸ç§: {', '.join(results['failed'])}")
    
    # ä¿å­˜ç»“æœæŠ¥å‘Š
    report_file = Path('data') / f'top30_expansion_download_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(report_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    return results

if __name__ == "__main__":
    main()