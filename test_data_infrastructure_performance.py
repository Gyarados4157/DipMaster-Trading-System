#!/usr/bin/env python3
"""
æ•°æ®åŸºç¡€è®¾æ–½æ€§èƒ½æµ‹è¯• - Data Infrastructure Performance Testing
éªŒè¯DipMaster Trading Systemæ•°æ®åŸºç¡€è®¾æ–½çš„æ€§èƒ½å’Œè´¨é‡

Test Categories:
- æ•°æ®è®¿é—®æ€§èƒ½æµ‹è¯•
- è´¨é‡ç›‘æ§åŠŸèƒ½æµ‹è¯•  
- APIæ¥å£æ€§èƒ½æµ‹è¯•
- å®æ—¶æµå¤„ç†æµ‹è¯•
- å­˜å‚¨æ•ˆç‡æµ‹è¯•
"""

import asyncio
import time
import json
import pandas as pd
import numpy as np
from datetime import datetime, timezone, timedelta
from pathlib import Path
import logging
import sys
import statistics
from typing import Dict, List, Any, Tuple
import concurrent.futures
import requests
import websockets
import traceback
import psutil
import gc

# å¯¼å…¥æµ‹è¯•æ¨¡å—
sys.path.append(str(Path(__file__).parent))
from src.data.professional_data_infrastructure import ProfessionalDataInfrastructure
from src.data.data_quality_monitor import DataQualityMonitor
from src.data.realtime_data_stream import DataStreamManager

class PerformanceTestSuite:
    """æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.logger = self._setup_logging()
        self.test_results = {
            'test_suite': 'Data Infrastructure Performance',
            'test_start': datetime.now(timezone.utc).isoformat(),
            'test_environment': self._get_test_environment(),
            'tests': {}
        }
        
        # æµ‹è¯•é…ç½®
        self.test_symbols = ['BTCUSDT', 'ETHUSDT', 'ADAUSDT', 'SOLUSDT', 'XRPUSDT']
        self.test_timeframes = ['1m', '5m', '15m', '1h']
        
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('performance_test.log'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        return logging.getLogger(__name__)
        
    def _get_test_environment(self) -> Dict[str, Any]:
        """è·å–æµ‹è¯•ç¯å¢ƒä¿¡æ¯"""
        return {
            'python_version': sys.version,
            'platform': sys.platform,
            'cpu_count': psutil.cpu_count(),
            'memory_total_gb': psutil.virtual_memory().total / 1024**3,
            'disk_free_gb': psutil.disk_usage('.').free / 1024**3
        }
        
    def measure_time(self, func):
        """æ—¶é—´æµ‹é‡è£…é¥°å™¨"""
        def wrapper(*args, **kwargs):
            start_time = time.time()
            result = func(*args, **kwargs)
            end_time = time.time()
            return result, (end_time - start_time)
        return wrapper
        
    async def test_data_loading_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•æ•°æ®åŠ è½½æ€§èƒ½"""
        self.logger.info("å¼€å§‹æ•°æ®åŠ è½½æ€§èƒ½æµ‹è¯•...")
        
        results = {
            'test_name': 'data_loading_performance',
            'status': 'running',
            'metrics': {}
        }
        
        try:
            # åˆå§‹åŒ–åŸºç¡€è®¾æ–½
            infrastructure = ProfessionalDataInfrastructure()
            
            loading_times = []
            record_counts = []
            memory_usage = []
            
            # æµ‹è¯•ä¸åŒå¸ç§å’Œæ—¶é—´æ¡†æ¶çš„åŠ è½½æ€§èƒ½
            for symbol in self.test_symbols:
                for timeframe in self.test_timeframes:
                    try:
                        # è®°å½•å†…å­˜ä½¿ç”¨
                        memory_before = psutil.Process().memory_info().rss / 1024**2
                        
                        # æµ‹é‡åŠ è½½æ—¶é—´
                        start_time = time.time()
                        df = infrastructure.get_data(symbol, timeframe)
                        end_time = time.time()
                        
                        if not df.empty:
                            loading_time = end_time - start_time
                            record_count = len(df)
                            
                            loading_times.append(loading_time)
                            record_counts.append(record_count)
                            
                            # è®¡ç®—åŠ è½½é€Ÿåº¦
                            records_per_second = record_count / loading_time if loading_time > 0 else 0
                            
                            memory_after = psutil.Process().memory_info().rss / 1024**2
                            memory_delta = memory_after - memory_before
                            memory_usage.append(memory_delta)
                            
                            self.logger.info(f"{symbol} {timeframe}: {record_count} æ¡è®°å½•, "
                                           f"{loading_time:.3f}s, {records_per_second:.0f} è®°å½•/ç§’")
                            
                    except Exception as e:
                        self.logger.warning(f"åŠ è½½ {symbol} {timeframe} å¤±è´¥: {e}")
                        
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            if loading_times:
                results['metrics'] = {
                    'total_tests': len(loading_times),
                    'avg_loading_time_s': statistics.mean(loading_times),
                    'median_loading_time_s': statistics.median(loading_times),
                    'p95_loading_time_s': np.percentile(loading_times, 95),
                    'p99_loading_time_s': np.percentile(loading_times, 99),
                    'avg_records_per_second': statistics.mean(
                        [count / time for count, time in zip(record_counts, loading_times) if time > 0]
                    ),
                    'total_records_loaded': sum(record_counts),
                    'avg_memory_usage_mb': statistics.mean(memory_usage) if memory_usage else 0,
                    'max_memory_usage_mb': max(memory_usage) if memory_usage else 0
                }
                
                results['status'] = 'passed'
                
                # æ€§èƒ½åˆ¤æ–­
                avg_loading_time = results['metrics']['avg_loading_time_s']
                if avg_loading_time > 5.0:
                    results['status'] = 'warning'
                    results['warning'] = f"å¹³å‡åŠ è½½æ—¶é—´è¿‡é•¿: {avg_loading_time:.3f}s"
                elif avg_loading_time > 10.0:
                    results['status'] = 'failed'
                    results['error'] = f"åŠ è½½æ€§èƒ½ä¸è¾¾æ ‡: {avg_loading_time:.3f}s"
                    
            else:
                results['status'] = 'failed'
                results['error'] = 'æœªèƒ½åŠ è½½ä»»ä½•æ•°æ®'
                
        except Exception as e:
            results['status'] = 'failed'
            results['error'] = str(e)
            self.logger.error(f"æ•°æ®åŠ è½½æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            
        return results
        
    async def test_quality_monitoring_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•è´¨é‡ç›‘æ§æ€§èƒ½"""
        self.logger.info("å¼€å§‹è´¨é‡ç›‘æ§æ€§èƒ½æµ‹è¯•...")
        
        results = {
            'test_name': 'quality_monitoring_performance',
            'status': 'running',
            'metrics': {}
        }
        
        try:
            # åˆå§‹åŒ–è´¨é‡ç›‘æ§
            quality_monitor = DataQualityMonitor({
                'auto_repair': True,
                'db_path': 'data/test_quality_monitor.db'
            })
            
            # åˆå§‹åŒ–åŸºç¡€è®¾æ–½
            infrastructure = ProfessionalDataInfrastructure()
            
            assessment_times = []
            quality_scores = []
            
            # æµ‹è¯•è´¨é‡è¯„ä¼°æ€§èƒ½
            for symbol in self.test_symbols[:3]:  # é™åˆ¶æµ‹è¯•æ•°é‡
                try:
                    # åŠ è½½æ•°æ®
                    df = infrastructure.get_data(symbol, '5m')
                    
                    if not df.empty:
                        # æµ‹é‡è´¨é‡è¯„ä¼°æ—¶é—´
                        start_time = time.time()
                        metrics = quality_monitor.assess_data_quality(df, symbol, '5m')
                        end_time = time.time()
                        
                        assessment_time = end_time - start_time
                        assessment_times.append(assessment_time)
                        quality_scores.append(metrics.overall_score)
                        
                        # è®¡ç®—è¯„ä¼°é€Ÿåº¦
                        records_per_second = len(df) / assessment_time if assessment_time > 0 else 0
                        
                        self.logger.info(f"{symbol} è´¨é‡è¯„ä¼°: {len(df)} æ¡è®°å½•, "
                                       f"{assessment_time:.3f}s, è¯„åˆ†: {metrics.overall_score:.3f}")
                        
                except Exception as e:
                    self.logger.warning(f"è´¨é‡è¯„ä¼° {symbol} å¤±è´¥: {e}")
                    
            # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆæ€§èƒ½
            report_start_time = time.time()
            try:
                report = quality_monitor.get_quality_report(days=1)
                report_generation_time = time.time() - report_start_time
            except Exception as e:
                report_generation_time = -1
                self.logger.warning(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
                
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            if assessment_times:
                results['metrics'] = {
                    'total_assessments': len(assessment_times),
                    'avg_assessment_time_s': statistics.mean(assessment_times),
                    'median_assessment_time_s': statistics.median(assessment_times),
                    'max_assessment_time_s': max(assessment_times),
                    'avg_quality_score': statistics.mean(quality_scores),
                    'min_quality_score': min(quality_scores),
                    'report_generation_time_s': report_generation_time
                }
                
                results['status'] = 'passed'
                
                # æ€§èƒ½åˆ¤æ–­
                avg_assessment_time = results['metrics']['avg_assessment_time_s']
                if avg_assessment_time > 2.0:
                    results['status'] = 'warning'
                    results['warning'] = f"è´¨é‡è¯„ä¼°æ—¶é—´è¿‡é•¿: {avg_assessment_time:.3f}s"
                elif avg_assessment_time > 5.0:
                    results['status'] = 'failed'
                    results['error'] = f"è´¨é‡ç›‘æ§æ€§èƒ½ä¸è¾¾æ ‡: {avg_assessment_time:.3f}s"
                    
            else:
                results['status'] = 'failed'
                results['error'] = 'æœªèƒ½å®Œæˆä»»ä½•è´¨é‡è¯„ä¼°'
                
        except Exception as e:
            results['status'] = 'failed'
            results['error'] = str(e)
            self.logger.error(f"è´¨é‡ç›‘æ§æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            
        return results
        
    async def test_concurrent_access_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘è®¿é—®æ€§èƒ½"""
        self.logger.info("å¼€å§‹å¹¶å‘è®¿é—®æ€§èƒ½æµ‹è¯•...")
        
        results = {
            'test_name': 'concurrent_access_performance',
            'status': 'running',
            'metrics': {}
        }
        
        try:
            infrastructure = ProfessionalDataInfrastructure()
            
            # å¹¶å‘æµ‹è¯•å‚æ•°
            concurrent_levels = [1, 5, 10, 20]
            concurrent_results = {}
            
            for concurrency in concurrent_levels:
                self.logger.info(f"æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency}")
                
                # å‡†å¤‡å¹¶å‘ä»»åŠ¡
                async def load_data_task(symbol, timeframe):
                    try:
                        start_time = time.time()
                        df = infrastructure.get_data(symbol, timeframe)
                        end_time = time.time()
                        return {
                            'success': True,
                            'duration': end_time - start_time,
                            'record_count': len(df) if not df.empty else 0
                        }
                    except Exception as e:
                        return {
                            'success': False,
                            'error': str(e),
                            'duration': 0,
                            'record_count': 0
                        }
                
                # åˆ›å»ºå¹¶å‘ä»»åŠ¡
                tasks = []
                test_combinations = []
                for i in range(concurrency):
                    symbol = self.test_symbols[i % len(self.test_symbols)]
                    timeframe = self.test_timeframes[i % len(self.test_timeframes)]
                    test_combinations.append((symbol, timeframe))
                    tasks.append(load_data_task(symbol, timeframe))
                
                # æ‰§è¡Œå¹¶å‘æµ‹è¯•
                start_time = time.time()
                task_results = await asyncio.gather(*tasks, return_exceptions=True)
                total_duration = time.time() - start_time
                
                # ç»Ÿè®¡ç»“æœ
                successful_tasks = [r for r in task_results if isinstance(r, dict) and r.get('success', False)]
                failed_tasks = len(task_results) - len(successful_tasks)
                
                if successful_tasks:
                    avg_duration = statistics.mean([r['duration'] for r in successful_tasks])
                    total_records = sum([r['record_count'] for r in successful_tasks])
                    throughput = len(successful_tasks) / total_duration if total_duration > 0 else 0
                    
                    concurrent_results[concurrency] = {
                        'total_tasks': len(task_results),
                        'successful_tasks': len(successful_tasks),
                        'failed_tasks': failed_tasks,
                        'success_rate': len(successful_tasks) / len(task_results),
                        'total_duration_s': total_duration,
                        'avg_task_duration_s': avg_duration,
                        'throughput_tasks_per_s': throughput,
                        'total_records_loaded': total_records
                    }
                else:
                    concurrent_results[concurrency] = {
                        'total_tasks': len(task_results),
                        'successful_tasks': 0,
                        'failed_tasks': failed_tasks,
                        'success_rate': 0,
                        'error': 'æ‰€æœ‰ä»»åŠ¡å¤±è´¥'
                    }
                    
            results['metrics'] = {
                'concurrent_test_results': concurrent_results,
                'max_successful_concurrency': max([
                    level for level, result in concurrent_results.items()
                    if result.get('success_rate', 0) > 0.8
                ], default=0),
                'peak_throughput_tasks_per_s': max([
                    result.get('throughput_tasks_per_s', 0)
                    for result in concurrent_results.values()
                ], default=0)
            }
            
            results['status'] = 'passed'
            
            # æ€§èƒ½åˆ¤æ–­
            max_concurrency = results['metrics']['max_successful_concurrency']
            if max_concurrency < 5:
                results['status'] = 'warning'
                results['warning'] = f"å¹¶å‘å¤„ç†èƒ½åŠ›è¾ƒä½: {max_concurrency}"
            elif max_concurrency < 2:
                results['status'] = 'failed'
                results['error'] = f"å¹¶å‘æ€§èƒ½ä¸è¾¾æ ‡: {max_concurrency}"
                
        except Exception as e:
            results['status'] = 'failed'
            results['error'] = str(e)
            self.logger.error(f"å¹¶å‘è®¿é—®æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            
        return results
        
    async def test_memory_usage(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        self.logger.info("å¼€å§‹å†…å­˜ä½¿ç”¨æµ‹è¯•...")
        
        results = {
            'test_name': 'memory_usage',
            'status': 'running',
            'metrics': {}
        }
        
        try:
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024**2  # MB
            
            infrastructure = ProfessionalDataInfrastructure()
            memory_samples = [initial_memory]
            
            # åŠ è½½å¤§é‡æ•°æ®å¹¶ç›‘æ§å†…å­˜
            for symbol in self.test_symbols:
                for timeframe in ['5m', '15m']:
                    try:
                        df = infrastructure.get_data(symbol, timeframe)
                        current_memory = process.memory_info().rss / 1024**2
                        memory_samples.append(current_memory)
                        
                        self.logger.debug(f"åŠ è½½ {symbol} {timeframe} åå†…å­˜: {current_memory:.1f}MB")
                        
                    except Exception as e:
                        self.logger.warning(f"å†…å­˜æµ‹è¯•åŠ è½½ {symbol} {timeframe} å¤±è´¥: {e}")
                        
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            final_memory = process.memory_info().rss / 1024**2
            memory_samples.append(final_memory)
            
            # è®¡ç®—å†…å­˜ç»Ÿè®¡
            results['metrics'] = {
                'initial_memory_mb': initial_memory,
                'peak_memory_mb': max(memory_samples),
                'final_memory_mb': final_memory,
                'memory_increase_mb': final_memory - initial_memory,
                'memory_samples_count': len(memory_samples),
                'avg_memory_mb': statistics.mean(memory_samples),
                'memory_efficiency_score': min(1.0, 500 / max(memory_samples)) if memory_samples else 0
            }
            
            results['status'] = 'passed'
            
            # å†…å­˜ä½¿ç”¨åˆ¤æ–­
            peak_memory = results['metrics']['peak_memory_mb']
            if peak_memory > 1000:  # 1GB
                results['status'] = 'warning'
                results['warning'] = f"å†…å­˜ä½¿ç”¨è¾ƒé«˜: {peak_memory:.1f}MB"
            elif peak_memory > 2000:  # 2GB
                results['status'] = 'failed'
                results['error'] = f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {peak_memory:.1f}MB"
                
        except Exception as e:
            results['status'] = 'failed'
            results['error'] = str(e)
            self.logger.error(f"å†…å­˜ä½¿ç”¨æµ‹è¯•å¤±è´¥: {e}")
            
        return results
        
    async def test_storage_efficiency(self) -> Dict[str, Any]:
        """æµ‹è¯•å­˜å‚¨æ•ˆç‡"""
        self.logger.info("å¼€å§‹å­˜å‚¨æ•ˆç‡æµ‹è¯•...")
        
        results = {
            'test_name': 'storage_efficiency',
            'status': 'running',
            'metrics': {}
        }
        
        try:
            # æ£€æŸ¥æ•°æ®å­˜å‚¨è·¯å¾„
            storage_path = Path("data/professional_storage")
            
            if not storage_path.exists():
                results['status'] = 'skipped'
                results['reason'] = 'å­˜å‚¨è·¯å¾„ä¸å­˜åœ¨'
                return results
                
            # è®¡ç®—å­˜å‚¨ç»Ÿè®¡
            total_size = 0
            file_count = 0
            file_sizes = []
            
            for file_path in storage_path.rglob("*.parquet"):
                if file_path.is_file():
                    file_size = file_path.stat().st_size
                    total_size += file_size
                    file_count += 1
                    file_sizes.append(file_size)
                    
            # ä¼°ç®—åŸå§‹æ•°æ®å¤§å°ï¼ˆåŸºäºè®°å½•æ•°ï¼‰
            estimated_raw_size = 0
            infrastructure = ProfessionalDataInfrastructure()
            
            for symbol in self.test_symbols[:3]:  # é™åˆ¶æµ‹è¯•èŒƒå›´
                try:
                    df = infrastructure.get_data(symbol, '5m')
                    if not df.empty:
                        # ä¼°ç®—ï¼šæ¯æ¡è®°å½•çº¦40å­—èŠ‚(5ä¸ªfloat64 = 8*5=40)
                        estimated_raw_size += len(df) * 40
                except Exception:
                    continue
                    
            # è®¡ç®—å‹ç¼©æ¯”
            compression_ratio = total_size / estimated_raw_size if estimated_raw_size > 0 else 0
            
            results['metrics'] = {
                'total_storage_size_mb': total_size / 1024**2,
                'file_count': file_count,
                'avg_file_size_mb': statistics.mean(file_sizes) / 1024**2 if file_sizes else 0,
                'median_file_size_mb': statistics.median(file_sizes) / 1024**2 if file_sizes else 0,
                'estimated_raw_size_mb': estimated_raw_size / 1024**2,
                'compression_ratio': compression_ratio,
                'storage_efficiency_score': min(1.0, 0.5 / compression_ratio) if compression_ratio > 0 else 0
            }
            
            results['status'] = 'passed'
            
            # å­˜å‚¨æ•ˆç‡åˆ¤æ–­
            if compression_ratio > 0.3:  # å‹ç¼©æ¯”è¿‡ä½
                results['status'] = 'warning'
                results['warning'] = f"å‹ç¼©æ•ˆç‡è¾ƒä½: {compression_ratio:.3f}"
            elif compression_ratio > 0.5:
                results['status'] = 'failed'
                results['error'] = f"å­˜å‚¨æ•ˆç‡ä¸è¾¾æ ‡: {compression_ratio:.3f}"
                
        except Exception as e:
            results['status'] = 'failed'
            results['error'] = str(e)
            self.logger.error(f"å­˜å‚¨æ•ˆç‡æµ‹è¯•å¤±è´¥: {e}")
            
        return results
        
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        self.logger.info("=" * 80)
        self.logger.info("å¼€å§‹DipMasteræ•°æ®åŸºç¡€è®¾æ–½æ€§èƒ½æµ‹è¯•å¥—ä»¶")
        self.logger.info("=" * 80)
        
        # æµ‹è¯•åˆ—è¡¨
        test_functions = [
            self.test_data_loading_performance,
            self.test_quality_monitoring_performance,
            self.test_concurrent_access_performance,
            self.test_memory_usage,
            self.test_storage_efficiency
        ]
        
        # æ‰§è¡Œæµ‹è¯•
        for test_func in test_functions:
            try:
                test_result = await test_func()
                self.test_results['tests'][test_result['test_name']] = test_result
                
                # è¾“å‡ºæµ‹è¯•ç»“æœ
                status = test_result['status']
                status_symbol = "âœ…" if status == 'passed' else "âš ï¸" if status == 'warning' else "âŒ"
                self.logger.info(f"{status_symbol} {test_result['test_name']}: {status.upper()}")
                
                if 'error' in test_result:
                    self.logger.error(f"   é”™è¯¯: {test_result['error']}")
                elif 'warning' in test_result:
                    self.logger.warning(f"   è­¦å‘Š: {test_result['warning']}")
                    
            except Exception as e:
                self.logger.error(f"æµ‹è¯• {test_func.__name__} å¼‚å¸¸: {e}")
                self.test_results['tests'][test_func.__name__] = {
                    'test_name': test_func.__name__,
                    'status': 'error',
                    'error': str(e)
                }
                
        # ç”Ÿæˆæµ‹è¯•æ‘˜è¦
        total_tests = len(self.test_results['tests'])
        passed_tests = sum(1 for t in self.test_results['tests'].values() if t['status'] == 'passed')
        warning_tests = sum(1 for t in self.test_results['tests'].values() if t['status'] == 'warning')
        failed_tests = sum(1 for t in self.test_results['tests'].values() if t['status'] in ['failed', 'error'])
        
        self.test_results.update({
            'test_end': datetime.now(timezone.utc).isoformat(),
            'summary': {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'warning_tests': warning_tests,
                'failed_tests': failed_tests,
                'success_rate': passed_tests / total_tests if total_tests > 0 else 0,
                'overall_status': 'passed' if failed_tests == 0 else 'warning' if passed_tests > failed_tests else 'failed'
            }
        })
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        results_path = Path("data/performance_test_results.json")
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, ensure_ascii=False, indent=2, default=str)
            
        self.logger.info("=" * 80)
        self.logger.info("æ€§èƒ½æµ‹è¯•å®Œæˆ")
        self.logger.info(f"æ€»è®¡: {total_tests} ä¸ªæµ‹è¯•")
        self.logger.info(f"é€šè¿‡: {passed_tests}, è­¦å‘Š: {warning_tests}, å¤±è´¥: {failed_tests}")
        self.logger.info(f"æˆåŠŸç‡: {passed_tests/total_tests:.1%}")
        self.logger.info(f"ç»“æœä¿å­˜: {results_path}")
        self.logger.info("=" * 80)
        
        return self.test_results

async def main():
    """ä¸»å‡½æ•°"""
    try:
        test_suite = PerformanceTestSuite()
        results = await test_suite.run_all_tests()
        
        # è¾“å‡ºå…³é”®æŒ‡æ ‡
        print("\nğŸ“Š å…³é”®æ€§èƒ½æŒ‡æ ‡:")
        
        if 'data_loading_performance' in results['tests']:
            loading_metrics = results['tests']['data_loading_performance'].get('metrics', {})
            if loading_metrics:
                print(f"   æ•°æ®åŠ è½½é€Ÿåº¦: {loading_metrics.get('avg_records_per_second', 0):.0f} è®°å½•/ç§’")
                print(f"   å¹³å‡å“åº”æ—¶é—´: {loading_metrics.get('avg_loading_time_s', 0):.3f} ç§’")
                
        if 'concurrent_access_performance' in results['tests']:
            concurrent_metrics = results['tests']['concurrent_access_performance'].get('metrics', {})
            if concurrent_metrics:
                print(f"   æœ€å¤§å¹¶å‘å¤„ç†: {concurrent_metrics.get('max_successful_concurrency', 0)} ä¸ªä»»åŠ¡")
                print(f"   å³°å€¼ååé‡: {concurrent_metrics.get('peak_throughput_tasks_per_s', 0):.1f} ä»»åŠ¡/ç§’")
                
        if 'memory_usage' in results['tests']:
            memory_metrics = results['tests']['memory_usage'].get('metrics', {})
            if memory_metrics:
                print(f"   å³°å€¼å†…å­˜ä½¿ç”¨: {memory_metrics.get('peak_memory_mb', 0):.1f} MB")
                print(f"   å†…å­˜æ•ˆç‡è¯„åˆ†: {memory_metrics.get('memory_efficiency_score', 0):.3f}")
                
        return results['summary']['overall_status'] == 'passed'
        
    except KeyboardInterrupt:
        print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return False
    except Exception as e:
        print(f"\næµ‹è¯•å¼‚å¸¸: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)