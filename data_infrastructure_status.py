#!/usr/bin/env python3
"""
Data Infrastructure Status Dashboard
DipMaster Trading System - æ•°æ®åŸºç¡€è®¾æ–½çŠ¶æ€ä»ªè¡¨æ¿

å®æ—¶æ˜¾ç¤ºæ•°æ®åŸºç¡€è®¾æ–½çš„è¿è¡ŒçŠ¶æ€ã€è´¨é‡æŒ‡æ ‡å’Œç›‘æ§ä¿¡æ¯
"""

import sys
import time
import json
import sqlite3
from pathlib import Path
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from collections import defaultdict, Counter
import argparse

class DataInfrastructureStatus:
    """æ•°æ®åŸºç¡€è®¾æ–½çŠ¶æ€æ˜¾ç¤ºå™¨"""
    
    def __init__(self):
        self.data_path = Path("data/enhanced_market_data")
        self.monitoring_db = Path("data/monitoring.db")
        self.reports_path = Path("data")
        
        # TOP30ç¬¦å·
        self.top30_symbols = [
            "BTCUSDT", "ETHUSDT", "BNBUSDT", "XRPUSDT", "SOLUSDT",
            "ADAUSDT", "DOGEUSDT", "AVAXUSDT", "TRXUSDT", "LINKUSDT",
            "LTCUSDT", "DOTUSDT", "MATICUSDT", "UNIUSDT", "ICPUSDT",
            "NEARUSDT", "XLMUSDT", "ATOMUSDT", "VETUSDT", "FILUSDT",
            "APTUSDT", "ARBUSDT", "OPUSDT", "GRTUSDT", "MKRUSDT",
            "AAVEUSDT", "COMPUSDT", "ALGOUSDT", "TONUSDT", "INJUSDT"
        ]
        
        self.timeframes = ['1m', '5m', '15m', '1h', '4h', '1d']
    
    def get_file_coverage_status(self) -> Dict[str, Any]:
        """è·å–æ–‡ä»¶è¦†ç›–çŠ¶æ€"""
        if not self.data_path.exists():
            return {
                'status': 'no_data_directory',
                'total_files': 0,
                'expected_files': len(self.top30_symbols) * len(self.timeframes),
                'coverage_percentage': 0
            }
        
        parquet_files = list(self.data_path.glob("*.parquet"))
        expected_files = len(self.top30_symbols) * len(self.timeframes)
        
        # æŒ‰äº¤æ˜“å¯¹ç»Ÿè®¡
        symbol_coverage = {}
        for symbol in self.top30_symbols:
            symbol_files = [f for f in parquet_files if f.name.startswith(f"{symbol}_")]
            symbol_coverage[symbol] = {
                'files': len(symbol_files),
                'expected': len(self.timeframes),
                'coverage': len(symbol_files) / len(self.timeframes) * 100
            }
        
        # æŒ‰æ—¶é—´æ¡†æ¶ç»Ÿè®¡
        timeframe_coverage = {}
        for tf in self.timeframes:
            tf_files = [f for f in parquet_files if f"_{tf}_" in f.name]
            timeframe_coverage[tf] = {
                'files': len(tf_files),
                'expected': len(self.top30_symbols),
                'coverage': len(tf_files) / len(self.top30_symbols) * 100
            }
        
        # æ–‡ä»¶å¤§å°ç»Ÿè®¡
        total_size = sum(f.stat().st_size for f in parquet_files)
        avg_size = total_size / len(parquet_files) if parquet_files else 0
        
        return {
            'status': 'active',
            'total_files': len(parquet_files),
            'expected_files': expected_files,
            'coverage_percentage': len(parquet_files) / expected_files * 100 if expected_files > 0 else 0,
            'total_size_gb': total_size / 1024**3,
            'avg_file_size_mb': avg_size / 1024**2,
            'by_symbol': symbol_coverage,
            'by_timeframe': timeframe_coverage,
            'last_updated': self._get_latest_file_timestamp()
        }
    
    def _get_latest_file_timestamp(self) -> Optional[str]:
        """è·å–æœ€æ–°æ–‡ä»¶æ—¶é—´æˆ³"""
        if not self.data_path.exists():
            return None
        
        parquet_files = list(self.data_path.glob("*.parquet"))
        if not parquet_files:
            return None
        
        latest_mtime = max(f.stat().st_mtime for f in parquet_files)
        return datetime.fromtimestamp(latest_mtime, tz=timezone.utc).isoformat()
    
    def get_quality_metrics(self) -> Dict[str, Any]:
        """è·å–è´¨é‡æŒ‡æ ‡"""
        if not self.monitoring_db.exists():
            return {
                'status': 'no_monitoring_data',
                'message': 'ç›‘æ§æ•°æ®åº“ä¸å­˜åœ¨'
            }
        
        try:
            conn = sqlite3.connect(str(self.monitoring_db))
            
            # æœ€æ–°çš„å¹³å‡è´¨é‡åˆ†æ•°
            cursor = conn.execute("""
                SELECT AVG(overall_score) as avg_score,
                       AVG(completeness) as avg_completeness,
                       AVG(consistency) as avg_consistency, 
                       AVG(accuracy) as avg_accuracy,
                       COUNT(*) as total_assessments
                FROM quality_metrics 
                WHERE timestamp > datetime('now', '-24 hours')
            """)
            
            result = cursor.fetchone()
            if result and result[0] is not None:
                avg_metrics = {
                    'overall_score': result[0],
                    'completeness': result[1],
                    'consistency': result[2],
                    'accuracy': result[3],
                    'assessments_24h': result[4]
                }
            else:
                avg_metrics = {
                    'overall_score': 0,
                    'completeness': 0,
                    'consistency': 0,
                    'accuracy': 0,
                    'assessments_24h': 0
                }
            
            # æŒ‰æ—¶é—´æ¡†æ¶ç»Ÿè®¡è´¨é‡
            cursor = conn.execute("""
                SELECT timeframe, AVG(overall_score) as avg_score
                FROM quality_metrics 
                WHERE timestamp > datetime('now', '-24 hours')
                GROUP BY timeframe
                ORDER BY avg_score DESC
            """)
            
            timeframe_quality = dict(cursor.fetchall())
            
            # è´¨é‡è¶‹åŠ¿ (æœ€è¿‘24å°æ—¶æ¯å°æ—¶å¹³å‡)
            cursor = conn.execute("""
                SELECT strftime('%H', timestamp) as hour,
                       AVG(overall_score) as avg_score
                FROM quality_metrics 
                WHERE timestamp > datetime('now', '-24 hours')
                GROUP BY hour
                ORDER BY hour
            """)
            
            hourly_trend = dict(cursor.fetchall())
            
            conn.close()
            
            return {
                'status': 'active',
                'average_metrics': avg_metrics,
                'by_timeframe': timeframe_quality,
                'hourly_trend': hourly_trend,
                'last_check': datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            return {
                'status': 'error',
                'message': f'è´¨é‡æŒ‡æ ‡è·å–å¤±è´¥: {e}'
            }
    
    def get_alert_status(self) -> Dict[str, Any]:
        """è·å–å‘Šè­¦çŠ¶æ€"""
        if not self.monitoring_db.exists():
            return {
                'status': 'no_monitoring_data',
                'total_alerts': 0,
                'active_alerts': 0
            }
        
        try:
            conn = sqlite3.connect(str(self.monitoring_db))
            
            # æœ€è¿‘24å°æ—¶å‘Šè­¦ç»Ÿè®¡
            cursor = conn.execute("""
                SELECT severity, COUNT(*) as count
                FROM alerts 
                WHERE timestamp > datetime('now', '-24 hours')
                GROUP BY severity
            """)
            
            alerts_by_severity = dict(cursor.fetchall())
            
            # æœ€è¿‘çš„å‘Šè­¦
            cursor = conn.execute("""
                SELECT symbol, timeframe, alert_type, severity, message, timestamp
                FROM alerts 
                WHERE timestamp > datetime('now', '-24 hours')
                ORDER BY timestamp DESC
                LIMIT 10
            """)
            
            recent_alerts = []
            for row in cursor.fetchall():
                recent_alerts.append({
                    'symbol': row[0],
                    'timeframe': row[1],
                    'type': row[2],
                    'severity': row[3],
                    'message': row[4],
                    'timestamp': row[5]
                })
            
            # å‘Šè­¦ç±»å‹ç»Ÿè®¡
            cursor = conn.execute("""
                SELECT alert_type, COUNT(*) as count
                FROM alerts 
                WHERE timestamp > datetime('now', '-24 hours')
                GROUP BY alert_type
            """)
            
            alerts_by_type = dict(cursor.fetchall())
            
            conn.close()
            
            total_alerts = sum(alerts_by_severity.values())
            critical_alerts = alerts_by_severity.get('critical', 0) + alerts_by_severity.get('high', 0)
            
            return {
                'status': 'active',
                'total_alerts_24h': total_alerts,
                'critical_alerts': critical_alerts,
                'by_severity': alerts_by_severity,
                'by_type': alerts_by_type,
                'recent_alerts': recent_alerts,
                'last_check': datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            return {
                'status': 'error',
                'message': f'å‘Šè­¦çŠ¶æ€è·å–å¤±è´¥: {e}'
            }
    
    def get_system_health(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿå¥åº·åº¦"""
        coverage = self.get_file_coverage_status()
        quality = self.get_quality_metrics()
        alerts = self.get_alert_status()
        
        # è®¡ç®—å¥åº·åº¦è¯„åˆ†
        health_score = 0
        
        # æ•°æ®è¦†ç›–åº¦ (40%)
        if coverage['status'] == 'active':
            coverage_score = coverage['coverage_percentage'] / 100
            health_score += coverage_score * 0.4
        
        # æ•°æ®è´¨é‡ (40%)
        if quality['status'] == 'active' and quality['average_metrics']['overall_score'] > 0:
            quality_score = quality['average_metrics']['overall_score']
            health_score += quality_score * 0.4
        
        # å‘Šè­¦çŠ¶æ€ (20%)
        alert_penalty = 0
        if alerts['status'] == 'active':
            # ä¸¥é‡å‘Šè­¦æ‰£åˆ†
            critical_count = alerts.get('critical_alerts', 0)
            alert_penalty = min(critical_count * 0.1, 0.2)  # æœ€å¤šæ‰£20%
        
        health_score = max(0, health_score - alert_penalty)
        
        # å¥åº·ç­‰çº§
        if health_score >= 0.9:
            health_level = 'excellent'
            health_color = 'ğŸŸ¢'
        elif health_score >= 0.8:
            health_level = 'good'
            health_color = 'ğŸŸ¢'
        elif health_score >= 0.6:
            health_level = 'fair'
            health_color = 'ğŸŸ¡'
        elif health_score >= 0.4:
            health_level = 'poor'
            health_color = 'ğŸŸ '
        else:
            health_level = 'critical'
            health_color = 'ğŸ”´'
        
        return {
            'health_score': health_score,
            'health_level': health_level,
            'health_color': health_color,
            'components': {
                'data_coverage': coverage,
                'data_quality': quality,
                'alert_status': alerts
            },
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
    
    def print_status_dashboard(self):
        """æ‰“å°çŠ¶æ€ä»ªè¡¨æ¿"""
        health = self.get_system_health()
        
        # æ ‡é¢˜
        print("\n" + "="*80)
        print(f"{health['health_color']} DipMaster Data Infrastructure Status Dashboard")
        print(f"ğŸ“Š System Health: {health['health_level'].upper()} ({health['health_score']:.1%})")
        print(f"ğŸ• Last Update: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*80)
        
        # æ•°æ®è¦†ç›–çŠ¶æ€
        coverage = health['components']['data_coverage']
        print(f"\nğŸ“ DATA COVERAGE")
        print(f"   Files: {coverage['total_files']:,} / {coverage['expected_files']:,} ({coverage['coverage_percentage']:.1f}%)")
        print(f"   Size:  {coverage['total_size_gb']:.2f} GB")
        print(f"   Updated: {coverage.get('last_updated', 'Unknown')}")
        
        # æ•°æ®è´¨é‡çŠ¶æ€
        quality = health['components']['data_quality']
        print(f"\nğŸ“ˆ DATA QUALITY")
        if quality['status'] == 'active':
            metrics = quality['average_metrics']
            print(f"   Overall:     {metrics['overall_score']:.1%}")
            print(f"   Completeness: {metrics['completeness']:.1%}")
            print(f"   Consistency:  {metrics['consistency']:.1%}")
            print(f"   Accuracy:     {metrics['accuracy']:.1%}")
            print(f"   Assessments:  {metrics['assessments_24h']} (24h)")
        else:
            print(f"   Status: {quality.get('message', 'No data')}")
        
        # å‘Šè­¦çŠ¶æ€
        alerts = health['components']['alert_status']
        print(f"\nğŸš¨ ALERTS (24h)")
        if alerts['status'] == 'active':
            print(f"   Total: {alerts['total_alerts_24h']:,}")
            print(f"   Critical: {alerts['critical_alerts']:,}")
            
            if alerts['by_severity']:
                print(f"   By Severity: ", end="")
                severity_strs = [f"{sev}:{count}" for sev, count in alerts['by_severity'].items()]
                print(", ".join(severity_strs))
            
            if alerts['recent_alerts']:
                print(f"   Recent:")
                for alert in alerts['recent_alerts'][:3]:  # æ˜¾ç¤ºæœ€è¿‘3ä¸ª
                    print(f"     â€¢ {alert['symbol']} {alert['timeframe']}: {alert['message'][:50]}...")
        else:
            print(f"   Status: {alerts.get('message', 'No data')}")
        
        # TOPå¸ç§çŠ¶æ€
        if coverage['status'] == 'active' and coverage['by_symbol']:
            print(f"\nğŸ’° TOP SYMBOLS STATUS")
            
            # æŒ‰è¦†ç›–ç‡æ’åºæ˜¾ç¤ºå‰10å’Œå5
            symbol_items = list(coverage['by_symbol'].items())
            symbol_items.sort(key=lambda x: x[1]['coverage'], reverse=True)
            
            print("   Best Coverage:")
            for symbol, data in symbol_items[:5]:
                status_icon = "âœ…" if data['coverage'] == 100 else "âš ï¸" if data['coverage'] >= 50 else "âŒ"
                print(f"     {status_icon} {symbol}: {data['files']}/{data['expected']} ({data['coverage']:.0f}%)")
            
            if len(symbol_items) > 5:
                print("   Needs Attention:")
                for symbol, data in symbol_items[-3:]:
                    status_icon = "âœ…" if data['coverage'] == 100 else "âš ï¸" if data['coverage'] >= 50 else "âŒ"
                    print(f"     {status_icon} {symbol}: {data['files']}/{data['expected']} ({data['coverage']:.0f}%)")
        
        # ç³»ç»Ÿå»ºè®®
        print(f"\nğŸ’¡ RECOMMENDATIONS")
        if health['health_score'] >= 0.9:
            print("   ğŸ‰ System is performing excellently!")
            print("   â€¢ Continue monitoring for optimal performance")
        elif health['health_score'] >= 0.8:
            print("   âœ¨ System is performing well")
            print("   â€¢ Monitor for any quality degradation")
        elif health['health_score'] >= 0.6:
            print("   âš ï¸  System needs attention")
            if coverage['coverage_percentage'] < 80:
                print("   â€¢ Improve data coverage by downloading missing files")
            if alerts['critical_alerts'] > 0:
                print("   â€¢ Address critical alerts immediately")
        else:
            print("   ğŸš¨ System requires immediate attention!")
            print("   â€¢ Check system logs for errors")
            print("   â€¢ Consider running initial data collection")
            print("   â€¢ Verify exchange connectivity")
        
        print("\n" + "="*80 + "\n")
    
    def print_detailed_status(self):
        """æ‰“å°è¯¦ç»†çŠ¶æ€"""
        health = self.get_system_health()
        
        print(json.dumps(health, indent=2, default=str))
    
    def export_status_report(self) -> str:
        """å¯¼å‡ºçŠ¶æ€æŠ¥å‘Š"""
        health = self.get_system_health()
        
        report_file = f"data/infrastructure_status_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(health, f, indent=2, default=str)
        
        return report_file

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="DipMaster Data Infrastructure Status Dashboard")
    parser.add_argument('--json', action='store_true', help='è¾“å‡ºJSONæ ¼å¼')
    parser.add_argument('--export', action='store_true', help='å¯¼å‡ºçŠ¶æ€æŠ¥å‘Š')
    parser.add_argument('--watch', type=int, metavar='SECONDS', help='ç›‘è§†æ¨¡å¼ï¼Œæ¯Nç§’åˆ·æ–°')
    
    args = parser.parse_args()
    
    status_monitor = DataInfrastructureStatus()
    
    if args.export:
        report_file = status_monitor.export_status_report()
        print(f"çŠ¶æ€æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {report_file}")
        return
    
    if args.watch:
        try:
            while True:
                # æ¸…å± (åœ¨Unixç³»ç»Ÿä¸Š)
                import os
                os.system('clear' if os.name == 'posix' else 'cls')
                
                if args.json:
                    status_monitor.print_detailed_status()
                else:
                    status_monitor.print_status_dashboard()
                
                print(f"ğŸ”„ Refreshing in {args.watch} seconds... (Ctrl+C to stop)")
                time.sleep(args.watch)
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Monitoring stopped.")
            return
    
    if args.json:
        status_monitor.print_detailed_status()
    else:
        status_monitor.print_status_dashboard()

if __name__ == "__main__":
    main()