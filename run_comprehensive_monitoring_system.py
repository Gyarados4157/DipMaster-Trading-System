#!/usr/bin/env python3
"""
DipMaster Comprehensive Monitoring System Integration Runner
å…¨é¢ç›‘æ§ç³»ç»Ÿé›†æˆè¿è¡Œå™¨ - å®Œæ•´çš„ç›‘æ§ç”Ÿæ€ç³»ç»Ÿå¯åŠ¨å’Œé›†æˆè„šæœ¬

Features:
- Integrated startup of all monitoring components
- Configuration management and validation
- Health checks and failover capabilities
- Seamless integration with existing trading system
- Auto-recovery and monitoring system orchestration
- Production-ready deployment configuration

Author: DipMaster Trading System Monitoring Agent
Date: 2025-08-18
Version: 2.0.0
"""

import asyncio
import sys
import os
import json
import logging
import argparse
import signal
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime, timezone
import yaml

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.monitoring.comprehensive_monitoring_system import (
    ComprehensiveMonitoringSystem,
    create_comprehensive_monitoring_system,
    MonitoringMode
)
from src.monitoring.monitoring_dashboard_service import (
    MonitoringDashboardService,
    create_monitoring_dashboard_service
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('logs/comprehensive_monitoring.log', mode='a')
    ]
)
logger = logging.getLogger(__name__)


class MonitoringSystemOrchestrator:
    """
    ç›‘æ§ç³»ç»Ÿç¼–æ’å™¨
    
    è´Ÿè´£åè°ƒå¯åŠ¨ã€ç®¡ç†å’Œç›‘æ§æ‰€æœ‰ç›‘æ§ç³»ç»Ÿç»„ä»¶ï¼Œ
    æä¾›å®Œæ•´çš„ç›‘æ§ç”Ÿæ€ç³»ç»Ÿç®¡ç†ã€‚
    """
    
    def __init__(self, config_path: Optional[str] = None, mode: str = "development"):
        """
        åˆå§‹åŒ–ç›‘æ§ç³»ç»Ÿç¼–æ’å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            mode: è¿è¡Œæ¨¡å¼ (development, paper_trading, live_trading, research_only)
        """
        self.config_path = config_path
        self.mode = MonitoringMode(mode)
        self.config = {}
        self.is_running = False
        self.shutdown_event = asyncio.Event()
        
        # æ ¸å¿ƒç»„ä»¶
        self.monitoring_system: Optional[ComprehensiveMonitoringSystem] = None
        self.dashboard_service: Optional[MonitoringDashboardService] = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'start_time': None,
            'signals_processed': 0,
            'positions_tracked': 0,
            'executions_monitored': 0,
            'reports_generated': 0,
            'alerts_generated': 0,
            'uptime_seconds': 0
        }
        
        # åŠ è½½é…ç½®
        self._load_configuration()
        
        # è®¾ç½®ä¿¡å·å¤„ç†
        self._setup_signal_handlers()
        
        logger.info(f"ğŸš€ MonitoringSystemOrchestrator initialized in {self.mode.value} mode")
    
    def _load_configuration(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            # é»˜è®¤é…ç½®
            default_config = {
                'mode': self.mode.value,
                'kafka': {
                    'enabled': True,
                    'servers': ['localhost:9092'],
                    'client_id': 'dipmaster-monitoring',
                    'buffer_max_size': 1000
                },
                'database': {
                    'path': 'data/monitoring_production.db',
                    'backup_enabled': True,
                    'retention_days': 90
                },
                'logging': {
                    'level': 'INFO',
                    'directory': 'logs',
                    'max_file_size_mb': 100,
                    'retention_days': 30,
                    'structured_logging': True
                },
                'monitoring': {
                    'consistency_check_interval_seconds': 30,
                    'drift_detection_interval_seconds': 300,
                    'risk_monitoring_interval_seconds': 60,
                    'health_check_interval_seconds': 60,
                    'performance_snapshot_interval_seconds': 900,
                    'report_generation_enabled': True
                },
                'dashboard': {
                    'enabled': True,
                    'host': '0.0.0.0',
                    'port': 8080,
                    'websocket_port': 8081,
                    'cache_duration_seconds': 30,
                    'cors_enabled': True,
                    'start_server': True
                },
                'thresholds': {
                    'consistency': {
                        'signal_position_match_min': 95.0,
                        'position_execution_match_min': 98.0,
                        'price_deviation_max_bps': 20.0,
                        'timing_deviation_max_minutes': 2.0,
                        'boundary_compliance_min': 100.0
                    },
                    'drift': {
                        'warning_threshold_pct': 5.0,
                        'critical_threshold_pct': 10.0,
                        'statistical_significance': 0.05,
                        'min_data_points': 30
                    },
                    'risk': {
                        'var_95_limit': 200000.0,
                        'var_99_limit': 300000.0,
                        'max_drawdown_limit': 0.15,
                        'daily_loss_limit': 1000.0,
                        'position_limit_count': 10,
                        'leverage_limit': 3.0
                    },
                    'system_health': {
                        'cpu_warning': 80.0,
                        'memory_warning': 80.0,
                        'disk_warning': 85.0,
                        'response_time_warning_ms': 1000.0,
                        'error_rate_warning': 5.0
                    }
                },
                'dipmaster_params': {
                    'rsi_range': [30, 50],
                    'max_holding_minutes': 180,
                    'boundary_minutes': [15, 30, 45, 0],
                    'target_profit_pct': 0.8,
                    'dip_threshold_pct': 0.2,
                    'volume_multiplier': 1.5
                },
                'alerts': {
                    'email_notifications': False,
                    'webhook_url': None,
                    'suppression_enabled': True,
                    'escalation_enabled': True,
                    'auto_resolution_enabled': True
                },
                'backup': {
                    'enabled': True,
                    'interval_hours': 6,
                    'retention_days': 30,
                    'compress': True
                }
            }
            
            # ä»é…ç½®æ–‡ä»¶åŠ è½½
            if self.config_path and Path(self.config_path).exists():
                if self.config_path.endswith('.json'):
                    with open(self.config_path, 'r') as f:
                        file_config = json.load(f)
                elif self.config_path.endswith(('.yml', '.yaml')):
                    with open(self.config_path, 'r') as f:
                        file_config = yaml.safe_load(f)
                else:
                    raise ValueError(f"Unsupported config file format: {self.config_path}")
                
                # åˆå¹¶é…ç½®
                self.config = self._deep_merge_dicts(default_config, file_config)
                logger.info(f"âœ… Configuration loaded from {self.config_path}")
                
            else:
                self.config = default_config
                logger.info("âœ… Using default configuration")
            
            # éªŒè¯é…ç½®
            self._validate_configuration()
            
        except Exception as e:
            logger.error(f"âŒ Failed to load configuration: {e}")
            self.config = {}
            raise
    
    def _deep_merge_dicts(self, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
        """æ·±åº¦åˆå¹¶å­—å…¸"""
        result = base.copy()
        
        for key, value in override.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge_dicts(result[key], value)
            else:
                result[key] = value
        
        return result
    
    def _validate_configuration(self):
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        required_keys = ['mode', 'kafka', 'database', 'monitoring', 'thresholds']
        
        for key in required_keys:
            if key not in self.config:
                raise ValueError(f"Missing required configuration key: {key}")
        
        # åˆ›å»ºå¿…éœ€çš„ç›®å½•
        log_dir = Path(self.config['logging']['directory'])
        log_dir.mkdir(parents=True, exist_ok=True)
        
        db_path = Path(self.config['database']['path'])
        db_path.parent.mkdir(parents=True, exist_ok=True)
        
        logger.info("âœ… Configuration validation completed")
    
    def _setup_signal_handlers(self):
        """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
        def signal_handler(signum, frame):
            logger.info(f"ğŸ›‘ Received signal {signum}, initiating graceful shutdown...")
            asyncio.create_task(self._shutdown_gracefully())
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    
    async def start(self):
        """å¯åŠ¨å®Œæ•´çš„ç›‘æ§ç³»ç»Ÿ"""
        if self.is_running:
            logger.warning("âš ï¸ Monitoring system already running")
            return
        
        try:
            self.is_running = True
            self.stats['start_time'] = datetime.now(timezone.utc).timestamp()
            
            logger.info("ğŸš€ Starting DipMaster Comprehensive Monitoring System...")
            
            # 1. åˆå§‹åŒ–å’Œå¯åŠ¨æ ¸å¿ƒç›‘æ§ç³»ç»Ÿ
            await self._start_monitoring_system()
            
            # 2. åˆå§‹åŒ–å’Œå¯åŠ¨ä»ªè¡¨æ¿æœåŠ¡
            if self.config['dashboard']['enabled']:
                await self._start_dashboard_service()
            
            # 3. è¿è¡Œå¥åº·æ£€æŸ¥
            await self._perform_startup_health_check()
            
            # 4. å¯åŠ¨é›†æˆç›‘æ§å¾ªç¯
            await self._start_integration_monitoring()
            
            logger.info("âœ… DipMaster Comprehensive Monitoring System started successfully")
            
            # ç­‰å¾…å…³é—­ä¿¡å·
            await self.shutdown_event.wait()
            
        except Exception as e:
            logger.error(f"âŒ Failed to start monitoring system: {e}")
            self.is_running = False
            raise
    
    async def _start_monitoring_system(self):
        """å¯åŠ¨æ ¸å¿ƒç›‘æ§ç³»ç»Ÿ"""
        try:
            logger.info("ğŸ” Initializing comprehensive monitoring system...")
            
            monitoring_config = {
                'mode': self.config['mode'],
                'kafka': self.config['kafka'],
                'db_path': self.config['database']['path'],
                'log_dir': self.config['logging']['directory'],
                'thresholds': self.config['thresholds'],
                'dipmaster_params': self.config['dipmaster_params'],
                'baseline_performance_file': self.config.get('baseline_performance_file')
            }
            
            self.monitoring_system = create_comprehensive_monitoring_system(monitoring_config)
            await self.monitoring_system.start()
            
            logger.info("âœ… Comprehensive monitoring system started")
            
        except Exception as e:
            logger.error(f"âŒ Failed to start monitoring system: {e}")
            raise
    
    async def _start_dashboard_service(self):
        """å¯åŠ¨ä»ªè¡¨æ¿æœåŠ¡"""
        try:
            logger.info("ğŸ¯ Initializing monitoring dashboard service...")
            
            if not self.monitoring_system:
                raise RuntimeError("Monitoring system must be started before dashboard service")
            
            dashboard_config = self.config['dashboard'].copy()
            
            self.dashboard_service = create_monitoring_dashboard_service(
                self.monitoring_system,
                dashboard_config
            )
            
            await self.dashboard_service.start()
            
            logger.info(f"âœ… Monitoring dashboard service started on {dashboard_config['host']}:{dashboard_config['port']}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to start dashboard service: {e}")
            raise
    
    async def _perform_startup_health_check(self):
        """æ‰§è¡Œå¯åŠ¨å¥åº·æ£€æŸ¥"""
        try:
            logger.info("ğŸ¥ Performing startup health check...")
            
            # æ£€æŸ¥ç›‘æ§ç³»ç»ŸçŠ¶æ€
            if self.monitoring_system:
                system_status = await self.monitoring_system.get_system_status()
                if not system_status['is_running']:
                    raise RuntimeError("Monitoring system not running after startup")
                
                logger.info(f"âœ… Monitoring system health: {system_status['system_health_score']:.1f}%")
            
            # æ£€æŸ¥ä»ªè¡¨æ¿æœåŠ¡çŠ¶æ€
            if self.dashboard_service:
                dashboard_stats = self.dashboard_service.get_monitoring_statistics()
                logger.info(f"âœ… Dashboard service health: Active")
            
            # æ£€æŸ¥Kafkaè¿æ¥
            if self.config['kafka']['enabled'] and self.monitoring_system:
                if hasattr(self.monitoring_system, 'kafka_streamer') and self.monitoring_system.kafka_streamer:
                    kafka_stats = self.monitoring_system.kafka_streamer.get_event_stats()
                    if kafka_stats['is_running']:
                        logger.info("âœ… Kafka connection: Active")
                    else:
                        logger.warning("âš ï¸ Kafka connection: Inactive")
            
            logger.info("âœ… Startup health check completed")
            
        except Exception as e:
            logger.error(f"âŒ Startup health check failed: {e}")
            raise
    
    async def _start_integration_monitoring(self):
        """å¯åŠ¨é›†æˆç›‘æ§å¾ªç¯"""
        try:
            logger.info("ğŸ”„ Starting integration monitoring loop...")
            
            # å¯åŠ¨åå°ä»»åŠ¡
            background_tasks = [
                asyncio.create_task(self._integration_health_monitor()),
                asyncio.create_task(self._statistics_updater()),
                asyncio.create_task(self._backup_manager()),
                asyncio.create_task(self._log_rotator())
            ]
            
            # ç­‰å¾…ä»»åŠ¡å®Œæˆæˆ–å…³é—­ä¿¡å·
            done, pending = await asyncio.wait(
                background_tasks + [asyncio.create_task(self.shutdown_event.wait())],
                return_when=asyncio.FIRST_COMPLETED
            )
            
            # å–æ¶ˆå¾…å¤„ç†çš„ä»»åŠ¡
            for task in pending:
                task.cancel()
            
            logger.info("âœ… Integration monitoring loop completed")
            
        except Exception as e:
            logger.error(f"âŒ Integration monitoring loop failed: {e}")
    
    async def _integration_health_monitor(self):
        """é›†æˆå¥åº·ç›‘æ§å™¨"""
        while self.is_running:
            try:
                # æ£€æŸ¥ç»„ä»¶å¥åº·çŠ¶æ€
                if self.monitoring_system:
                    health_status = await self.monitoring_system.get_health_status()
                    if health_status['status'] != 'healthy':
                        logger.warning(f"âš ï¸ Monitoring system health degraded: {health_status}")
                
                # æ£€æŸ¥ä»ªè¡¨æ¿æœåŠ¡å¥åº·çŠ¶æ€
                if self.dashboard_service:
                    # ç®€å•çš„å¿ƒè·³æ£€æŸ¥
                    try:
                        stats = self.dashboard_service.get_monitoring_statistics()
                        if not stats:
                            logger.warning("âš ï¸ Dashboard service may be unresponsive")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Dashboard service health check failed: {e}")
                
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ Error in integration health monitor: {e}")
                await asyncio.sleep(30)
    
    async def _statistics_updater(self):
        """ç»Ÿè®¡ä¿¡æ¯æ›´æ–°å™¨"""
        while self.is_running:
            try:
                if self.monitoring_system:
                    system_stats = self.monitoring_system.get_monitoring_statistics()
                    
                    self.stats['signals_processed'] = system_stats['system_stats']['signals_validated']
                    self.stats['positions_tracked'] = system_stats['system_stats']['positions_tracked']
                    self.stats['executions_monitored'] = system_stats['system_stats']['executions_monitored']
                    self.stats['alerts_generated'] = system_stats['system_stats']['alerts_generated']
                    self.stats['reports_generated'] = system_stats['system_stats']['reports_generated']
                
                if self.stats['start_time']:
                    self.stats['uptime_seconds'] = datetime.now(timezone.utc).timestamp() - self.stats['start_time']
                
                await asyncio.sleep(30)  # æ¯30ç§’æ›´æ–°ç»Ÿè®¡
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ Error updating statistics: {e}")
                await asyncio.sleep(10)
    
    async def _backup_manager(self):
        """å¤‡ä»½ç®¡ç†å™¨"""
        if not self.config['backup']['enabled']:
            return
        
        backup_interval = self.config['backup']['interval_hours'] * 3600
        
        while self.is_running:
            try:
                await self._perform_backup()
                await asyncio.sleep(backup_interval)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ Error in backup manager: {e}")
                await asyncio.sleep(3600)  # é”™è¯¯å1å°æ—¶é‡è¯•
    
    async def _perform_backup(self):
        """æ‰§è¡Œå¤‡ä»½"""
        try:
            timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
            backup_dir = Path(f"backups/monitoring_{timestamp}")
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            # å¤‡ä»½æ•°æ®åº“
            db_path = Path(self.config['database']['path'])
            if db_path.exists():
                backup_db_path = backup_dir / f"monitoring_{timestamp}.db"
                import shutil
                shutil.copy2(db_path, backup_db_path)
                
                # å‹ç¼©å¤‡ä»½
                if self.config['backup']['compress']:
                    import gzip
                    with open(backup_db_path, 'rb') as f_in:
                        with gzip.open(f"{backup_db_path}.gz", 'wb') as f_out:
                            shutil.copyfileobj(f_in, f_out)
                    backup_db_path.unlink()  # åˆ é™¤æœªå‹ç¼©æ–‡ä»¶
                    
                logger.info(f"âœ… Database backup completed: {backup_dir}")
            
            # æ¸…ç†æ—§å¤‡ä»½
            await self._cleanup_old_backups()
            
        except Exception as e:
            logger.error(f"âŒ Failed to perform backup: {e}")
    
    async def _cleanup_old_backups(self):
        """æ¸…ç†æ—§å¤‡ä»½"""
        try:
            backup_root = Path("backups")
            if not backup_root.exists():
                return
            
            retention_days = self.config['backup']['retention_days']
            cutoff_time = datetime.now().timestamp() - (retention_days * 24 * 3600)
            
            for backup_dir in backup_root.iterdir():
                if backup_dir.is_dir() and backup_dir.stat().st_mtime < cutoff_time:
                    import shutil
                    shutil.rmtree(backup_dir)
                    logger.info(f"ğŸ—‘ï¸ Removed old backup: {backup_dir}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to cleanup old backups: {e}")
    
    async def _log_rotator(self):
        """æ—¥å¿—è½®è½¬å™¨"""
        while self.is_running:
            try:
                log_dir = Path(self.config['logging']['directory'])
                max_size = self.config['logging']['max_file_size_mb'] * 1024 * 1024
                retention_days = self.config['logging']['retention_days']
                
                # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å¤§å°
                for log_file in log_dir.glob("*.log"):
                    if log_file.stat().st_size > max_size:
                        # è½®è½¬æ—¥å¿—æ–‡ä»¶
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        rotated_name = log_file.with_suffix(f".{timestamp}.log")
                        log_file.rename(rotated_name)
                        logger.info(f"ğŸ“ Rotated log file: {log_file} -> {rotated_name}")
                
                # æ¸…ç†æ—§æ—¥å¿—
                cutoff_time = datetime.now().timestamp() - (retention_days * 24 * 3600)
                for log_file in log_dir.glob("*.log*"):
                    if log_file.stat().st_mtime < cutoff_time:
                        log_file.unlink()
                        logger.info(f"ğŸ—‘ï¸ Removed old log file: {log_file}")
                
                await asyncio.sleep(3600)  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ Error in log rotator: {e}")
                await asyncio.sleep(1800)  # é”™è¯¯å30åˆ†é’Ÿé‡è¯•
    
    async def stop(self):
        """åœæ­¢ç›‘æ§ç³»ç»Ÿ"""
        if not self.is_running:
            logger.warning("âš ï¸ Monitoring system not running")
            return
        
        try:
            logger.info("ğŸ›‘ Stopping DipMaster Comprehensive Monitoring System...")
            
            self.is_running = False
            
            # åœæ­¢ä»ªè¡¨æ¿æœåŠ¡
            if self.dashboard_service:
                await self.dashboard_service.stop()
                logger.info("âœ… Dashboard service stopped")
            
            # åœæ­¢ç›‘æ§ç³»ç»Ÿ
            if self.monitoring_system:
                await self.monitoring_system.stop()
                logger.info("âœ… Monitoring system stopped")
            
            # æœ€ç»ˆå¤‡ä»½
            if self.config['backup']['enabled']:
                logger.info("ğŸ“¦ Performing final backup...")
                await self._perform_backup()
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            await self._generate_shutdown_report()
            
            logger.info("âœ… DipMaster Comprehensive Monitoring System stopped successfully")
            
        except Exception as e:
            logger.error(f"âŒ Error stopping monitoring system: {e}")
    
    async def _shutdown_gracefully(self):
        """ä¼˜é›…å…³é—­"""
        try:
            await self.stop()
            self.shutdown_event.set()
        except Exception as e:
            logger.error(f"âŒ Error during graceful shutdown: {e}")
            self.shutdown_event.set()
    
    async def _generate_shutdown_report(self):
        """ç”Ÿæˆå…³é—­æŠ¥å‘Š"""
        try:
            uptime_hours = self.stats['uptime_seconds'] / 3600
            
            report = {
                'shutdown_time': datetime.now(timezone.utc).isoformat(),
                'total_uptime_hours': uptime_hours,
                'statistics': self.stats.copy(),
                'mode': self.mode.value,
                'final_status': 'clean_shutdown'
            }
            
            # ä¿å­˜æŠ¥å‘Š
            report_path = Path('reports/monitoring/shutdown_reports')
            report_path.mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
            report_file = report_path / f"shutdown_report_{timestamp}.json"
            
            with open(report_file, 'w') as f:
                json.dump(report, f, indent=2)
            
            logger.info(f"ğŸ“Š Shutdown report generated: {report_file}")
            logger.info(f"ğŸ“Š Final statistics: {self.stats['signals_processed']} signals, "
                       f"{self.stats['positions_tracked']} positions, "
                       f"{self.stats['alerts_generated']} alerts, "
                       f"{uptime_hours:.1f}h uptime")
            
        except Exception as e:
            logger.error(f"âŒ Failed to generate shutdown report: {e}")
    
    # Integration methods for existing trading system
    
    async def record_trading_signal(self, signal_data: Dict[str, Any]):
        """è®°å½•äº¤æ˜“ä¿¡å·ï¼ˆé›†æˆæ¥å£ï¼‰"""
        if self.monitoring_system:
            await self.monitoring_system.record_signal(signal_data)
    
    async def record_trading_position(self, position_data: Dict[str, Any]):
        """è®°å½•äº¤æ˜“æŒä»“ï¼ˆé›†æˆæ¥å£ï¼‰"""
        if self.monitoring_system:
            await self.monitoring_system.record_position(position_data)
    
    async def record_order_execution(self, execution_data: Dict[str, Any]):
        """è®°å½•è®¢å•æ‰§è¡Œï¼ˆé›†æˆæ¥å£ï¼‰"""
        if self.monitoring_system:
            await self.monitoring_system.record_execution(execution_data)
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€ï¼ˆé›†æˆæ¥å£ï¼‰"""
        return {
            'is_running': self.is_running,
            'mode': self.mode.value,
            'uptime_seconds': self.stats['uptime_seconds'],
            'statistics': self.stats.copy(),
            'components': {
                'monitoring_system': self.monitoring_system is not None,
                'dashboard_service': self.dashboard_service is not None
            }
        }


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="DipMaster Comprehensive Monitoring System")
    parser.add_argument(
        '--config',
        type=str,
        help='Configuration file path (JSON or YAML)'
    )
    parser.add_argument(
        '--mode',
        type=str,
        default='development',
        choices=['development', 'paper_trading', 'live_trading', 'research_only'],
        help='Running mode'
    )
    parser.add_argument(
        '--log-level',
        type=str,
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        help='Logging level'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    
    # åˆ›å»ºå¹¶å¯åŠ¨ç›‘æ§ç³»ç»Ÿç¼–æ’å™¨
    orchestrator = MonitoringSystemOrchestrator(args.config, args.mode)
    
    try:
        await orchestrator.start()
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Received keyboard interrupt, shutting down...")
    except Exception as e:
        logger.error(f"âŒ Fatal error: {e}")
        return 1
    finally:
        await orchestrator.stop()
    
    return 0


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))