#!/usr/bin/env python3
"""
å¤šå¸ç§2å¹´å†å²æ•°æ®ä¸‹è½½å™¨
ä¸‹è½½æŒ‡å®š9ä¸ªå¸ç§çš„2å¹´5åˆ†é’ŸKçº¿æ•°æ®ï¼Œç”¨äºDipMaster V3æ·±åº¦å›æµ‹

ç›®æ ‡å¸ç§:
- XRPUSDT, DOGEUSDT, ICPUSDT, IOTAUSDT
- SOLUSDT, SUIUSDT, ALGOUSDT, BNBUSDT, ADAUSDT
"""

import requests
import pandas as pd
import json
import time
from datetime import datetime, timedelta
from pathlib import Path
import logging
from typing import List, Dict, Optional
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'data_download_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class MultiSymbolDataDownloader:
    """å¤šå¸ç§å†å²æ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self):
        # ç›®æ ‡å¸ç§åˆ—è¡¨
        self.symbols = [
            'XRPUSDT', 'DOGEUSDT', 'ICPUSDT', 'IOTAUSDT',
            'SOLUSDT', 'SUIUSDT', 'ALGOUSDT', 'BNBUSDT', 'ADAUSDT'
        ]
        
        # Binance APIé…ç½®
        self.base_url = "https://api.binance.com"
        self.klines_endpoint = "/api/v3/klines"
        
        # æ•°æ®å‚æ•°
        self.interval = "5m"  # 5åˆ†é’ŸKçº¿
        self.limit = 1000     # æ¯æ¬¡è¯·æ±‚æœ€å¤§æ•°é‡
        
        # æ—¶é—´èŒƒå›´ï¼š2å¹´æ•°æ®
        self.end_time = datetime.now()
        self.start_time = self.end_time - timedelta(days=730)  # 2å¹´
        
        # è¾“å‡ºç›®å½•
        self.output_dir = Path("data/market_data")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.download_stats = {}
        
    def get_klines(self, symbol: str, start_time: int, end_time: int) -> List[List]:
        """è·å–Kçº¿æ•°æ®"""
        url = f"{self.base_url}{self.klines_endpoint}"
        params = {
            'symbol': symbol,
            'interval': self.interval,
            'startTime': start_time,
            'endTime': end_time,
            'limit': self.limit
        }
        
        try:
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"è·å–{symbol}æ•°æ®å¤±è´¥: {e}")
            return []
    
    def download_symbol_data(self, symbol: str) -> bool:
        """ä¸‹è½½å•ä¸ªå¸ç§çš„å®Œæ•´æ•°æ®"""
        logger.info(f"ğŸ“Š å¼€å§‹ä¸‹è½½ {symbol} æ•°æ®...")
        
        all_klines = []
        current_start = int(self.start_time.timestamp() * 1000)
        end_timestamp = int(self.end_time.timestamp() * 1000)
        
        request_count = 0
        
        while current_start < end_timestamp:
            # è®¡ç®—æœ¬æ¬¡è¯·æ±‚çš„ç»“æŸæ—¶é—´
            current_end = min(
                current_start + (self.limit * 5 * 60 * 1000),  # 5åˆ†é’Ÿ * limitæ¡ * æ¯«ç§’
                end_timestamp
            )
            
            # è·å–æ•°æ®
            klines = self.get_klines(symbol, current_start, current_end)
            
            if not klines:
                logger.warning(f"âš ï¸ {symbol}: æœªè·å–åˆ°æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {current_start} - {current_end}")
                break
            
            all_klines.extend(klines)
            request_count += 1
            
            # æ›´æ–°èµ·å§‹æ—¶é—´ä¸ºæœ€åä¸€æ¡æ•°æ®çš„æ—¶é—´+1åˆ†é’Ÿ
            if klines:
                last_time = klines[-1][0]  # å¼€ç›˜æ—¶é—´
                current_start = last_time + (5 * 60 * 1000)  # +5åˆ†é’Ÿ
                
                # è¿›åº¦æ˜¾ç¤º
                progress = (current_start - int(self.start_time.timestamp() * 1000)) / (end_timestamp - int(self.start_time.timestamp() * 1000)) * 100
                if request_count % 10 == 0:
                    logger.info(f"   {symbol} ä¸‹è½½è¿›åº¦: {progress:.1f}% ({len(all_klines)}æ¡æ•°æ®)")
            
            # APIé™åˆ¶ï¼šé¿å…è¯·æ±‚è¿‡é¢‘
            time.sleep(0.1)
            
            # å®‰å…¨é€€å‡º
            if request_count > 1000:  # é˜²æ­¢æ— é™å¾ªç¯
                logger.warning(f"âš ï¸ {symbol}: è¯·æ±‚æ¬¡æ•°è¿‡å¤šï¼Œåœæ­¢ä¸‹è½½")
                break
        
        if not all_klines:
            logger.error(f"âŒ {symbol}: æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            return False
        
        # è½¬æ¢ä¸ºDataFrame
        df = self.klines_to_dataframe(all_klines)
        
        if df.empty:
            logger.error(f"âŒ {symbol}: æ•°æ®è½¬æ¢å¤±è´¥")
            return False
        
        # æ•°æ®æ¸…ç†å’ŒéªŒè¯
        df = self.clean_data(df, symbol)
        
        if len(df) < 1000:
            logger.warning(f"âš ï¸ {symbol}: æ•°æ®é‡ä¸è¶³({len(df)}æ¡)ï¼Œå»ºè®®é‡æ–°ä¸‹è½½")
        
        # ä¿å­˜æ•°æ®
        output_file = self.output_dir / f"{symbol}_5m_2years.csv"
        df.to_csv(output_file, index=False)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'symbol': symbol,
            'interval': self.interval,
            'total_records': len(df),
            'start_time': str(df['timestamp'].min()),
            'end_time': str(df['timestamp'].max()),
            'download_time': datetime.now().isoformat(),
            'file_size_mb': output_file.stat().st_size / 1024 / 1024,
            'api_requests': request_count
        }
        
        metadata_file = self.output_dir / f"{symbol}_5m_2years_metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # æ›´æ–°ç»Ÿè®¡
        self.download_stats[symbol] = metadata
        
        logger.info(f"âœ… {symbol}: æˆåŠŸä¸‹è½½{len(df)}æ¡æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
        logger.info(f"   æ–‡ä»¶: {output_file}")
        
        return True
    
    def klines_to_dataframe(self, klines: List[List]) -> pd.DataFrame:
        """å°†Kçº¿æ•°æ®è½¬æ¢ä¸ºDataFrame"""
        if not klines:
            return pd.DataFrame()
        
        columns = [
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_asset_volume', 'number_of_trades',
            'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
        ]
        
        df = pd.DataFrame(klines, columns=columns)
        
        # æ•°æ®ç±»å‹è½¬æ¢
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')
        
        numeric_columns = ['open', 'high', 'low', 'close', 'volume', 
                          'quote_asset_volume', 'taker_buy_base_asset_volume', 
                          'taker_buy_quote_asset_volume']
        
        for col in numeric_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        df['number_of_trades'] = pd.to_numeric(df['number_of_trades'], errors='coerce')
        
        return df
    
    def clean_data(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """æ•°æ®æ¸…ç†"""
        initial_count = len(df)
        
        # ç§»é™¤é‡å¤æ•°æ®
        df = df.drop_duplicates(subset=['timestamp'])
        
        # ç§»é™¤ç©ºå€¼
        df = df.dropna(subset=['open', 'high', 'low', 'close', 'volume'])
        
        # ç§»é™¤ä»·æ ¼ä¸º0æˆ–è´Ÿæ•°çš„æ•°æ®
        price_columns = ['open', 'high', 'low', 'close']
        for col in price_columns:
            df = df[df[col] > 0]
        
        # ç§»é™¤å¼‚å¸¸ä»·æ ¼å˜åŠ¨ï¼ˆè¶…è¿‡50%çš„è·³å˜ï¼‰
        for col in price_columns:
            price_change = df[col].pct_change().abs()
            df = df[price_change < 0.5]
        
        # ç¡®ä¿OHLCé€»è¾‘æ­£ç¡®
        df = df[
            (df['high'] >= df['low']) & 
            (df['high'] >= df['open']) & 
            (df['high'] >= df['close']) &
            (df['low'] <= df['open']) & 
            (df['low'] <= df['close'])
        ]
        
        # æŒ‰æ—¶é—´æ’åº
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        cleaned_count = len(df)
        removed_count = initial_count - cleaned_count
        
        if removed_count > 0:
            logger.info(f"   {symbol}: æ¸…ç†äº†{removed_count}æ¡å¼‚å¸¸æ•°æ® ({removed_count/initial_count*100:.1f}%)")
        
        return df
    
    def download_all_symbols(self) -> Dict[str, bool]:
        """ä¸‹è½½æ‰€æœ‰å¸ç§æ•°æ®"""
        logger.info("ğŸš€ å¼€å§‹ä¸‹è½½æ‰€æœ‰å¸ç§çš„2å¹´å†å²æ•°æ®...")
        logger.info(f"ğŸ“Š ç›®æ ‡å¸ç§: {self.symbols}")
        logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {self.start_time.strftime('%Y-%m-%d')} ~ {self.end_time.strftime('%Y-%m-%d')}")
        logger.info(f"ğŸ”¢ æ•°æ®å‘¨æœŸ: {self.interval}")
        
        results = {}
        success_count = 0
        
        for i, symbol in enumerate(self.symbols, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ“ˆ [{i}/{len(self.symbols)}] å¤„ç† {symbol}")
            logger.info(f"{'='*60}")
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ•°æ®
            existing_file = self.output_dir / f"{symbol}_5m_2years.csv"
            if existing_file.exists():
                logger.info(f"âš ï¸ {symbol}: æ•°æ®æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
                logger.info(f"   å¦‚éœ€é‡æ–°ä¸‹è½½ï¼Œè¯·åˆ é™¤: {existing_file}")
                results[symbol] = True
                success_count += 1
                continue
            
            try:
                success = self.download_symbol_data(symbol)
                results[symbol] = success
                
                if success:
                    success_count += 1
                else:
                    logger.error(f"âŒ {symbol}: ä¸‹è½½å¤±è´¥")
                
            except Exception as e:
                logger.error(f"ğŸ’¥ {symbol}: ä¸‹è½½å¼‚å¸¸: {e}")
                results[symbol] = False
            
            # ä¸‹è½½é—´éš”ï¼Œé¿å…APIé™åˆ¶
            if i < len(self.symbols):
                logger.info("â³ ç­‰å¾…3ç§’åç»§ç»­ä¸‹è½½ä¸‹ä¸€ä¸ªå¸ç§...")
                time.sleep(3)
        
        # ä¸‹è½½æ€»ç»“
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ“Š ä¸‹è½½æ€»ç»“")
        logger.info(f"{'='*60}")
        logger.info(f"âœ… æˆåŠŸ: {success_count}/{len(self.symbols)} ä¸ªå¸ç§")
        logger.info(f"âŒ å¤±è´¥: {len(self.symbols) - success_count} ä¸ªå¸ç§")
        
        if success_count > 0:
            logger.info(f"\nğŸ“ æ•°æ®æ–‡ä»¶ä½ç½®: {self.output_dir}")
            
            # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
            total_records = sum(stats['total_records'] for stats in self.download_stats.values())
            total_size = sum(stats['file_size_mb'] for stats in self.download_stats.values())
            
            logger.info(f"ğŸ“ˆ æ€»æ•°æ®é‡: {total_records:,} æ¡Kçº¿")
            logger.info(f"ğŸ’¾ æ€»æ–‡ä»¶å¤§å°: {total_size:.2f} MB")
        
        # å¤±è´¥å¸ç§è¯¦æƒ…
        failed_symbols = [symbol for symbol, success in results.items() if not success]
        if failed_symbols:
            logger.warning(f"âš ï¸ ä»¥ä¸‹å¸ç§ä¸‹è½½å¤±è´¥: {failed_symbols}")
            logger.warning("   å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å¸ç§åç§°æ˜¯å¦æ­£ç¡®")
        
        return results
    
    def generate_summary_report(self) -> None:
        """ç”Ÿæˆä¸‹è½½æ‘˜è¦æŠ¥å‘Š"""
        if not self.download_stats:
            logger.warning("âš ï¸ æ²¡æœ‰ä¸‹è½½ç»Ÿè®¡æ•°æ®å¯ç”ŸæˆæŠ¥å‘Š")
            return
        
        summary = {
            'download_summary': {
                'total_symbols': len(self.symbols),
                'successful_downloads': len(self.download_stats),
                'failed_downloads': len(self.symbols) - len(self.download_stats),
                'download_completion_time': datetime.now().isoformat()
            },
            'symbol_details': self.download_stats,
            'aggregate_statistics': {
                'total_records': sum(stats['total_records'] for stats in self.download_stats.values()),
                'total_size_mb': sum(stats['file_size_mb'] for stats in self.download_stats.values()),
                'earliest_data': min(stats['start_time'] for stats in self.download_stats.values()),
                'latest_data': max(stats['end_time'] for stats in self.download_stats.values()),
                'total_api_requests': sum(stats['api_requests'] for stats in self.download_stats.values())
            }
        }
        
        # ä¿å­˜æ‘˜è¦æŠ¥å‘Š
        summary_file = self.output_dir / f"download_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜: {summary_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š å¤šå¸ç§2å¹´å†å²æ•°æ®ä¸‹è½½å™¨")
    print("=" * 60)
    print("ç›®æ ‡å¸ç§: XRPUSDT, DOGEUSDT, ICPUSDT, IOTAUSDT")
    print("          SOLUSDT, SUIUSDT, ALGOUSDT, BNBUSDT, ADAUSDT")
    print("æ•°æ®å‘¨æœŸ: 5åˆ†é’ŸKçº¿")
    print("æ—¶é—´èŒƒå›´: 2å¹´å†å²æ•°æ®")
    print("=" * 60)
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = MultiSymbolDataDownloader()
    
    # å¼€å§‹ä¸‹è½½
    results = downloader.download_all_symbols()
    
    # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
    downloader.generate_summary_report()
    
    # æ ¹æ®ç»“æœè¿”å›é€€å‡ºç 
    success_count = sum(1 for success in results.values() if success)
    total_count = len(results)
    
    if success_count == total_count:
        print(f"\nğŸ‰ æ‰€æœ‰{total_count}ä¸ªå¸ç§æ•°æ®ä¸‹è½½å®Œæˆï¼")
        return 0
    elif success_count > 0:
        print(f"\nâš ï¸ éƒ¨åˆ†æˆåŠŸï¼š{success_count}/{total_count}ä¸ªå¸ç§ä¸‹è½½å®Œæˆ")
        return 1
    else:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥ï¼šæ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•å¸ç§æ•°æ®")
        return 2

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)