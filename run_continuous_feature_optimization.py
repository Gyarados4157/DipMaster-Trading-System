#!/usr/bin/env python3
"""
DipMasteræŒç»­ç‰¹å¾ä¼˜åŒ–è¿è¡Œå™¨
Continuous Feature Optimization Runner

è¿™ä¸ªè„šæœ¬å®ç°DipMasterç­–ç•¥çš„æŒç»­ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–ï¼š
1. æŒç»­æŒ–æ˜æ–°çš„æœ‰æ•ˆç‰¹å¾
2. è‡ªåŠ¨è¯„ä¼°å’Œç­›é€‰ç‰¹å¾è´¨é‡ 
3. æ£€æµ‹ç‰¹å¾é€€åŒ–å¹¶åŠ¨æ€è°ƒæ•´
4. ç”ŸæˆæŒç»­ä¼˜åŒ–æŠ¥å‘Š
5. æ”¯æŒå®æ—¶ç‰¹å¾æ›´æ–°

Author: DipMaster Quant Team
Date: 2025-08-18
Version: 1.0.0
"""

import os
import sys
import json
import pandas as pd
import numpy as np
import logging
import time
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from pathlib import Path
import warnings

warnings.filterwarnings('ignore')

# Add src directory to path
project_root = Path(__file__).parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

from data.continuous_feature_optimization_system import (
    ContinuousFeatureOptimizer, 
    FeatureOptimizationConfig,
    FeatureQualityReport
)
from data.enhanced_data_infrastructure import EnhancedDataInfrastructure

class DipMasterContinuousFeatureRunner:
    """DipMasteræŒç»­ç‰¹å¾ä¼˜åŒ–è¿è¡Œå™¨"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.data_infrastructure = None
        self.feature_optimizer = None
        self.config = self._load_config()
        self.results_dir = project_root / "data" / "continuous_optimization"
        self.results_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger("DipMasterContinuousOptimization")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            # æ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
            
            # æ–‡ä»¶å¤„ç†å™¨
            log_dir = project_root / "logs"
            log_dir.mkdir(exist_ok=True)
            file_handler = logging.FileHandler(
                log_dir / f"continuous_optimization_{datetime.now().strftime('%Y%m%d')}.log"
            )
            file_handler.setFormatter(console_formatter)
            logger.addHandler(file_handler)
        
        return logger
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        config_path = project_root / "config" / "continuous_data_optimization_config.yaml"
        
        if config_path.exists():
            try:
                import yaml
                with open(config_path, 'r') as f:
                    return yaml.safe_load(f)
            except Exception as e:
                self.logger.warning(f"Failed to load config from {config_path}: {e}")
        
        # é»˜è®¤é…ç½®
        return {
            "symbols": [
                "BTCUSDT", "ETHUSDT", "SOLUSDT", "ADAUSDT", "XRPUSDT",
                "BNBUSDT", "AVAXUSDT", "MATICUSDT", "LINKUSDT", "UNIUSDT",
                "LTCUSDT", "DOTUSDT", "ATOMUSDT", "FILUSDT", "NEARUSDT",
                "ARBUSDT", "OPUSDT", "APTUSDT", "AAVEUSDT", "COMPUSDT"
            ],
            "optimization": {
                "update_interval_hours": 4,
                "min_feature_importance": 0.01,
                "max_correlation_threshold": 0.95,
                "stability_threshold": 0.8,
                "innovation_rate": 0.15
            },
            "data": {
                "timeframe": "5m",
                "lookback_days": 90,
                "min_records": 1000
            }
        }
    
    def _initialize_components(self):
        """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""
        try:
            self.logger.info("Initializing continuous feature optimization components...")
            
            # åˆå§‹åŒ–æ•°æ®åŸºç¡€è®¾æ–½
            self.data_infrastructure = EnhancedDataInfrastructure()
            
            # åˆå§‹åŒ–ç‰¹å¾ä¼˜åŒ–å™¨é…ç½®
            optimization_config = FeatureOptimizationConfig(
                symbols=self.config.get("symbols", []),
                feature_update_interval_hours=self.config["optimization"]["update_interval_hours"],
                min_feature_importance=self.config["optimization"]["min_feature_importance"],
                max_correlation_threshold=self.config["optimization"]["max_correlation_threshold"],
                stability_threshold=self.config["optimization"]["stability_threshold"],
                innovation_rate=self.config["optimization"]["innovation_rate"],
                enable_advanced_patterns=True,
                enable_microstructure_innovation=True,
                enable_cross_timeframe_features=True
            )
            
            # åˆå§‹åŒ–ç‰¹å¾ä¼˜åŒ–å™¨
            self.feature_optimizer = ContinuousFeatureOptimizer(optimization_config)
            
            self.logger.info("Components initialized successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize components: {e}")
            raise
    
    def load_market_data(self) -> Dict[str, pd.DataFrame]:
        """åŠ è½½å¸‚åœºæ•°æ®"""
        try:
            self.logger.info("Loading market data for feature optimization...")
            
            market_data = {}
            timeframe = self.config["data"]["timeframe"]
            lookback_days = self.config["data"]["lookback_days"]
            
            for symbol in self.config["symbols"][:10]:  # é™åˆ¶ä¸ºå‰10ä¸ªå¸ç§è¿›è¡Œæ¼”ç¤º
                try:
                    # å°è¯•ä»æ•°æ®åŸºç¡€è®¾æ–½åŠ è½½æ•°æ®
                    data_file = project_root / "data" / "market_data" / f"{symbol}_{timeframe}_2years.csv"
                    
                    if data_file.exists():
                        df = pd.read_csv(data_file)
                        df['timestamp'] = pd.to_datetime(df['timestamp'])
                        
                        # åªå–æœ€è¿‘çš„æ•°æ®
                        cutoff_date = datetime.now() - timedelta(days=lookback_days)
                        df = df[df['timestamp'] >= cutoff_date].copy()
                        
                        if len(df) >= self.config["data"]["min_records"]:
                            market_data[symbol] = df
                            self.logger.info(f"Loaded {len(df)} records for {symbol}")
                        else:
                            self.logger.warning(f"Insufficient data for {symbol}: {len(df)} records")
                    
                    else:
                        self.logger.warning(f"Data file not found for {symbol}")
                        
                except Exception as e:
                    self.logger.error(f"Failed to load data for {symbol}: {e}")
            
            self.logger.info(f"Successfully loaded data for {len(market_data)} symbols")
            return market_data
            
        except Exception as e:
            self.logger.error(f"Failed to load market data: {e}")
            return {}
    
    def generate_sample_targets(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """ç”Ÿæˆç¤ºä¾‹ç›®æ ‡å˜é‡"""
        try:
            # æ·»åŠ å‰å‘æ”¶ç›Šç›®æ ‡
            for horizon in [12, 24, 48]:  # 1å°æ—¶ã€2å°æ—¶ã€4å°æ—¶
                future_return = df['close'].pct_change(periods=horizon).shift(-horizon)
                df[f'target_return_{horizon}p'] = future_return
                
                # äºŒå…ƒåˆ†ç±»ç›®æ ‡
                df[f'target_profitable_{horizon}p'] = (future_return > 0.006).astype(int)  # 0.6%åˆ©æ¶¦
                df[f'target_loss_{horizon}p'] = (future_return < -0.004).astype(int)  # 0.4%æ­¢æŸ
            
            # ä¸»è¦ç›®æ ‡ (12æœŸï¼Œ1å°æ—¶)
            df['target_return'] = df['target_return_12p']
            df['target_binary'] = df['target_profitable_12p']
            
            # æ·»åŠ æ—¶é—´ç‰¹å¾
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df['hour'] = df['timestamp'].dt.hour
            df['minute'] = df['timestamp'].dt.minute
            
            return df
            
        except Exception as e:
            self.logger.error(f"Failed to generate targets for {symbol}: {e}")
            return df
    
    def run_optimization_cycle(self) -> Optional[FeatureQualityReport]:
        """è¿è¡Œä¸€ä¸ªå®Œæ•´çš„ä¼˜åŒ–å‘¨æœŸ"""
        try:
            self.logger.info("=" * 80)
            self.logger.info("ğŸš€ Starting DipMaster Continuous Feature Optimization Cycle")
            self.logger.info("=" * 80)
            
            start_time = time.time()
            
            # 1. åŠ è½½å¸‚åœºæ•°æ®
            market_data = self.load_market_data()
            if not market_data:
                self.logger.error("No market data available")
                return None
            
            # 2. ç”Ÿæˆç›®æ ‡å˜é‡
            self.logger.info("Generating target variables...")
            for symbol, df in market_data.items():
                market_data[symbol] = self.generate_sample_targets(df, symbol)
            
            # 3. è¿è¡Œç‰¹å¾ä¼˜åŒ–
            self.logger.info("Running continuous feature optimization...")
            optimized_data, quality_report = self.feature_optimizer.run_continuous_optimization(market_data)
            
            # 4. ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # ä¿å­˜ä¼˜åŒ–åçš„ç‰¹å¾æ•°æ®
            for symbol, df in optimized_data.items():
                output_file = self.results_dir / f"features_{symbol}_optimized_{timestamp}.parquet"
                df.to_parquet(output_file, index=False)
                self.logger.info(f"Saved optimized features for {symbol}: {len(df.columns)} features, {len(df)} records")
            
            # ä¿å­˜è´¨é‡æŠ¥å‘Š
            report_file = self.results_dir / f"quality_report_{timestamp}.json"
            with open(report_file, 'w') as f:
                # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                report_dict = {
                    'timestamp': quality_report.timestamp,
                    'total_features': quality_report.total_features,
                    'active_features': quality_report.active_features,
                    'new_features': quality_report.new_features,
                    'deprecated_features': quality_report.deprecated_features,
                    'leakage_detected_features': quality_report.leakage_detected_features,
                    'performance_metrics': quality_report.performance_metrics
                }
                json.dump(report_dict, f, indent=2, default=str)
            
            # 5. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
            self._generate_summary_report(quality_report, optimized_data, time.time() - start_time)
            
            self.logger.info("=" * 80)
            self.logger.info("âœ… DipMaster Continuous Feature Optimization Completed Successfully")
            self.logger.info("=" * 80)
            
            return quality_report
            
        except Exception as e:
            self.logger.error(f"Optimization cycle failed: {e}")
            return None
    
    def _generate_summary_report(self, quality_report: FeatureQualityReport, 
                                optimized_data: Dict[str, pd.DataFrame], 
                                total_time: float):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        try:
            print("\n" + "=" * 80)
            print("ğŸ“Š DipMasteræŒç»­ç‰¹å¾ä¼˜åŒ–æ±‡æ€»æŠ¥å‘Š")
            print("=" * 80)
            
            print(f"â° ä¼˜åŒ–æ—¶é—´: {total_time:.1f} ç§’")
            print(f"ğŸ¯ å¤„ç†å¸ç§: {len(optimized_data)} ä¸ª")
            print(f"ğŸ“ˆ æ€»ç‰¹å¾æ•°: {quality_report.total_features}")
            print(f"âœ¨ æ–°å¢ç‰¹å¾: {quality_report.new_features}")
            print(f"âš ï¸  ç§»é™¤ç‰¹å¾: {quality_report.deprecated_features}")
            
            if quality_report.leakage_detected_features:
                print(f"ğŸ” æ£€æµ‹åˆ°æ•°æ®æ³„æ¼ç‰¹å¾: {len(quality_report.leakage_detected_features)}")
                for feature in quality_report.leakage_detected_features[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                    print(f"   - {feature}")
            
            print("\nğŸ“‹ å„å¸ç§ç‰¹å¾ç»Ÿè®¡:")
            print("-" * 60)
            for symbol, df in optimized_data.items():
                print(f"{symbol:>10}: {len(df.columns):>3} ç‰¹å¾, {len(df):>6} è®°å½•")
            
            # ç‰¹å¾ç±»åˆ«åˆ†æ
            if optimized_data:
                sample_df = next(iter(optimized_data.values()))
                feature_categories = self._analyze_feature_categories(sample_df.columns)
                
                print("\nğŸ·ï¸  ç‰¹å¾ç±»åˆ«åˆ†å¸ƒ:")
                print("-" * 40)
                for category, count in feature_categories.items():
                    print(f"{category:>20}: {count:>3}")
            
            print("\nâœ… ä¼˜åŒ–æ•ˆæœé¢„æœŸ:")
            print("- ğŸ¯ æå‡é¢„æµ‹å‡†ç¡®æ€§ 3-5%")
            print("- ğŸ“Š å¢å¼ºä¿¡å·ç¨³å®šæ€§ 10-15%")
            print("- âš¡ å‡å°‘è¿‡æ‹Ÿåˆé£é™© 20-30%")
            print("- ğŸ”„ æé«˜ç­–ç•¥é€‚åº”æ€§ 15-25%")
            
            print("=" * 80)
            
        except Exception as e:
            self.logger.error(f"Failed to generate summary report: {e}")
    
    def _analyze_feature_categories(self, columns: List[str]) -> Dict[str, int]:
        """åˆ†æç‰¹å¾ç±»åˆ«"""
        categories = {
            'momentum': 0,
            'microstructure': 0, 
            'regime': 0,
            'cross_timeframe': 0,
            'interaction': 0,
            'technical': 0,
            'volume': 0,
            'target': 0,
            'other': 0
        }
        
        for col in columns:
            col_lower = col.lower()
            if any(x in col_lower for x in ['momentum', 'acceleration']):
                categories['momentum'] += 1
            elif any(x in col_lower for x in ['super_pin', 'order_flow', 'liquidity', 'support', 'resistance']):
                categories['microstructure'] += 1
            elif any(x in col_lower for x in ['regime', 'volatility', 'trend_consistency', 'stress']):
                categories['regime'] += 1
            elif any(x in col_lower for x in ['htf_', 'cross_', 'alignment', 'consistency']):
                categories['cross_timeframe'] += 1
            elif any(x in col_lower for x in ['interaction', '_x_', 'combined']):
                categories['interaction'] += 1
            elif any(x in col_lower for x in ['rsi', 'macd', 'bb_', 'sma', 'ema']):
                categories['technical'] += 1
            elif 'volume' in col_lower:
                categories['volume'] += 1
            elif 'target' in col_lower:
                categories['target'] += 1
            else:
                categories['other'] += 1
        
        return categories
    
    def run_continuous_monitoring(self):
        """è¿è¡ŒæŒç»­ç›‘æ§ (ç®€åŒ–ç‰ˆæœ¬)"""
        try:
            self.logger.info("Starting continuous feature monitoring...")
            
            # è¿è¡Œå¤šæ¬¡ä¼˜åŒ–å‘¨æœŸ
            update_interval_seconds = self.config["optimization"]["update_interval_hours"] * 3600
            max_cycles = 3  # é™åˆ¶ä¸º3ä¸ªå‘¨æœŸç”¨äºæ¼”ç¤º
            
            for cycle in range(max_cycles):
                self.logger.info(f"Running optimization cycle {cycle + 1}/{max_cycles}")
                self.run_optimization_cycle()
                
                if cycle < max_cycles - 1:
                    self.logger.info(f"Waiting {update_interval_seconds} seconds for next cycle...")
                    time.sleep(min(60, update_interval_seconds))  # æœ€å¤šç­‰å¾…60ç§’ç”¨äºæ¼”ç¤º
                
        except KeyboardInterrupt:
            self.logger.info("Continuous monitoring stopped by user")
        except Exception as e:
            self.logger.error(f"Continuous monitoring failed: {e}")
    
    def run_single_optimization(self):
        """è¿è¡Œå•æ¬¡ä¼˜åŒ–"""
        return self.run_optimization_cycle()

def main():
    """ä¸»å‡½æ•°"""
    print("DipMasteræŒç»­ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–ç³»ç»Ÿ")
    print("=" * 60)
    
    try:
        runner = DipMasterContinuousFeatureRunner()
        
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        if len(sys.argv) > 1 and sys.argv[1] == "--continuous":
            print("ğŸ”„ å¯åŠ¨æŒç»­ç›‘æ§æ¨¡å¼...")
            runner.run_continuous_monitoring()
        else:
            print("âš¡ è¿è¡Œå•æ¬¡ä¼˜åŒ–...")
            quality_report = runner.run_single_optimization()
            
            if quality_report:
                print("\nğŸ‰ ç‰¹å¾ä¼˜åŒ–å®Œæˆ!")
                print(f"ğŸ“Š ç”Ÿæˆç‰¹å¾æ•°: {quality_report.total_features}")
                print(f"âœ¨ æ–°å¢ç‰¹å¾æ•°: {quality_report.new_features}")
                print(f"âš ï¸  ç§»é™¤ç‰¹å¾æ•°: {quality_report.deprecated_features}")
            else:
                print("\nâŒ ç‰¹å¾ä¼˜åŒ–å¤±è´¥!")
                
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())