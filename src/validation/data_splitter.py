#!/usr/bin/env python3
"""
ä¸¥æ ¼æ•°æ®åˆ†å‰²å™¨ - è§£å†³è¿‡æ‹Ÿåˆçš„æ ¸å¿ƒç»„ä»¶
Strict Data Splitter - Core Component for Overfitting Prevention

æ ¸å¿ƒåŸåˆ™:
1. æ—¶é—´é¡ºåºåˆ†å‰² (60% è®­ç»ƒ / 20% éªŒè¯ / 20% æµ‹è¯•)
2. æµ‹è¯•é›†ç»å¯¹ä¸å¯è§¦ç¢°åŸåˆ™
3. æ¶ˆé™¤é€‰æ‹©åå·®ï¼Œæ‰€æœ‰å¸ç§ä¸€è‡´éªŒè¯
4. æ•°æ®æ³„æ¼é˜²æŠ¤

Author: DipMaster Trading Team
Date: 2025-08-15
Version: 1.0.0
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from dataclasses import dataclass
import json

logger = logging.getLogger(__name__)

@dataclass
class DataSplitConfig:
    """æ•°æ®åˆ†å‰²é…ç½®"""
    train_ratio: float = 0.60
    val_ratio: float = 0.20
    test_ratio: float = 0.20
    min_train_samples: int = 50000  # æœ€å°‘è®­ç»ƒæ ·æœ¬
    min_val_samples: int = 10000    # æœ€å°‘éªŒè¯æ ·æœ¬
    min_test_samples: int = 10000   # æœ€å°‘æµ‹è¯•æ ·æœ¬
    symbols: List[str] = None       # å¸ç§åˆ—è¡¨

@dataclass
class DataSplit:
    """æ•°æ®åˆ†å‰²ç»“æœ"""
    train_start: datetime
    train_end: datetime
    val_start: datetime
    val_end: datetime
    test_start: datetime
    test_end: datetime
    train_samples: int
    val_samples: int
    test_samples: int
    split_timestamp: datetime
    integrity_hash: str

class DataSplitter:
    """
    ä¸¥æ ¼æ•°æ®åˆ†å‰²å™¨
    
    æ ¸å¿ƒåŠŸèƒ½:
    1. æ—¶é—´é¡ºåºä¸¥æ ¼åˆ†å‰²
    2. é˜²æ­¢æ•°æ®æ³„æ¼
    3. æ¶ˆé™¤é€‰æ‹©åå·®
    4. åˆ†å‰²å®Œæ•´æ€§éªŒè¯
    """
    
    def __init__(self, config: DataSplitConfig = None):
        self.config = config or DataSplitConfig()
        self.splits: Dict[str, DataSplit] = {}
        self.lock_file_path = Path("data/validation/SPLIT_LOCK.json")
        self.split_metadata_path = Path("data/validation/split_metadata.json")
        
        # ç¡®ä¿éªŒè¯ç›®å½•å­˜åœ¨
        self.validation_dir = Path("data/validation")
        self.validation_dir.mkdir(parents=True, exist_ok=True)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå¸ç§ï¼Œä½¿ç”¨æ ‡å‡†9å¸ç§
        if self.config.symbols is None:
            self.config.symbols = [
                'BTCUSDT', 'ADAUSDT', 'ALGOUSDT', 'BNBUSDT', 'DOGEUSDT',
                'ICPUSDT', 'IOTAUSDT', 'SOLUSDT', 'SUIUSDT', 'XRPUSDT'
            ]
    
    def create_strict_split(self, symbol: str, data_path: str) -> DataSplit:
        """
        åˆ›å»ºä¸¥æ ¼çš„æ—¶é—´é¡ºåºåˆ†å‰²
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            DataSplit: åˆ†å‰²ç»“æœ
        """
        logger.info(f"ä¸º {symbol} åˆ›å»ºä¸¥æ ¼æ•°æ®åˆ†å‰²...")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨é”å®šçš„åˆ†å‰²
        if self._is_split_locked():
            logger.warning("å‘ç°å·²é”å®šçš„æ•°æ®åˆ†å‰²ï¼ŒåŠ è½½ç°æœ‰åˆ†å‰²...")
            return self._load_locked_split(symbol)
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(data_path)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.sort_values('timestamp')
        
        total_samples = len(df)
        logger.info(f"{symbol} æ€»æ ·æœ¬æ•°: {total_samples}")
        
        # éªŒè¯æœ€å°æ ·æœ¬è¦æ±‚
        if not self._validate_minimum_samples(total_samples):
            raise ValueError(f"æ•°æ®é‡ä¸è¶³: {total_samples} < æœ€å°è¦æ±‚")
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        train_end_idx = int(total_samples * self.config.train_ratio)
        val_end_idx = int(total_samples * (self.config.train_ratio + self.config.val_ratio))
        
        # è·å–æ—¶é—´æˆ³
        train_start = df.iloc[0]['timestamp']
        train_end = df.iloc[train_end_idx - 1]['timestamp']
        val_start = df.iloc[train_end_idx]['timestamp']
        val_end = df.iloc[val_end_idx - 1]['timestamp']
        test_start = df.iloc[val_end_idx]['timestamp']
        test_end = df.iloc[-1]['timestamp']
        
        # åˆ›å»ºåˆ†å‰²å¯¹è±¡
        split = DataSplit(
            train_start=train_start,
            train_end=train_end,
            val_start=val_start,
            val_end=val_end,
            test_start=test_start,
            test_end=test_end,
            train_samples=train_end_idx,
            val_samples=val_end_idx - train_end_idx,
            test_samples=total_samples - val_end_idx,
            split_timestamp=datetime.now(),
            integrity_hash=self._calculate_integrity_hash(df)
        )
        
        logger.info(f"æ•°æ®åˆ†å‰²å®Œæˆ:")
        logger.info(f"  è®­ç»ƒé›†: {train_start} -> {train_end} ({split.train_samples} æ ·æœ¬)")
        logger.info(f"  éªŒè¯é›†: {val_start} -> {val_end} ({split.val_samples} æ ·æœ¬)")
        logger.info(f"  æµ‹è¯•é›†: {test_start} -> {test_end} ({split.test_samples} æ ·æœ¬)")
        
        return split
    
    def split_all_symbols(self, data_dir: str = "data/market_data") -> Dict[str, DataSplit]:
        """
        ä¸ºæ‰€æœ‰å¸ç§åˆ›å»ºä¸€è‡´çš„æ•°æ®åˆ†å‰²
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            
        Returns:
            Dict[str, DataSplit]: æ‰€æœ‰å¸ç§çš„åˆ†å‰²ç»“æœ
        """
        logger.info("å¼€å§‹ä¸ºæ‰€æœ‰å¸ç§åˆ›å»ºä¸€è‡´çš„æ•°æ®åˆ†å‰²...")
        
        data_path = Path(data_dir)
        splits = {}
        
        for symbol in self.config.symbols:
            # æŸ¥æ‰¾5åˆ†é’Ÿæ•°æ®æ–‡ä»¶
            symbol_file = f"{symbol}_5m_2years.csv"
            file_path = data_path / symbol_file
            
            if not file_path.exists():
                logger.warning(f"æœªæ‰¾åˆ° {symbol} çš„æ•°æ®æ–‡ä»¶: {file_path}")
                continue
            
            try:
                split = self.create_strict_split(symbol, str(file_path))
                splits[symbol] = split
                
                # ä¿å­˜å•ä¸ªå¸ç§åˆ†å‰²
                self._save_symbol_split(symbol, split)
                
            except Exception as e:
                logger.error(f"ä¸º {symbol} åˆ›å»ºåˆ†å‰²å¤±è´¥: {e}")
                continue
        
        # éªŒè¯æ‰€æœ‰åˆ†å‰²çš„ä¸€è‡´æ€§
        self._validate_split_consistency(splits)
        
        # é”å®šåˆ†å‰²ï¼ˆé˜²æ­¢åç»­ä¿®æ”¹ï¼‰
        self._lock_splits(splits)
        
        logger.info(f"æˆåŠŸä¸º {len(splits)} ä¸ªå¸ç§åˆ›å»ºæ•°æ®åˆ†å‰²")
        return splits
    
    def get_split_data(self, symbol: str, split_type: str, data_path: str) -> pd.DataFrame:
        """
        è·å–æŒ‡å®šåˆ†å‰²çš„æ•°æ®
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            split_type: åˆ†å‰²ç±»å‹ ('train', 'val', 'test')
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            pd.DataFrame: åˆ†å‰²åçš„æ•°æ®
        """
        if symbol not in self.splits:
            raise ValueError(f"æœªæ‰¾åˆ° {symbol} çš„åˆ†å‰²ä¿¡æ¯")
        
        split = self.splits[symbol]
        
        # åŠ è½½å®Œæ•´æ•°æ®
        df = pd.read_csv(data_path)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.sort_values('timestamp')
        
        # æ ¹æ®åˆ†å‰²ç±»å‹è¿”å›æ•°æ®
        if split_type == 'train':
            return df[(df['timestamp'] >= split.train_start) & 
                     (df['timestamp'] <= split.train_end)]
        elif split_type == 'val':
            return df[(df['timestamp'] >= split.val_start) & 
                     (df['timestamp'] <= split.val_end)]
        elif split_type == 'test':
            # ğŸš¨ é‡è¦è­¦å‘Šï¼šæµ‹è¯•é›†è®¿é—®è®°å½•
            self._log_test_access(symbol)
            return df[(df['timestamp'] >= split.test_start) & 
                     (df['timestamp'] <= split.test_end)]
        else:
            raise ValueError(f"æ— æ•ˆçš„åˆ†å‰²ç±»å‹: {split_type}")
    
    def _validate_minimum_samples(self, total_samples: int) -> bool:
        """éªŒè¯æœ€å°æ ·æœ¬è¦æ±‚"""
        required_samples = (self.config.min_train_samples + 
                          self.config.min_val_samples + 
                          self.config.min_test_samples)
        return total_samples >= required_samples
    
    def _calculate_integrity_hash(self, df: pd.DataFrame) -> str:
        """è®¡ç®—æ•°æ®å®Œæ•´æ€§å“ˆå¸Œ"""
        import hashlib
        data_str = f"{len(df)}_{df.iloc[0]['timestamp']}_{df.iloc[-1]['timestamp']}"
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def _validate_split_consistency(self, splits: Dict[str, DataSplit]) -> None:
        """éªŒè¯æ‰€æœ‰åˆ†å‰²çš„æ—¶é—´ä¸€è‡´æ€§"""
        logger.info("éªŒè¯åˆ†å‰²ä¸€è‡´æ€§...")
        
        if not splits:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„åˆ†å‰²æ•°æ®")
        
        # æ£€æŸ¥æ—¶é—´èŒƒå›´ä¸€è‡´æ€§
        first_split = list(splits.values())[0]
        base_train_period = (first_split.train_end - first_split.train_start).days
        base_val_period = (first_split.val_end - first_split.val_start).days
        base_test_period = (first_split.test_end - first_split.test_start).days
        
        for symbol, split in splits.items():
            train_period = (split.train_end - split.train_start).days
            val_period = (split.val_end - split.val_start).days
            test_period = (split.test_end - split.test_start).days
            
            if abs(train_period - base_train_period) > 7:  # å…è®¸7å¤©è¯¯å·®
                logger.warning(f"{symbol} è®­ç»ƒæœŸä¸ä¸€è‡´: {train_period} vs {base_train_period}")
            
            if abs(val_period - base_val_period) > 7:
                logger.warning(f"{symbol} éªŒè¯æœŸä¸ä¸€è‡´: {val_period} vs {base_val_period}")
            
            if abs(test_period - base_test_period) > 7:
                logger.warning(f"{symbol} æµ‹è¯•æœŸä¸ä¸€è‡´: {test_period} vs {base_test_period}")
        
        logger.info("åˆ†å‰²ä¸€è‡´æ€§éªŒè¯å®Œæˆ")
    
    def _is_split_locked(self) -> bool:
        """æ£€æŸ¥åˆ†å‰²æ˜¯å¦å·²é”å®š"""
        return self.lock_file_path.exists()
    
    def _lock_splits(self, splits: Dict[str, DataSplit]) -> None:
        """é”å®šåˆ†å‰²ï¼Œé˜²æ­¢åç»­ä¿®æ”¹"""
        lock_data = {
            'locked_at': datetime.now().isoformat(),
            'symbols': list(splits.keys()),
            'warning': 'ğŸš¨ æµ‹è¯•é›†å·²é”å®šï¼ç¦æ­¢ä»»ä½•ä¿®æ”¹ï¼',
            'test_access_log': []
        }
        
        with open(self.lock_file_path, 'w') as f:
            json.dump(lock_data, f, indent=2)
        
        logger.warning("ğŸ”’ æ•°æ®åˆ†å‰²å·²é”å®šï¼æµ‹è¯•é›†ä¸å¯å†æ¬¡è®¿é—®ç”¨äºä¼˜åŒ–ï¼")
    
    def _load_locked_split(self, symbol: str) -> DataSplit:
        """åŠ è½½å·²é”å®šçš„åˆ†å‰²"""
        split_file = self.validation_dir / f"{symbol}_split.json"
        if not split_file.exists():
            raise ValueError(f"æœªæ‰¾åˆ° {symbol} çš„é”å®šåˆ†å‰²")
        
        with open(split_file) as f:
            data = json.load(f)
        
        return DataSplit(**data)
    
    def _save_symbol_split(self, symbol: str, split: DataSplit) -> None:
        """ä¿å­˜å•ä¸ªå¸ç§çš„åˆ†å‰²ä¿¡æ¯"""
        split_file = self.validation_dir / f"{symbol}_split.json"
        
        split_data = {
            'train_start': split.train_start.isoformat(),
            'train_end': split.train_end.isoformat(),
            'val_start': split.val_start.isoformat(),
            'val_end': split.val_end.isoformat(),
            'test_start': split.test_start.isoformat(),
            'test_end': split.test_end.isoformat(),
            'train_samples': split.train_samples,
            'val_samples': split.val_samples,
            'test_samples': split.test_samples,
            'split_timestamp': split.split_timestamp.isoformat(),
            'integrity_hash': split.integrity_hash
        }
        
        with open(split_file, 'w') as f:
            json.dump(split_data, f, indent=2)
    
    def _log_test_access(self, symbol: str) -> None:
        """è®°å½•æµ‹è¯•é›†è®¿é—®ï¼ˆé‡è¦çš„å®¡è®¡åŠŸèƒ½ï¼‰"""
        access_log = {
            'symbol': symbol,
            'accessed_at': datetime.now().isoformat(),
            'warning': 'âš ï¸  æµ‹è¯•é›†è¢«è®¿é—®ï¼ç¡®ä¿è¿™æ˜¯æœ€ç»ˆéªŒè¯ï¼'
        }
        
        log_file = self.validation_dir / "test_access_log.json"
        
        if log_file.exists():
            with open(log_file) as f:
                logs = json.load(f)
        else:
            logs = []
        
        logs.append(access_log)
        
        with open(log_file, 'w') as f:
            json.dump(logs, f, indent=2)
        
        logger.critical(f"ğŸš¨ æµ‹è¯•é›†è®¿é—®è®°å½•: {symbol} - ç¡®ä¿è¿™æ˜¯æœ€ç»ˆéªŒè¯ï¼")

    def get_split_summary(self) -> Dict:
        """è·å–åˆ†å‰²æ‘˜è¦"""
        if not self.splits:
            return {"error": "æ²¡æœ‰å¯ç”¨çš„åˆ†å‰²æ•°æ®"}
        
        summary = {
            'total_symbols': len(self.splits),
            'split_ratios': {
                'train': self.config.train_ratio,
                'val': self.config.val_ratio,
                'test': self.config.test_ratio
            },
            'symbols_detail': {}
        }
        
        for symbol, split in self.splits.items():
            summary['symbols_detail'][symbol] = {
                'train_period': f"{split.train_start.date()} to {split.train_end.date()}",
                'val_period': f"{split.val_start.date()} to {split.val_end.date()}",
                'test_period': f"{split.test_start.date()} to {split.test_end.date()}",
                'sample_counts': {
                    'train': split.train_samples,
                    'val': split.val_samples,
                    'test': split.test_samples
                }
            }
        
        return summary