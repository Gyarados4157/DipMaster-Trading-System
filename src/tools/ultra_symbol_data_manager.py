#!/usr/bin/env python3
"""
Ultra Symbol Data Manager - æ‰©å±•å¸ç§æ•°æ®ç®¡ç†å™¨
==============================================

åŠŸèƒ½ï¼š
1. ä¸‹è½½æ‰©å±•çš„ä¼˜è´¨å¸ç§æ•°æ®ï¼ˆé¿å¼€BTC/ETHï¼‰
2. æ•°æ®è´¨é‡æ£€æŸ¥å’Œæ¸…æ´—
3. å®æ—¶æ•°æ®æ›´æ–°å’Œç»´æŠ¤
4. å¸ç§è¯„çº§å’Œç­›é€‰

ç›®æ ‡å¸ç§æ± ï¼š
- Tier 1: MATIC, DOT, AVAX, LINK, NEAR, ATOM (é«˜æµåŠ¨æ€§Layer1/DeFi)
- Tier 2: UNI, VET, XLM, FTM (ä¸»æµå¸ç§)
- Tier 3: SAND, MANA, CHZ, ENJ, GALA (æ¸¸æˆ/å…ƒå®‡å®™)

Author: DipMaster Ultra Team
Date: 2025-08-15
Version: 1.0.0
"""

import os
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Set
from dataclasses import dataclass, field
import asyncio
import logging
import time
from binance.client import Client
import requests
from pathlib import Path

logger = logging.getLogger(__name__)


@dataclass
class SymbolInfo:
    """å¸ç§ä¿¡æ¯"""
    symbol: str
    tier: int                    # 1-3çº§åˆ†çº§
    market_cap_rank: int = 0     # å¸‚å€¼æ’å
    daily_volume_24h: float = 0  # 24å°æ—¶æˆäº¤é‡
    price_precision: int = 4     # ä»·æ ¼ç²¾åº¦
    quantity_precision: int = 6  # æ•°é‡ç²¾åº¦
    min_notional: float = 10     # æœ€å°è®¢å•ä»·å€¼
    is_active: bool = True       # æ˜¯å¦æ´»è·ƒäº¤æ˜“
    last_updated: datetime = field(default_factory=datetime.now)


@dataclass  
class DataQuality:
    """æ•°æ®è´¨é‡è¯„ä¼°"""
    symbol: str
    completeness: float = 0.0    # æ•°æ®å®Œæ•´æ€§ 0-1
    gap_count: int = 0          # æ•°æ®ç¼ºå£æ•°é‡
    price_anomaly_count: int = 0 # ä»·æ ¼å¼‚å¸¸æ•°é‡
    volume_anomaly_count: int = 0# æˆäº¤é‡å¼‚å¸¸æ•°é‡
    quality_score: float = 0.0   # ç»¼åˆè´¨é‡è¯„åˆ† 0-100
    issues: List[str] = field(default_factory=list)  # é—®é¢˜åˆ—è¡¨


class UltraSymbolDataManager:
    """è¶…çº§å¸ç§æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, data_dir: str = None):
        self.data_dir = Path(data_dir) if data_dir else Path("data/market_data")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # Binanceå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨å…è´¹APIï¼‰
        self.client = Client()  # ä¸éœ€è¦APIå¯†é’¥è·å–å¸‚åœºæ•°æ®
        
        # æ‰©å±•çš„å¸ç§æ± å®šä¹‰
        self.tier_1_symbols = [
            "MATICUSDT", "DOTUSDT", "AVAXUSDT", "LINKUSDT", 
            "NEARUSDT", "ATOMUSDT", "FTMUSDT"
        ]
        
        self.tier_2_symbols = [
            "UNIUSDT", "VETUSDT", "XLMUSDT", "HBARUSDT",
            "ARUSDT", "IMXUSDT", "FLOWUSDT"
        ]
        
        self.tier_3_symbols = [
            "SANDUSDT", "MANAUSDT", "CHZUSDT", "ENJUSDT",
            "GALAUSDT", "AXSUSDT"
        ]
        
        # å·²æœ‰çš„å¸ç§ï¼ˆä¸é‡å¤ä¸‹è½½ï¼‰
        self.existing_symbols = [
            "DOGEUSDT", "IOTAUSDT", "SOLUSDT", "SUIUSDT", 
            "ALGOUSDT", "BNBUSDT", "ADAUSDT", "XRPUSDT", "ICPUSDT"
        ]
        
        # å¸ç§ä¿¡æ¯å­˜å‚¨
        self.symbol_info: Dict[str, SymbolInfo] = {}
        self.data_quality: Dict[str, DataQuality] = {}
        
        # æ•°æ®ä¸‹è½½é…ç½®
        self.timeframe = "5m"
        self.lookback_days = 730  # 2å¹´æ•°æ®
        self.batch_size = 1000    # æ¯æ‰¹æ¬¡ä¸‹è½½æ•°é‡
        self.request_delay = 0.1  # APIè¯·æ±‚å»¶è¿Ÿ
        
    def initialize_symbol_pool(self):
        """åˆå§‹åŒ–å¸ç§æ± """
        logger.info("ğŸ”„ Initializing expanded symbol pool...")
        
        # è·å–äº¤æ˜“æ‰€äº¤æ˜“ä¿¡æ¯
        exchange_info = self.client.get_exchange_info()
        symbols_info = {s['symbol']: s for s in exchange_info['symbols']}
        
        # åˆå§‹åŒ–å„çº§åˆ«å¸ç§
        for tier, symbols in enumerate([
            self.tier_1_symbols, 
            self.tier_2_symbols, 
            self.tier_3_symbols
        ], 1):
            for symbol in symbols:
                if symbol in symbols_info:
                    info = symbols_info[symbol]
                    
                    # æå–ç²¾åº¦å’Œæœ€å°è®¢å•ä¿¡æ¯
                    price_precision = len(str(info['filters'][0]['tickSize']).split('.')[-1].rstrip('0'))
                    quantity_precision = len(str(info['filters'][2]['stepSize']).split('.')[-1].rstrip('0'))
                    min_notional = float(info['filters'][3]['minNotional'])
                    
                    self.symbol_info[symbol] = SymbolInfo(
                        symbol=symbol,
                        tier=tier,
                        price_precision=price_precision,
                        quantity_precision=quantity_precision,
                        min_notional=min_notional,
                        is_active=info['status'] == 'TRADING'
                    )
                else:
                    logger.warning(f"âŒ Symbol {symbol} not found on Binance")
                    
        # è·å–24å°æ—¶ç»Ÿè®¡
        self._update_market_stats()
        
        logger.info(f"âœ… Symbol pool initialized: {len(self.symbol_info)} symbols")
        logger.info(f"  â€¢ Tier 1: {len(self.tier_1_symbols)} symbols")
        logger.info(f"  â€¢ Tier 2: {len(self.tier_2_symbols)} symbols") 
        logger.info(f"  â€¢ Tier 3: {len(self.tier_3_symbols)} symbols")
        
    def _update_market_stats(self):
        """æ›´æ–°å¸‚åœºç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è·å–24å°æ—¶ç»Ÿè®¡
            stats = self.client.get_ticker()
            stats_dict = {s['symbol']: s for s in stats}
            
            for symbol, info in self.symbol_info.items():
                if symbol in stats_dict:
                    stat = stats_dict[symbol]
                    info.daily_volume_24h = float(stat['quoteVolume'])
                    
        except Exception as e:
            logger.error(f"Error updating market stats: {e}")
            
    def download_symbol_data(self, symbol: str, start_date: str = None, 
                           end_date: str = None) -> bool:
        """ä¸‹è½½å•ä¸ªå¸ç§æ•°æ®"""
        
        if not start_date:
            start_date = (datetime.now() - timedelta(days=self.lookback_days)).strftime("%Y-%m-%d")
        if not end_date:
            end_date = datetime.now().strftime("%Y-%m-%d")
            
        logger.info(f"ğŸ“¥ Downloading {symbol} data from {start_date} to {end_date}")
        
        try:
            # è·å–Kçº¿æ•°æ®
            klines = self.client.get_historical_klines(
                symbol=symbol,
                interval=self.timeframe,
                start_str=start_date,
                end_str=end_date
            )
            
            if not klines:
                logger.error(f"âŒ No data received for {symbol}")
                return False
                
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(klines, columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume',
                'close_time', 'quote_asset_volume', 'number_of_trades',
                'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
            ])
            
            # æ•°æ®ç±»å‹è½¬æ¢
            numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume']
            for col in numeric_columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            quality = self._assess_data_quality(symbol, df)
            self.data_quality[symbol] = quality
            
            # ä¿å­˜æ•°æ®
            filename = self.data_dir / f"{symbol}_{self.timeframe}_2years.csv"
            df.to_csv(filename, index=False)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                "symbol": symbol,
                "interval": self.timeframe,
                "total_records": len(df),
                "start_time": df['timestamp'].min().strftime("%Y-%m-%d %H:%M:%S"),
                "end_time": df['timestamp'].max().strftime("%Y-%m-%d %H:%M:%S"),
                "download_time": datetime.now().isoformat(),
                "file_size_mb": filename.stat().st_size / 1024 / 1024,
                "data_quality": quality.__dict__
            }
            
            metadata_file = self.data_dir / f"{symbol}_{self.timeframe}_2years_metadata.json"
            with open(metadata_file, 'w') as f:
                json.dump(metadata, f, indent=2, default=str)
                
            logger.info(f"âœ… {symbol}: {len(df)} records, Quality: {quality.quality_score:.1f}/100")
            
            # APIé™åˆ¶å»¶è¿Ÿ
            time.sleep(self.request_delay)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error downloading {symbol}: {e}")
            return False
            
    def _assess_data_quality(self, symbol: str, df: pd.DataFrame) -> DataQuality:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        quality = DataQuality(symbol=symbol)
        
        if len(df) == 0:
            return quality
            
        # 1. å®Œæ•´æ€§æ£€æŸ¥
        expected_records = self.lookback_days * 24 * 60 / 5  # 5åˆ†é’Ÿæ•°æ®
        quality.completeness = len(df) / expected_records
        
        # 2. æ•°æ®ç¼ºå£æ£€æŸ¥
        df_sorted = df.sort_values('timestamp')
        time_diffs = df_sorted['timestamp'].diff()
        expected_diff = pd.Timedelta(minutes=5)
        gaps = time_diffs[time_diffs > expected_diff * 1.5]
        quality.gap_count = len(gaps)
        
        # 3. ä»·æ ¼å¼‚å¸¸æ£€æŸ¥
        price_changes = df['close'].pct_change()
        price_anomalies = abs(price_changes) > 0.2  # 20%ä»¥ä¸Šä»·æ ¼å˜åŒ–è§†ä¸ºå¼‚å¸¸
        quality.price_anomaly_count = price_anomalies.sum()
        
        # 4. æˆäº¤é‡å¼‚å¸¸æ£€æŸ¥
        volume_median = df['volume'].median()
        volume_anomalies = df['volume'] > volume_median * 50  # 50å€ä¸­ä½æ•°æˆäº¤é‡
        quality.volume_anomaly_count = volume_anomalies.sum()
        
        # 5. é›¶å€¼æ£€æŸ¥
        zero_prices = (df[['open', 'high', 'low', 'close']] == 0).any(axis=1).sum()
        zero_volumes = (df['volume'] == 0).sum()
        
        # ç»¼åˆè¯„åˆ†
        completeness_score = quality.completeness * 40
        gap_penalty = min(quality.gap_count / 100, 0.2) * 20
        anomaly_penalty = min((quality.price_anomaly_count + quality.volume_anomaly_count) / 1000, 0.2) * 20
        zero_penalty = min((zero_prices + zero_volumes) / len(df), 0.2) * 20
        
        quality.quality_score = max(0, completeness_score - gap_penalty - anomaly_penalty - zero_penalty)
        
        # é—®é¢˜è®°å½•
        if quality.completeness < 0.95:
            quality.issues.append(f"æ•°æ®å®Œæ•´æ€§ä¸è¶³: {quality.completeness:.1%}")
        if quality.gap_count > 100:
            quality.issues.append(f"æ•°æ®ç¼ºå£è¿‡å¤š: {quality.gap_count}ä¸ª")
        if quality.price_anomaly_count > 10:
            quality.issues.append(f"ä»·æ ¼å¼‚å¸¸: {quality.price_anomaly_count}æ¬¡")
        if quality.volume_anomaly_count > 10:
            quality.issues.append(f"æˆäº¤é‡å¼‚å¸¸: {quality.volume_anomaly_count}æ¬¡")
            
        return quality
        
    def download_all_symbols(self, max_concurrent: int = 3):
        """æ‰¹é‡ä¸‹è½½æ‰€æœ‰å¸ç§æ•°æ®"""
        all_new_symbols = self.tier_1_symbols + self.tier_2_symbols + self.tier_3_symbols
        
        # è¿‡æ»¤å·²å­˜åœ¨çš„å¸ç§
        symbols_to_download = []
        for symbol in all_new_symbols:
            data_file = self.data_dir / f"{symbol}_{self.timeframe}_2years.csv"
            if not data_file.exists():
                symbols_to_download.append(symbol)
            else:
                logger.info(f"â­ï¸  Skipping {symbol} (already exists)")
                
        logger.info(f"ğŸ“¦ Starting batch download: {len(symbols_to_download)} symbols")
        
        successful_downloads = 0
        failed_downloads = 0
        
        # ä¸²è¡Œä¸‹è½½ï¼ˆé¿å…APIé™åˆ¶ï¼‰
        for i, symbol in enumerate(symbols_to_download, 1):
            logger.info(f"ğŸ“¥ [{i}/{len(symbols_to_download)}] Downloading {symbol}...")
            
            if self.download_symbol_data(symbol):
                successful_downloads += 1
            else:
                failed_downloads += 1
                
            # è¿›åº¦æŠ¥å‘Š
            if i % 5 == 0 or i == len(symbols_to_download):
                logger.info(f"ğŸ“Š Progress: {i}/{len(symbols_to_download)} "
                           f"(âœ…{successful_downloads} âŒ{failed_downloads})")
                           
        # ç”Ÿæˆä¸‹è½½æ‘˜è¦
        summary = {
            "download_summary": {
                "total_symbols": len(symbols_to_download),
                "successful_downloads": successful_downloads,
                "failed_downloads": failed_downloads,
                "download_completion_time": datetime.now().isoformat()
            },
            "quality_summary": {
                symbol: quality.__dict__ 
                for symbol, quality in self.data_quality.items()
            }
        }
        
        summary_file = self.data_dir / f"ultra_download_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
            
        logger.info(f"ğŸ‰ Batch download completed!")
        logger.info(f"  â€¢ Successful: {successful_downloads}")
        logger.info(f"  â€¢ Failed: {failed_downloads}")
        logger.info(f"  â€¢ Summary saved: {summary_file}")
        
        return successful_downloads, failed_downloads
        
    def get_quality_report(self) -> Dict:
        """è·å–æ•°æ®è´¨é‡æŠ¥å‘Š"""
        if not self.data_quality:
            return {"message": "No data quality information available"}
            
        report = {
            "æ€»å¸ç§æ•°": len(self.data_quality),
            "é«˜è´¨é‡å¸ç§": len([q for q in self.data_quality.values() if q.quality_score >= 85]),
            "ä¸­ç­‰è´¨é‡å¸ç§": len([q for q in self.data_quality.values() if 70 <= q.quality_score < 85]),
            "ä½è´¨é‡å¸ç§": len([q for q in self.data_quality.values() if q.quality_score < 70]),
            "å¹³å‡è´¨é‡è¯„åˆ†": np.mean([q.quality_score for q in self.data_quality.values()]),
            "è¯¦ç»†ä¿¡æ¯": {}
        }
        
        for symbol, quality in self.data_quality.items():
            report["è¯¦ç»†ä¿¡æ¯"][symbol] = {
                "è´¨é‡è¯„åˆ†": f"{quality.quality_score:.1f}/100",
                "æ•°æ®å®Œæ•´æ€§": f"{quality.completeness:.1%}",
                "æ•°æ®ç¼ºå£": quality.gap_count,
                "ä»·æ ¼å¼‚å¸¸": quality.price_anomaly_count,
                "æˆäº¤é‡å¼‚å¸¸": quality.volume_anomaly_count,
                "é—®é¢˜åˆ—è¡¨": quality.issues
            }
            
        return report
        
    def get_recommended_symbols(self, min_quality: float = 80, 
                              min_volume_24h: float = 10_000_000) -> List[str]:
        """è·å–æ¨èçš„ä¼˜è´¨å¸ç§"""
        recommended = []
        
        for symbol, info in self.symbol_info.items():
            # æ£€æŸ¥è´¨é‡è¦æ±‚
            quality = self.data_quality.get(symbol)
            if not quality or quality.quality_score < min_quality:
                continue
                
            # æ£€æŸ¥æˆäº¤é‡è¦æ±‚
            if info.daily_volume_24h < min_volume_24h:
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ´»è·ƒäº¤æ˜“
            if not info.is_active:
                continue
                
            recommended.append(symbol)
            
        # æŒ‰Tieræ’åº
        def sort_key(symbol):
            tier = self.symbol_info[symbol].tier
            quality_score = self.data_quality[symbol].quality_score
            volume = self.symbol_info[symbol].daily_volume_24h
            return (tier, -quality_score, -volume)
            
        recommended.sort(key=sort_key)
        
        return recommended
        
    def create_combined_dataset(self, symbols: List[str]) -> pd.DataFrame:
        """åˆ›å»ºåˆå¹¶çš„æ•°æ®é›†ä¾›å›æµ‹ä½¿ç”¨"""
        combined_data = []
        
        for symbol in symbols:
            data_file = self.data_dir / f"{symbol}_{self.timeframe}_2years.csv"
            if data_file.exists():
                df = pd.read_csv(data_file)
                df['symbol'] = symbol
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                combined_data.append(df)
            else:
                logger.warning(f"Data file not found for {symbol}")
                
        if combined_data:
            result = pd.concat(combined_data, ignore_index=True)
            result = result.sort_values(['timestamp', 'symbol'])
            logger.info(f"âœ… Combined dataset created: {len(result)} records, {len(symbols)} symbols")
            return result
        else:
            logger.error("âŒ No data available for combination")
            return pd.DataFrame()


async def main():
    """ä¸»å‡½æ•° - ä¸‹è½½æ‰©å±•å¸ç§æ•°æ®"""
    logger.info("ğŸš€ Starting Ultra Symbol Data Manager")
    
    # åˆ›å»ºæ•°æ®ç®¡ç†å™¨
    manager = UltraSymbolDataManager()
    
    # åˆå§‹åŒ–å¸ç§æ± 
    manager.initialize_symbol_pool()
    
    # ä¸‹è½½æ‰€æœ‰æ–°å¸ç§æ•°æ®
    success_count, fail_count = manager.download_all_symbols()
    
    # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    quality_report = manager.get_quality_report()
    logger.info("ğŸ“Š Data Quality Report:")
    for key, value in quality_report.items():
        if key != "è¯¦ç»†ä¿¡æ¯":
            logger.info(f"  â€¢ {key}: {value}")
            
    # è·å–æ¨èå¸ç§
    recommended = manager.get_recommended_symbols()
    logger.info(f"ğŸ¯ Recommended high-quality symbols ({len(recommended)}):")
    for symbol in recommended[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
        info = manager.symbol_info[symbol]
        quality = manager.data_quality[symbol]
        logger.info(f"  â€¢ {symbol} [Tier {info.tier}] - Quality: {quality.quality_score:.1f}, "
                   f"Volume: ${info.daily_volume_24h/1e6:.1f}M")
        
    logger.info("ğŸ‰ Ultra Symbol Data Manager completed successfully!")
    return manager


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO, 
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # è¿è¡Œä¸»å‡½æ•°
    asyncio.run(main())