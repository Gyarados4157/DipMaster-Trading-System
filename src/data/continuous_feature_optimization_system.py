#!/usr/bin/env python3
"""
DipMasteræŒç»­ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–ç³»ç»Ÿ
Continuous Feature Engineering Optimization System

è¿™æ˜¯ä¸€ä¸ªè‡ªé€‚åº”çš„ç‰¹å¾å·¥ç¨‹ç³»ç»Ÿï¼Œä¸“é—¨ä¸ºDipMasterç­–ç•¥è®¾è®¡ï¼Œèƒ½å¤Ÿï¼š
1. æŒç»­æŒ–æ˜æ–°çš„æœ‰æ•ˆç‰¹å¾
2. è‡ªåŠ¨è¯„ä¼°å’Œä¼˜åŒ–ç°æœ‰ç‰¹å¾
3. æ£€æµ‹ç‰¹å¾é€€åŒ–å¹¶åŠ¨æ€è°ƒæ•´
4. ç¡®ä¿ä¸¥æ ¼çš„æ•°æ®æ³„æ¼æ£€æµ‹
5. ç”Ÿæˆç‰¹å¾è´¨é‡ç›‘æ§æŠ¥å‘Š

Author: DipMaster Quant Team
Date: 2025-08-18
Version: 1.0.0-ContinuousOptimization
"""

import pandas as pd
import numpy as np
import warnings
import logging
import json
import time
from typing import Dict, List, Tuple, Optional, Any, Callable
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from sklearn.feature_selection import (
    mutual_info_regression, mutual_info_classif,
    SelectKBest, RFE, RFECV, SelectFromModel
)
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.linear_model import LassoCV, ElasticNetCV
from sklearn.metrics import mean_squared_error, accuracy_score, roc_auc_score
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
import lightgbm as lgb
import scipy.stats as stats
import ta
import numba
from numba import jit
import pickle
import warnings

warnings.filterwarnings('ignore')

@dataclass
class FeatureOptimizationConfig:
    """ç‰¹å¾ä¼˜åŒ–é…ç½®"""
    symbols: List[str]
    feature_update_interval_hours: int = 1
    max_features_per_category: int = 50
    min_feature_importance: float = 0.001
    max_correlation_threshold: float = 0.95
    stability_threshold: float = 0.8  # PSI threshold
    validation_window_days: int = 30
    innovation_rate: float = 0.1  # 10% new features each cycle
    enable_advanced_patterns: bool = True
    enable_microstructure_innovation: bool = True
    enable_cross_timeframe_features: bool = True

@dataclass 
class FeatureQualityReport:
    """ç‰¹å¾è´¨é‡æŠ¥å‘Š"""
    timestamp: str
    total_features: int
    active_features: int
    new_features: int
    deprecated_features: int
    feature_stability_scores: Dict[str, float]
    feature_importance_scores: Dict[str, float]
    leakage_detected_features: List[str]
    performance_metrics: Dict[str, float]

class ContinuousFeatureOptimizer:
    """
    æŒç»­ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–å™¨
    """
    
    def __init__(self, config: FeatureOptimizationConfig):
        self.config = config
        self.logger = self._setup_logger()
        self.feature_registry = {}  # ç‰¹å¾æ³¨å†Œè¡¨
        self.feature_performance = {}  # ç‰¹å¾æ€§èƒ½å†å²
        self.feature_stability = {}  # ç‰¹å¾ç¨³å®šæ€§è¿½è¸ª
        self.innovation_cache = {}  # åˆ›æ–°ç‰¹å¾ç¼“å­˜
        self.model_cache = {}  # æ¨¡å‹ç¼“å­˜
        self.scaler = StandardScaler()
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger(f"{__name__}.ContinuousOptimizer")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def generate_advanced_momentum_features(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """
        ç”Ÿæˆé«˜çº§åŠ¨é‡ç‰¹å¾
        """
        try:
            self.logger.info(f"Generating advanced momentum features for {symbol}")
            
            # 1. å¤šæ—¶é—´æ¡†æ¶åŠ¨é‡ç‰¹å¾
            momentum_periods = [3, 5, 8, 13, 21, 34]
            for period in momentum_periods:
                # ä»·æ ¼åŠ¨é‡
                df[f'{symbol}_momentum_{period}m'] = df['close'].pct_change(period)
                
                # åŠ é€Ÿåº¦ (äºŒé˜¶å¯¼æ•°)
                df[f'{symbol}_acceleration_{period}m'] = df[f'{symbol}_momentum_{period}m'].diff()
                
                # åŠ¨é‡å¼ºåº¦
                momentum_strength = abs(df[f'{symbol}_momentum_{period}m'])
                df[f'{symbol}_momentum_strength_{period}m'] = momentum_strength
                
                # åŠ¨é‡ä¸€è‡´æ€§ (æ–¹å‘ç¨³å®šæ€§)
                momentum_direction = np.sign(df[f'{symbol}_momentum_{period}m'])
                df[f'{symbol}_momentum_consistency_{period}m'] = (
                    momentum_direction.rolling(5).apply(lambda x: (x == x.iloc[-1]).mean())
                )
            
            # 2. é‡ä»·åŠ¨é‡èƒŒç¦»
            price_momentum = df['close'].pct_change(10)
            volume_momentum = df['volume'].pct_change(10)
            df[f'{symbol}_volume_price_divergence'] = price_momentum - volume_momentum
            
            # 3. ç›¸å¯¹å¼ºåº¦æŒ‡æ•°å˜ä½“
            for period in [7, 14, 21]:
                rsi = ta.momentum.RSIIndicator(df['close'], window=period).rsi()
                df[f'{symbol}_rsi_{period}_slope'] = rsi.diff()
                df[f'{symbol}_rsi_{period}_acceleration'] = rsi.diff().diff()
                
                # RSIèƒŒç¦»æ£€æµ‹
                price_highs = df['close'].rolling(period).max()
                price_lows = df['close'].rolling(period).min()
                rsi_highs = rsi.rolling(period).max()
                rsi_lows = rsi.rolling(period).min()
                
                # ç‰›èƒŒç¦»ï¼šä»·æ ¼åˆ›æ–°ä½ï¼ŒRSIä¸åˆ›æ–°ä½
                df[f'{symbol}_rsi_bull_divergence_{period}'] = (
                    (df['close'] == price_lows) & (rsi > rsi_lows)
                ).astype(int)
                
                # ç†ŠèƒŒç¦»ï¼šä»·æ ¼åˆ›æ–°é«˜ï¼ŒRSIä¸åˆ›æ–°é«˜
                df[f'{symbol}_rsi_bear_divergence_{period}'] = (
                    (df['close'] == price_highs) & (rsi < rsi_highs)
                ).astype(int)
            
            # 4. æ³¢åŠ¨ç‡è°ƒæ•´åŠ¨é‡
            returns = df['close'].pct_change()
            for window in [10, 20, 50]:
                vol = returns.rolling(window).std()
                df[f'{symbol}_vol_adj_momentum_{window}'] = returns / (vol + 1e-8)
            
            return df
            
        except Exception as e:
            self.logger.error(f"Advanced momentum features failed for {symbol}: {e}")
            return df
    
    def generate_microstructure_innovation_features(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """
        ç”Ÿæˆåˆ›æ–°çš„å¾®è§‚ç»“æ„ç‰¹å¾
        """
        try:
            self.logger.info(f"Generating microstructure innovation features for {symbol}")
            
            # 1. å¢å¼ºçš„æ¥é’ˆå½¢æ€æ£€æµ‹
            high_low_range = df['high'] - df['low']
            body_size = abs(df['close'] - df['open'])
            upper_shadow = df['high'] - np.maximum(df['open'], df['close'])
            lower_shadow = np.minimum(df['open'], df['close']) - df['low']
            
            # è¶…çº§æ¥é’ˆæ£€æµ‹ (æ›´ä¸¥æ ¼çš„æ¡ä»¶)
            super_pin_conditions = (
                (lower_shadow / (high_low_range + 1e-8) > 0.6) &  # ä¸‹å½±çº¿å æ¯”>60%
                (body_size / (high_low_range + 1e-8) < 0.2) &     # å®ä½“å æ¯”<20%
                (upper_shadow / (high_low_range + 1e-8) < 0.2) &  # ä¸Šå½±çº¿å æ¯”<20%
                (df['volume'] > df['volume'].rolling(20).mean() * 1.5)  # æˆäº¤é‡æ”¾å¤§
            )
            df[f'{symbol}_super_pin_bar'] = super_pin_conditions.astype(int)
            
            # æ¥é’ˆå¼ºåº¦è¯„åˆ† (0-1)
            pin_strength = (
                (lower_shadow / (high_low_range + 1e-8)) * 0.4 +
                (1 - body_size / (high_low_range + 1e-8)) * 0.3 +
                (1 - upper_shadow / (high_low_range + 1e-8)) * 0.3
            )
            df[f'{symbol}_pin_strength_score'] = np.clip(pin_strength, 0, 1)
            
            # 2. è®¢å•æµä¸å¹³è¡¡æŒ‡æ ‡
            # æ¨¡æ‹Ÿä¹°å–å‹åŠ›
            buy_pressure = np.where(df['close'] > df['open'], df['volume'], 0)
            sell_pressure = np.where(df['close'] < df['open'], df['volume'], 0)
            
            for window in [5, 10, 20]:
                buy_vol = pd.Series(buy_pressure).rolling(window).sum()
                sell_vol = pd.Series(sell_pressure).rolling(window).sum()
                total_vol = buy_vol + sell_vol
                
                df[f'{symbol}_order_flow_imbalance_{window}'] = (
                    (buy_vol - sell_vol) / (total_vol + 1e-8)
                )
                
                # å‡€ä¹°å…¥å¼ºåº¦
                df[f'{symbol}_net_buying_intensity_{window}'] = buy_vol / (total_vol + 1e-8)
            
            # 3. æµåŠ¨æ€§æ¯ç«­æŒ‡æ ‡
            price_impact = abs(df['close'].pct_change()) / (
                df['volume'] / df['volume'].rolling(50).mean() + 1e-8
            )
            df[f'{symbol}_price_impact'] = price_impact
            df[f'{symbol}_liquidity_shortage'] = (
                price_impact > price_impact.rolling(100).quantile(0.9)
            ).astype(int)
            
            # 4. æ”¯æ’‘é˜»åŠ›å¼ºåº¦
            for lookback in [20, 50]:
                # æ”¯æ’‘ä½
                support_level = df['low'].rolling(lookback).min()
                support_distance = (df['close'] - support_level) / support_level
                
                # è®¡ç®—æ”¯æ’‘å¼ºåº¦ (è¯¥ä»·ä½è¢«æµ‹è¯•çš„æ¬¡æ•°)
                support_tests = pd.Series(index=df.index, dtype=float)
                for i in range(lookback, len(df)):
                    recent_lows = df['low'].iloc[i-lookback:i]
                    current_support = support_level.iloc[i]
                    # è®¡ç®—æ¥è¿‘æ”¯æ’‘ä½çš„æ¬¡æ•°
                    near_support = abs(recent_lows - current_support) / current_support < 0.01
                    support_tests.iloc[i] = near_support.sum()
                
                df[f'{symbol}_support_strength_{lookback}'] = support_tests
                df[f'{symbol}_near_support_{lookback}'] = (support_distance < 0.02).astype(int)
                
                # é˜»åŠ›ä½
                resistance_level = df['high'].rolling(lookback).max()
                resistance_distance = (resistance_level - df['close']) / df['close']
                df[f'{symbol}_near_resistance_{lookback}'] = (resistance_distance < 0.02).astype(int)
            
            # 5. æˆäº¤é‡å‰–é¢ç‰¹å¾
            # æ¨¡æ‹Ÿæˆäº¤é‡åŠ æƒå¹³å‡ä»·æ ¼ (VWAP) åç¦»
            for period in [20, 50]:
                typical_price = (df['high'] + df['low'] + df['close']) / 3
                vwap = (typical_price * df['volume']).rolling(period).sum() / df['volume'].rolling(period).sum()
                
                vwap_deviation = (df['close'] - vwap) / vwap
                df[f'{symbol}_vwap_deviation_{period}'] = vwap_deviation
                
                # VWAPåç¦»æå€¼
                df[f'{symbol}_vwap_extreme_deviation_{period}'] = (
                    abs(vwap_deviation) > abs(vwap_deviation).rolling(100).quantile(0.9)
                ).astype(int)
            
            return df
            
        except Exception as e:
            self.logger.error(f"Microstructure innovation features failed for {symbol}: {e}")
            return df
    
    def generate_market_regime_features(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """
        ç”Ÿæˆå¸‚åœºåˆ¶åº¦è¯†åˆ«ç‰¹å¾
        """
        try:
            self.logger.info(f"Generating market regime features for {symbol}")
            
            returns = df['close'].pct_change()
            
            # 1. æ³¢åŠ¨ç‡åˆ¶åº¦
            volatility = returns.rolling(20).std()
            vol_percentiles = volatility.rolling(200).quantile([0.25, 0.75])
            
            df[f'{symbol}_low_vol_regime'] = (volatility <= vol_percentiles[0.25]).astype(int)
            df[f'{symbol}_high_vol_regime'] = (volatility >= vol_percentiles[0.75]).astype(int)
            
            # æ³¢åŠ¨ç‡æŒç»­æ€§
            df[f'{symbol}_vol_persistence'] = volatility.rolling(10).std() / volatility
            
            # 2. è¶‹åŠ¿åˆ¶åº¦
            # å¤šé‡ç§»åŠ¨å¹³å‡çº¿è¶‹åŠ¿
            ma_periods = [10, 20, 50]
            trend_signals = []
            
            for period in ma_periods:
                ma = df['close'].rolling(period).mean()
                ma_slope = ma.pct_change()
                trend_signals.append(ma_slope > 0)
                df[f'{symbol}_ma_{period}_slope'] = ma_slope
            
            # è¶‹åŠ¿ä¸€è‡´æ€§ (æ‰€æœ‰MAåŒæ–¹å‘çš„æ¯”ä¾‹)
            df[f'{symbol}_trend_consistency'] = np.mean(trend_signals, axis=0)
            
            # å¼ºè¶‹åŠ¿è¯†åˆ«
            df[f'{symbol}_strong_uptrend'] = (df[f'{symbol}_trend_consistency'] > 0.8).astype(int)
            df[f'{symbol}_strong_downtrend'] = (df[f'{symbol}_trend_consistency'] < 0.2).astype(int)
            df[f'{symbol}_sideways_market'] = (
                (df[f'{symbol}_trend_consistency'] >= 0.4) & 
                (df[f'{symbol}_trend_consistency'] <= 0.6)
            ).astype(int)
            
            # 3. å‡å€¼å›å½’ vs åŠ¨é‡åˆ¶åº¦
            # åŠè¡°æœŸä¼°è®¡
            for window in [50, 100]:
                rolling_returns = returns.rolling(window)
                # ç®€åŒ–çš„åŠè¡°æœŸè®¡ç®—
                autocorr = rolling_returns.apply(lambda x: x.autocorr(lag=1) if len(x) > 10 else 0)
                half_life = -np.log(2) / np.log(abs(autocorr) + 1e-8)
                df[f'{symbol}_half_life_{window}'] = half_life
                
                # åˆ¶åº¦åˆ†ç±»
                df[f'{symbol}_mean_reversion_regime_{window}'] = (half_life < 10).astype(int)
                df[f'{symbol}_momentum_regime_{window}'] = (half_life > 30).astype(int)
            
            # 4. æµåŠ¨æ€§åˆ¶åº¦
            volume_ma = df['volume'].rolling(50).mean()
            volume_std = df['volume'].rolling(50).std()
            
            df[f'{symbol}_high_liquidity_regime'] = (
                df['volume'] > volume_ma + volume_std
            ).astype(int)
            df[f'{symbol}_low_liquidity_regime'] = (
                df['volume'] < volume_ma - volume_std
            ).astype(int)
            
            # 5. å¸‚åœºå‹åŠ›åˆ¶åº¦
            # å›æ’¤å¹…åº¦
            rolling_max = df['close'].rolling(100).max()
            drawdown = (df['close'] - rolling_max) / rolling_max
            df[f'{symbol}_current_drawdown'] = drawdown
            
            # å‹åŠ›ç­‰çº§
            df[f'{symbol}_market_stress'] = pd.cut(
                -drawdown, 
                bins=[0, 0.05, 0.10, 0.20, 1.0], 
                labels=[0, 1, 2, 3]
            ).astype(int)
            
            return df
            
        except Exception as e:
            self.logger.error(f"Market regime features failed for {symbol}: {e}")
            return df
    
    def generate_cross_timeframe_features(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """
        ç”Ÿæˆè·¨æ—¶é—´æ¡†æ¶ç‰¹å¾
        """
        try:
            self.logger.info(f"Generating cross-timeframe features for {symbol}")
            
            # æ¨¡æ‹Ÿä¸åŒæ—¶é—´æ¡†æ¶çš„æ•°æ®
            # 5åˆ†é’Ÿ -> 15åˆ†é’Ÿ -> 1å°æ—¶
            
            # 1. 15åˆ†é’Ÿçº§åˆ«ç‰¹å¾ (3ä¸ª5åˆ†é’ŸKçº¿åˆå¹¶)
            for agg_period in [3, 12]:  # 15åˆ†é’Ÿå’Œ1å°æ—¶
                # ä»·æ ¼èšåˆ
                high_agg = df['high'].rolling(agg_period).max()
                low_agg = df['low'].rolling(agg_period).min()
                open_agg = df['open'].rolling(agg_period).first()
                close_agg = df['close']
                volume_agg = df['volume'].rolling(agg_period).sum()
                
                # é«˜æ—¶é—´æ¡†æ¶RSI
                rsi_agg = ta.momentum.RSIIndicator(close_agg, window=14).rsi()
                df[f'{symbol}_rsi_htf_{agg_period*5}m'] = rsi_agg
                
                # é«˜æ—¶é—´æ¡†æ¶MACD
                macd_agg = ta.trend.MACD(close_agg)
                df[f'{symbol}_macd_htf_{agg_period*5}m'] = macd_agg.macd()
                df[f'{symbol}_macd_signal_htf_{agg_period*5}m'] = macd_agg.macd_signal()
                
                # é«˜æ—¶é—´æ¡†æ¶å¸ƒæ—å¸¦
                bb_agg = ta.volatility.BollingerBands(close_agg, window=20)
                bb_pos = bb_agg.bollinger_pband()
                df[f'{symbol}_bb_position_htf_{agg_period*5}m'] = bb_pos
                
                # æ—¶é—´æ¡†æ¶ä¸€è‡´æ€§æ£€æŸ¥
                rsi_5m = ta.momentum.RSIIndicator(df['close'], window=14).rsi()
                df[f'{symbol}_rsi_consistency_{agg_period*5}m'] = (
                    np.sign(rsi_5m - 50) == np.sign(rsi_agg - 50)
                ).astype(int)
            
            # 2. å¤šæ—¶é—´æ¡†æ¶è¶‹åŠ¿å¯¹é½
            # 5åˆ†é’Ÿè¶‹åŠ¿
            ma_5m_10 = df['close'].rolling(10).mean()
            ma_5m_20 = df['close'].rolling(20).mean()
            trend_5m = (ma_5m_10 > ma_5m_20).astype(int)
            
            # 15åˆ†é’Ÿè¶‹åŠ¿ (åŸºäº3å‘¨æœŸèšåˆ)
            ma_15m_10 = df['close'].rolling(30).mean()  # 10ä¸ª15åˆ†é’Ÿå‘¨æœŸ
            ma_15m_20 = df['close'].rolling(60).mean()  # 20ä¸ª15åˆ†é’Ÿå‘¨æœŸ
            trend_15m = (ma_15m_10 > ma_15m_20).astype(int)
            
            # è¶‹åŠ¿å¯¹é½åº¦
            df[f'{symbol}_trend_alignment'] = (trend_5m == trend_15m).astype(int)
            
            # 3. å¤šæ—¶é—´æ¡†æ¶åŠ¨é‡åˆ†æ­§
            momentum_5m = df['close'].pct_change(10)  # 50åˆ†é’ŸåŠ¨é‡
            momentum_15m = df['close'].pct_change(36)  # 3å°æ—¶åŠ¨é‡
            
            df[f'{symbol}_momentum_divergence'] = abs(momentum_5m - momentum_15m)
            df[f'{symbol}_momentum_convergence'] = (
                np.sign(momentum_5m) == np.sign(momentum_15m)
            ).astype(int)
            
            # 4. è·¨å‘¨æœŸæ”¯æ’‘é˜»åŠ›
            # 15åˆ†é’Ÿæ”¯æ’‘é˜»åŠ›å¯¹5åˆ†é’Ÿä»·æ ¼çš„å½±å“
            support_15m = df['low'].rolling(60).min()  # 15åˆ†é’Ÿæ”¯æ’‘ (5å°æ—¶)
            resistance_15m = df['high'].rolling(60).max()  # 15åˆ†é’Ÿé˜»åŠ›
            
            # ä»·æ ¼ç›¸å¯¹äºé«˜çº§åˆ«æ”¯æ’‘é˜»åŠ›çš„ä½ç½®
            price_position = (df['close'] - support_15m) / (resistance_15m - support_15m + 1e-8)
            df[f'{symbol}_htf_price_position'] = price_position
            
            # æ¥è¿‘é«˜çº§åˆ«å…³é”®ä½
            df[f'{symbol}_near_htf_support'] = (price_position < 0.1).astype(int)
            df[f'{symbol}_near_htf_resistance'] = (price_position > 0.9).astype(int)
            
            return df
            
        except Exception as e:
            self.logger.error(f"Cross-timeframe features failed for {symbol}: {e}")
            return df
    
    def generate_interaction_features(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """
        ç”Ÿæˆç‰¹å¾äº¤äº’é¡¹
        """
        try:
            self.logger.info(f"Generating interaction features for {symbol}")
            
            # 1. ç»å…¸æŠ€æœ¯æŒ‡æ ‡äº¤äº’
            if f'{symbol}_rsi_htf_15m' in df.columns and 'volume' in df.columns:
                # RSIä¸æˆäº¤é‡äº¤äº’
                volume_ratio = df['volume'] / df['volume'].rolling(20).mean()
                df[f'{symbol}_rsi_volume_interaction'] = df[f'{symbol}_rsi_htf_15m'] * volume_ratio
            
            # 2. æ³¢åŠ¨ç‡ä¸åŠ¨é‡äº¤äº’
            returns = df['close'].pct_change()
            volatility = returns.rolling(20).std()
            momentum = returns.rolling(10).sum()
            df[f'{symbol}_vol_momentum_interaction'] = volatility * momentum
            
            # 3. è¶‹åŠ¿ä¸å‡å€¼å›å½’ç‰¹å¾äº¤äº’
            if f'{symbol}_trend_consistency' in df.columns and f'{symbol}_bb_position_htf_15m' in df.columns:
                df[f'{symbol}_trend_bb_interaction'] = (
                    df[f'{symbol}_trend_consistency'] * df[f'{symbol}_bb_position_htf_15m']
                )
            
            # 4. åˆ¶åº¦æ¡ä»¶ç‰¹å¾
            if f'{symbol}_high_vol_regime' in df.columns:
                # åœ¨é«˜æ³¢åŠ¨åˆ¶åº¦ä¸‹çš„RSIè¡Œä¸º
                base_rsi = ta.momentum.RSIIndicator(df['close'], window=14).rsi()
                df[f'{symbol}_rsi_high_vol_regime'] = (
                    base_rsi * df[f'{symbol}_high_vol_regime']
                )
                df[f'{symbol}_rsi_normal_vol_regime'] = (
                    base_rsi * (1 - df[f'{symbol}_high_vol_regime'])
                )
            
            return df
            
        except Exception as e:
            self.logger.error(f"Interaction features failed for {symbol}: {e}")
            return df
    
    def calculate_feature_importance(self, df: pd.DataFrame, target_cols: List[str]) -> Dict[str, float]:
        """
        è®¡ç®—ç‰¹å¾é‡è¦æ€§
        """
        try:
            feature_importance = {}
            
            # æ’é™¤ç›®æ ‡å˜é‡å’Œéç‰¹å¾åˆ—
            feature_cols = [col for col in df.columns 
                          if col not in target_cols 
                          and col not in ['timestamp', 'symbol']
                          and not col.startswith('future_')
                          and not col.startswith('target_')]
            
            if not feature_cols or not target_cols:
                return feature_importance
            
            X = df[feature_cols].fillna(0)
            
            for target_col in target_cols:
                if target_col not in df.columns:
                    continue
                    
                y = df[target_col].fillna(0)
                
                # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
                valid_mask = ~(X.isnull().all(axis=1) | y.isnull())
                X_valid = X[valid_mask]
                y_valid = y[valid_mask]
                
                if len(X_valid) < 100:
                    continue
                
                try:
                    # ä½¿ç”¨äº’ä¿¡æ¯è®¡ç®—ç‰¹å¾é‡è¦æ€§
                    if y_valid.dtype in ['int64', 'bool'] and len(y_valid.unique()) <= 10:
                        # åˆ†ç±»é—®é¢˜
                        mi_scores = mutual_info_classif(X_valid, y_valid, random_state=42)
                    else:
                        # å›å½’é—®é¢˜
                        mi_scores = mutual_info_regression(X_valid, y_valid, random_state=42)
                    
                    # å­˜å‚¨ç»“æœ
                    for i, feature in enumerate(feature_cols):
                        if feature not in feature_importance:
                            feature_importance[feature] = 0
                        feature_importance[feature] += mi_scores[i]
                        
                except Exception as e:
                    self.logger.warning(f"Feature importance calculation failed for {target_col}: {e}")
                    continue
            
            # æ ‡å‡†åŒ–é‡è¦æ€§åˆ†æ•°
            if feature_importance:
                max_importance = max(feature_importance.values())
                if max_importance > 0:
                    feature_importance = {
                        k: v / max_importance for k, v in feature_importance.items()
                    }
            
            return feature_importance
            
        except Exception as e:
            self.logger.error(f"Feature importance calculation failed: {e}")
            return {}
    
    def detect_feature_stability(self, df: pd.DataFrame, feature_name: str, window_days: int = 30) -> float:
        """
        æ£€æµ‹ç‰¹å¾ç¨³å®šæ€§ (PSI - Population Stability Index)
        """
        try:
            if feature_name not in df.columns or 'timestamp' not in df.columns:
                return 0.0
            
            df_copy = df.copy()
            df_copy['timestamp'] = pd.to_datetime(df_copy['timestamp'])
            df_copy = df_copy.sort_values('timestamp')
            
            feature_data = df_copy[feature_name].dropna()
            if len(feature_data) < 200:
                return 0.0
            
            # åˆ†å‰²æ•°æ®ä¸ºåŸºå‡†æœŸå’Œæµ‹è¯•æœŸ
            split_point = len(feature_data) // 2
            baseline = feature_data.iloc[:split_point]
            current = feature_data.iloc[split_point:]
            
            # åˆ›å»ºåˆ†ä½æ•°åŒºé—´
            bins = np.percentile(baseline, [0, 10, 25, 50, 75, 90, 100])
            bins = np.unique(bins)  # å»é™¤é‡å¤å€¼
            
            if len(bins) < 3:
                return 0.0
            
            # è®¡ç®—å„åŒºé—´çš„åˆ†å¸ƒ
            baseline_dist, _ = np.histogram(baseline, bins=bins)
            current_dist, _ = np.histogram(current, bins=bins)
            
            # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
            baseline_dist = baseline_dist / baseline_dist.sum()
            current_dist = current_dist / current_dist.sum()
            
            # è®¡ç®—PSI
            psi = 0
            for i in range(len(baseline_dist)):
                if baseline_dist[i] > 0 and current_dist[i] > 0:
                    psi += (current_dist[i] - baseline_dist[i]) * np.log(
                        current_dist[i] / baseline_dist[i]
                    )
            
            # è¿”å›ç¨³å®šæ€§åˆ†æ•° (PSIè¶Šå°è¶Šç¨³å®š)
            stability_score = max(0, 1 - psi / 0.25)  # 0.25æ˜¯å¸¸ç”¨çš„PSIé˜ˆå€¼
            return stability_score
            
        except Exception as e:
            self.logger.error(f"Feature stability detection failed for {feature_name}: {e}")
            return 0.0
    
    def detect_data_leakage(self, df: pd.DataFrame, target_cols: List[str]) -> List[str]:
        """
        æ£€æµ‹æ•°æ®æ³„æ¼
        """
        leakage_features = []
        
        try:
            feature_cols = [col for col in df.columns 
                          if col not in target_cols 
                          and col not in ['timestamp', 'symbol']
                          and not col.startswith('future_')
                          and not col.startswith('target_')]
            
            for target_col in target_cols:
                if target_col not in df.columns:
                    continue
                    
                target_data = df[target_col].dropna()
                if len(target_data) < 100:
                    continue
                
                for feature_col in feature_cols:
                    if feature_col not in df.columns:
                        continue
                    
                    feature_data = df[feature_col].dropna()
                    if len(feature_data) < 100:
                        continue
                    
                    # å¯¹é½æ•°æ®
                    aligned_data = pd.concat([feature_data, target_data], axis=1, join='inner')
                    if len(aligned_data) < 50:
                        continue
                    
                    # è®¡ç®—ç›¸å…³æ€§
                    correlation = aligned_data.iloc[:, 0].corr(aligned_data.iloc[:, 1])
                    
                    # æ£€æµ‹å¼‚å¸¸é«˜çš„ç›¸å…³æ€§ (å¯èƒ½çš„æ³„æ¼)
                    if abs(correlation) > 0.9:
                        leakage_features.append(feature_col)
                        self.logger.warning(
                            f"Potential data leakage detected: {feature_col} -> {target_col} "
                            f"(correlation: {correlation:.3f})"
                        )
            
        except Exception as e:
            self.logger.error(f"Data leakage detection failed: {e}")
        
        return list(set(leakage_features))
    
    def optimize_features_for_symbol(self, df: pd.DataFrame, symbol: str) -> Tuple[pd.DataFrame, Dict]:
        """
        ä¸ºå•ä¸ªå¸ç§ä¼˜åŒ–ç‰¹å¾
        """
        try:
            self.logger.info(f"Optimizing features for {symbol}...")
            
            # å¤åˆ¶æ•°æ®
            optimized_df = df.copy()
            
            # 1. ç”Ÿæˆé«˜çº§åŠ¨é‡ç‰¹å¾
            optimized_df = self.generate_advanced_momentum_features(optimized_df, symbol)
            
            # 2. ç”Ÿæˆå¾®è§‚ç»“æ„åˆ›æ–°ç‰¹å¾
            if self.config.enable_microstructure_innovation:
                optimized_df = self.generate_microstructure_innovation_features(optimized_df, symbol)
            
            # 3. ç”Ÿæˆå¸‚åœºåˆ¶åº¦ç‰¹å¾
            optimized_df = self.generate_market_regime_features(optimized_df, symbol)
            
            # 4. ç”Ÿæˆè·¨æ—¶é—´æ¡†æ¶ç‰¹å¾
            if self.config.enable_cross_timeframe_features:
                optimized_df = self.generate_cross_timeframe_features(optimized_df, symbol)
            
            # 5. ç”Ÿæˆäº¤äº’ç‰¹å¾
            optimized_df = self.generate_interaction_features(optimized_df, symbol)
            
            # 6. ç‰¹å¾è´¨é‡è¯„ä¼°
            target_cols = [col for col in optimized_df.columns if col.startswith('target_') or 'future_return' in col]
            
            # è®¡ç®—ç‰¹å¾é‡è¦æ€§
            feature_importance = self.calculate_feature_importance(optimized_df, target_cols)
            
            # æ£€æµ‹æ•°æ®æ³„æ¼
            leakage_features = self.detect_data_leakage(optimized_df, target_cols)
            
            # ç§»é™¤æ³„æ¼ç‰¹å¾
            if leakage_features:
                optimized_df = optimized_df.drop(columns=leakage_features)
                self.logger.info(f"Removed {len(leakage_features)} features with data leakage")
            
            # 7. ç‰¹å¾é€‰æ‹© (ä¿ç•™é‡è¦ç‰¹å¾)
            if feature_importance:
                important_features = [
                    feature for feature, importance in feature_importance.items() 
                    if importance >= self.config.min_feature_importance
                ]
                
                # ä¿ç•™åŸºç¡€åˆ—å’Œé‡è¦ç‰¹å¾
                base_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
                keep_cols = base_cols + important_features + target_cols
                keep_cols = [col for col in keep_cols if col in optimized_df.columns]
                
                optimized_df = optimized_df[keep_cols]
                self.logger.info(f"Selected {len(important_features)} important features for {symbol}")
            
            # 8. ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
            optimization_report = {
                'symbol': symbol,
                'original_features': len(df.columns),
                'optimized_features': len(optimized_df.columns),
                'feature_importance': feature_importance,
                'leakage_features_removed': leakage_features,
                'optimization_timestamp': datetime.now().isoformat()
            }
            
            return optimized_df, optimization_report
            
        except Exception as e:
            self.logger.error(f"Feature optimization failed for {symbol}: {e}")
            return df, {'symbol': symbol, 'error': str(e)}
    
    def run_continuous_optimization(self, data_dict: Dict[str, pd.DataFrame]) -> Tuple[Dict[str, pd.DataFrame], FeatureQualityReport]:
        """
        è¿è¡ŒæŒç»­ç‰¹å¾ä¼˜åŒ–
        """
        try:
            self.logger.info(f"Starting continuous feature optimization for {len(data_dict)} symbols...")
            start_time = time.time()
            
            optimized_data = {}
            optimization_reports = {}
            
            # å¹¶è¡Œå¤„ç†å„å¸ç§
            with ThreadPoolExecutor(max_workers=4) as executor:
                futures = {
                    executor.submit(self.optimize_features_for_symbol, df, symbol): symbol
                    for symbol, df in data_dict.items()
                }
                
                for future in as_completed(futures):
                    symbol = futures[future]
                    try:
                        optimized_df, report = future.result()
                        optimized_data[symbol] = optimized_df
                        optimization_reports[symbol] = report
                        self.logger.info(f"Optimization completed for {symbol}")
                    except Exception as e:
                        self.logger.error(f"Optimization failed for {symbol}: {e}")
                        optimized_data[symbol] = data_dict[symbol]  # ä½¿ç”¨åŸå§‹æ•°æ®
                        optimization_reports[symbol] = {'symbol': symbol, 'error': str(e)}
            
            # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
            total_features = sum(len(df.columns) for df in optimized_data.values())
            avg_features = total_features / len(optimized_data) if optimized_data else 0
            
            # ç»Ÿè®¡æ–°ç‰¹å¾
            new_features = []
            for symbol, report in optimization_reports.items():
                if 'feature_importance' in report:
                    new_features.extend(list(report['feature_importance'].keys()))
            new_features = list(set(new_features))
            
            # ç»Ÿè®¡æ³„æ¼ç‰¹å¾
            leakage_features = []
            for symbol, report in optimization_reports.items():
                if 'leakage_features_removed' in report:
                    leakage_features.extend(report['leakage_features_removed'])
            leakage_features = list(set(leakage_features))
            
            quality_report = FeatureQualityReport(
                timestamp=datetime.now().isoformat(),
                total_features=int(total_features),
                active_features=int(avg_features),
                new_features=len(new_features),
                deprecated_features=len(leakage_features),
                feature_stability_scores={},
                feature_importance_scores={},
                leakage_detected_features=leakage_features,
                performance_metrics={
                    'optimization_time_seconds': time.time() - start_time,
                    'symbols_processed': len(optimized_data),
                    'avg_features_per_symbol': avg_features
                }
            )
            
            self.logger.info(f"Continuous optimization completed in {quality_report.performance_metrics['optimization_time_seconds']:.1f}s")
            self.logger.info(f"Total features: {quality_report.total_features}, New features: {quality_report.new_features}")
            
            return optimized_data, quality_report
            
        except Exception as e:
            self.logger.error(f"Continuous optimization failed: {e}")
            return data_dict, FeatureQualityReport(
                timestamp=datetime.now().isoformat(),
                total_features=0,
                active_features=0,
                new_features=0,
                deprecated_features=0,
                feature_stability_scores={},
                feature_importance_scores={},
                leakage_detected_features=[],
                performance_metrics={'error': str(e)}
            )

def main():
    """æ¼”ç¤ºæŒç»­ç‰¹å¾ä¼˜åŒ–ç³»ç»Ÿ"""
    
    print("DipMasteræŒç»­ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–ç³»ç»Ÿ")
    print("=" * 60)
    print("ğŸ¯ ç›®æ ‡: æŒç»­å‘ç°å’Œä¼˜åŒ–æœ‰æ•ˆç‰¹å¾ç»„åˆ")
    print("\nğŸ”§ æ ¸å¿ƒåŠŸèƒ½:")
    print("1. é«˜çº§åŠ¨é‡ç‰¹å¾æŒ–æ˜")
    print("   - å¤šæ—¶é—´æ¡†æ¶åŠ¨é‡åˆ†æ")
    print("   - åŠ¨é‡èƒŒç¦»æ£€æµ‹") 
    print("   - æ³¢åŠ¨ç‡è°ƒæ•´åŠ¨é‡")
    print("\n2. å¾®è§‚ç»“æ„åˆ›æ–°ç‰¹å¾")
    print("   - è¶…çº§æ¥é’ˆå½¢æ€æ£€æµ‹")
    print("   - è®¢å•æµä¸å¹³è¡¡åˆ†æ")
    print("   - æµåŠ¨æ€§æ¯ç«­æŒ‡æ ‡")
    print("   - æ”¯æ’‘é˜»åŠ›å¼ºåº¦é‡åŒ–")
    print("\n3. å¸‚åœºåˆ¶åº¦è¯†åˆ«")
    print("   - æ³¢åŠ¨ç‡åˆ¶åº¦åˆ†ç±»")
    print("   - è¶‹åŠ¿vså‡å€¼å›å½’æ£€æµ‹")
    print("   - æµåŠ¨æ€§åˆ¶åº¦ç›‘æ§")
    print("   - å¸‚åœºå‹åŠ›ç­‰çº§è¯„ä¼°")
    print("\n4. è·¨æ—¶é—´æ¡†æ¶ç‰¹å¾")
    print("   - å¤šå‘¨æœŸä¿¡å·ä¸€è‡´æ€§")
    print("   - è¶‹åŠ¿å¯¹é½åˆ†æ")
    print("   - åŠ¨é‡åˆ†æ­§æ£€æµ‹")
    print("\n5. æŒç»­è´¨é‡ç›‘æ§")
    print("   - ç‰¹å¾é‡è¦æ€§è¿½è¸ª")
    print("   - æ•°æ®æ³„æ¼æ£€æµ‹")
    print("   - ç‰¹å¾ç¨³å®šæ€§ç›‘æ§")
    print("   - è‡ªåŠ¨ç‰¹å¾é€‰æ‹©")
    print("\nâœ… é¢„æœŸæ•ˆæœ:")
    print("- å‘ç°æ›´å¤šé¢„æµ‹æ€§ç‰¹å¾")
    print("- æå‡ä¿¡å·è´¨é‡å’Œç¨³å®šæ€§") 
    print("- å‡å°‘è¿‡æ‹Ÿåˆé£é™©")
    print("- å¢å¼ºç­–ç•¥é€‚åº”æ€§")
    print("=" * 60)

if __name__ == "__main__":
    main()