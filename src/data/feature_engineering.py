"""
DipMaster Enhanced V4 ç‰¹å¾å·¥ç¨‹ç®¡é“
ä¸“ä¸º85%+èƒœç‡ç­–ç•¥è®¾è®¡çš„é«˜è´¨é‡æœºå™¨å­¦ä¹ ç‰¹å¾ç”Ÿæˆå¼•æ“

æ ¸å¿ƒç‰¹å¾ç±»åˆ«ï¼š
1. DipMasteræ ¸å¿ƒç‰¹å¾ - RSIé€¢è·Œã€ä»·æ ¼åŠ¨é‡ã€æˆäº¤é‡ç¡®è®¤ã€å¸ƒæ—å¸¦ä½ç½®
2. å¸‚åœºå¾®è§‚ç»“æ„ç‰¹å¾ - è®¢å•æµä¸å¹³è¡¡ã€æ³¢åŠ¨ç‡ã€æµåŠ¨æ€§ã€æ—¶é—´å‘¨æœŸ
3. è·¨æ—¶é—´æ¡†æ¶ç‰¹å¾ - å¤šæ—¶é—´æ¡†æ¶å¯¹é½ã€è¶‹åŠ¿ä¸€è‡´æ€§ã€åŠ¨é‡æ”¶æ•›
4. è·¨èµ„äº§ç‰¹å¾ - ç›¸å…³æ€§ã€ç›¸å¯¹å¼ºåº¦ã€è¡Œä¸šè½®åŠ¨
5. é«˜è´¨é‡æ ‡ç­¾ç”Ÿæˆ - å¤šç»´åº¦ç›®æ ‡å˜é‡å’Œé£é™©æ ‡ç­¾
"""

import pandas as pd
import numpy as np
import sqlite3
import warnings
from pathlib import Path
from datetime import datetime, timedelta
import json
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass, asdict
import logging
from scipy import stats
from scipy.stats import zscore
import talib as ta
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression
import numba
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import time

# é…ç½®è­¦å‘Š
warnings.filterwarnings('ignore')

@dataclass
class FeatureEngineConfig:
    """ç‰¹å¾å·¥ç¨‹é…ç½®ç±»"""
    # æ•°æ®å‚æ•°
    symbols: List[str]
    primary_timeframe: str = "5m"
    analysis_timeframes: List[str] = None
    lookback_periods: Dict[str, int] = None
    
    # DipMasteræ ¸å¿ƒå‚æ•°
    rsi_period: int = 14
    rsi_entry_range: Tuple[int, int] = (25, 45)
    bollinger_period: int = 20
    bollinger_std: float = 2.0
    volume_ma_period: int = 20
    
    # å¾®è§‚ç»“æ„å‚æ•°
    volatility_windows: List[int] = None
    momentum_periods: List[int] = None
    
    # æ ‡ç­¾ç”Ÿæˆå‚æ•°
    prediction_horizons: List[int] = None  # åˆ†é’Ÿ
    profit_targets: List[float] = None  # ç™¾åˆ†æ¯”
    stop_loss_threshold: float = 0.004
    max_holding_minutes: int = 180
    
    # è´¨é‡æ§åˆ¶å‚æ•°
    feature_stability_threshold: float = 0.2  # PSI threshold
    correlation_threshold: float = 0.95
    missing_threshold: float = 0.05
    outlier_percentile: Tuple[float, float] = (0.5, 99.5)
    
    def __post_init__(self):
        if self.analysis_timeframes is None:
            self.analysis_timeframes = ["5m", "15m", "1h"]
        if self.lookback_periods is None:
            self.lookback_periods = {"short": 20, "medium": 50, "long": 200}
        if self.volatility_windows is None:
            self.volatility_windows = [10, 20, 50]
        if self.momentum_periods is None:
            self.momentum_periods = [5, 10, 20, 50]
        if self.prediction_horizons is None:
            self.prediction_horizons = [15, 30, 60]  # 15åˆ†é’Ÿ, 30åˆ†é’Ÿ, 1å°æ—¶
        if self.profit_targets is None:
            self.profit_targets = [0.006, 0.012, 0.020]  # 0.6%, 1.2%, 2.0%

class FeatureQualityValidator:
    """ç‰¹å¾è´¨é‡éªŒè¯å™¨"""
    
    def __init__(self, config: FeatureEngineConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    def validate_no_lookahead_bias(self, features: pd.DataFrame, feature_metadata: Dict) -> Dict:
        """æ£€æµ‹å‰è§†åå·® - ç¡®ä¿ç‰¹å¾ä¸åŒ…å«æœªæ¥ä¿¡æ¯"""
        validation_results = {
            'has_lookahead_bias': False,
            'suspicious_features': [],
            'temporal_consistency': True,
            'details': {}
        }
        
        # æ£€æŸ¥ç‰¹å¾è®¡ç®—çš„æ—¶é—´çª—å£
        for feature_name, metadata in feature_metadata.items():
            if 'lookback_window' in metadata:
                window = metadata['lookback_window']
                if window < 0:  # è´Ÿæ•°çª—å£è¡¨ç¤ºä½¿ç”¨æœªæ¥æ•°æ®
                    validation_results['has_lookahead_bias'] = True
                    validation_results['suspicious_features'].append(feature_name)
        
        # æ—¶é—´åºåˆ—å•è°ƒæ€§æ£€æŸ¥
        if 'timestamp' in features.columns:
            timestamps = pd.to_datetime(features['timestamp'])
            if not timestamps.is_monotonic_increasing:
                validation_results['temporal_consistency'] = False
        
        return validation_results
    
    def calculate_feature_stability(self, features: pd.DataFrame, 
                                   time_periods: List[Tuple[str, str]]) -> Dict:
        """è®¡ç®—ç‰¹å¾ç¨³å®šæ€§ (PSI - Population Stability Index)"""
        stability_results = {}
        
        for feature in features.select_dtypes(include=[np.number]).columns:
            if feature in ['timestamp', 'symbol_encoded']:
                continue
                
            psi_scores = []
            
            for i in range(len(time_periods) - 1):
                period1_start, period1_end = time_periods[i]
                period2_start, period2_end = time_periods[i + 1]
                
                period1_data = features[
                    (features['timestamp'] >= period1_start) & 
                    (features['timestamp'] < period1_end)
                ][feature].dropna()
                
                period2_data = features[
                    (features['timestamp'] >= period2_start) & 
                    (features['timestamp'] < period2_end)
                ][feature].dropna()
                
                if len(period1_data) > 100 and len(period2_data) > 100:
                    psi = self._calculate_psi(period1_data, period2_data)
                    psi_scores.append(psi)
            
            stability_results[feature] = {
                'mean_psi': np.mean(psi_scores) if psi_scores else np.nan,
                'max_psi': np.max(psi_scores) if psi_scores else np.nan,
                'stability_rating': self._classify_stability(np.mean(psi_scores) if psi_scores else np.nan)
            }
        
        return stability_results
    
    def _calculate_psi(self, baseline: pd.Series, comparison: pd.Series, bins: int = 10) -> float:
        """è®¡ç®—Population Stability Index (PSI)"""
        try:
            # åˆ›å»ºåˆ†ç®±
            baseline_clean = baseline.dropna()
            comparison_clean = comparison.dropna()
            
            if len(baseline_clean) == 0 or len(comparison_clean) == 0:
                return np.nan
            
            # ä½¿ç”¨åŸºçº¿æ•°æ®åˆ›å»ºåˆ†ç®±è¾¹ç•Œ
            bin_edges = np.percentile(baseline_clean, np.linspace(0, 100, bins + 1))
            bin_edges = np.unique(bin_edges)  # ç§»é™¤é‡å¤å€¼
            
            if len(bin_edges) < 2:
                return np.nan
            
            # è®¡ç®—åˆ†å¸ƒ
            baseline_dist, _ = np.histogram(baseline_clean, bins=bin_edges)
            comparison_dist, _ = np.histogram(comparison_clean, bins=bin_edges)
            
            # è½¬æ¢ä¸ºæ¦‚ç‡
            baseline_dist = baseline_dist / np.sum(baseline_dist)
            comparison_dist = comparison_dist / np.sum(comparison_dist)
            
            # é¿å…é›¶æ¦‚ç‡
            baseline_dist = np.where(baseline_dist == 0, 0.0001, baseline_dist)
            comparison_dist = np.where(comparison_dist == 0, 0.0001, comparison_dist)
            
            # è®¡ç®—PSI
            psi = np.sum((comparison_dist - baseline_dist) * np.log(comparison_dist / baseline_dist))
            
            return psi
            
        except Exception as e:
            self.logger.warning(f"PSI calculation failed: {e}")
            return np.nan
    
    def _classify_stability(self, psi_score: float) -> str:
        """æ ¹æ®PSIåˆ†æ•°åˆ†ç±»ç¨³å®šæ€§"""
        if np.isnan(psi_score):
            return "unknown"
        elif psi_score < 0.1:
            return "stable"
        elif psi_score < 0.2:
            return "moderately_stable"
        else:
            return "unstable"
    
    def detect_multicollinearity(self, features: pd.DataFrame) -> Dict:
        """æ£€æµ‹å¤šé‡å…±çº¿æ€§"""
        numeric_features = features.select_dtypes(include=[np.number])
        correlation_matrix = numeric_features.corr()
        
        # æ‰¾å‡ºé«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = abs(correlation_matrix.iloc[i, j])
                if corr_value > self.config.correlation_threshold:
                    high_corr_pairs.append({
                        'feature1': correlation_matrix.columns[i],
                        'feature2': correlation_matrix.columns[j],
                        'correlation': corr_value
                    })
        
        return {
            'high_correlation_pairs': high_corr_pairs,
            'correlation_matrix': correlation_matrix,
            'multicollinearity_detected': len(high_corr_pairs) > 0
        }

class DipMasterFeatureEngine:
    """DipMaster Enhanced V4 æ ¸å¿ƒç‰¹å¾ç”Ÿæˆå¼•æ“"""
    
    def __init__(self, config: FeatureEngineConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.validator = FeatureQualityValidator(config)
        
    def generate_dipmaster_core_features(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """ç”ŸæˆDipMasteræ ¸å¿ƒç‰¹å¾"""
        features = df.copy()
        
        # åŸºç¡€æŠ€æœ¯æŒ‡æ ‡
        features['rsi'] = ta.RSI(features['close'], timeperiod=self.config.rsi_period)
        features['bb_upper'], features['bb_middle'], features['bb_lower'] = ta.BBANDS(
            features['close'], 
            timeperiod=self.config.bollinger_period, 
            nbdevup=self.config.bollinger_std,
            nbdevdn=self.config.bollinger_std
        )
        
        # DipMasteræ ¸å¿ƒä¿¡å·ç‰¹å¾
        features['rsi_in_dip_zone'] = (
            (features['rsi'] >= self.config.rsi_entry_range[0]) & 
            (features['rsi'] <= self.config.rsi_entry_range[1])
        ).astype(int)
        
        # ä»·æ ¼ç›¸å¯¹å¸ƒæ—å¸¦ä½ç½®
        features['bb_position'] = (features['close'] - features['bb_lower']) / (features['bb_upper'] - features['bb_lower'])
        features['bb_squeeze'] = (features['bb_upper'] - features['bb_lower']) / features['bb_middle']
        
        # é€¢è·Œç¡®è®¤ä¿¡å·
        features['price_dip_1m'] = (features['close'] < features['open']).astype(int)
        features['price_dip_5m'] = (features['close'] < features['close'].shift(1)).astype(int)
        features['price_dip_magnitude'] = (features['close'] - features['open']) / features['open']
        
        # æˆäº¤é‡ç¡®è®¤
        features['volume_ma'] = ta.SMA(features['volume'], timeperiod=self.config.volume_ma_period)
        features['volume_ratio'] = features['volume'] / features['volume_ma']
        features['volume_spike'] = (features['volume_ratio'] > 1.5).astype(int)
        
        # æˆäº¤é‡ä»·æ ¼ç¡®è®¤
        features['volume_price_trend'] = np.where(
            features['close'] > features['open'],
            features['volume'],  # ä¸Šæ¶¨æ—¶çš„æˆäº¤é‡
            -features['volume']  # ä¸‹è·Œæ—¶çš„æˆäº¤é‡ï¼ˆè´Ÿå€¼ï¼‰
        )
        features['vpt'] = features['volume_price_trend'].cumsum()  # Volume Price Trend
        
        # DipMasterç»¼åˆä¿¡å·å¼ºåº¦
        features['dipmaster_signal_strength'] = (
            features['rsi_in_dip_zone'] * 0.3 +
            features['price_dip_1m'] * 0.2 +
            features['volume_spike'] * 0.2 +
            (features['bb_position'] < 0.3).astype(int) * 0.3
        )
        
        return features
    
    def generate_microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç”Ÿæˆå¸‚åœºå¾®è§‚ç»“æ„ç‰¹å¾"""
        features = df.copy()
        
        # ä»·æ ¼æ³¢åŠ¨ç‡ç‰¹å¾
        for window in self.config.volatility_windows:
            features[f'volatility_{window}'] = features['close'].pct_change().rolling(window).std() * np.sqrt(288)  # å¹´åŒ–æ³¢åŠ¨ç‡
            features[f'volatility_regime_{window}'] = pd.qcut(
                features[f'volatility_{window}'], 
                q=3, 
                labels=['low', 'medium', 'high']
            ).cat.codes
        
        # GARCHæ³¢åŠ¨ç‡æ¨¡æ‹Ÿ (ç®€åŒ–ç‰ˆ)
        returns = features['close'].pct_change()
        features['garch_volatility'] = returns.ewm(alpha=0.1).std()
        
        # ä»·æ ¼åŠ¨é‡ç‰¹å¾
        for period in self.config.momentum_periods:
            features[f'momentum_{period}'] = features['close'].pct_change(period)
            features[f'momentum_strength_{period}'] = abs(features[f'momentum_{period}'])
            
        # ä»·æ ¼åŠ é€Ÿåº¦
        features['price_acceleration'] = features['close'].pct_change().diff()
        
        # æµåŠ¨æ€§ä»£ç†æŒ‡æ ‡
        features['price_impact'] = abs(features['close'].pct_change()) / (features['volume'] + 1)
        features['turnover_rate'] = features['volume'] / features['volume'].rolling(20).mean()
        
        # è®¢å•æµä¸å¹³è¡¡ä»£ç† (åŸºäºä»·æ ¼å’Œæˆäº¤é‡)
        features['buying_pressure'] = np.where(
            features['close'] > features['open'],
            features['volume'],
            0
        )
        features['selling_pressure'] = np.where(
            features['close'] < features['open'],
            features['volume'],
            0
        )
        
        features['order_flow_imbalance'] = (
            (features['buying_pressure'] - features['selling_pressure']) / 
            (features['buying_pressure'] + features['selling_pressure'] + 1)
        )
        
        # æ—¶é—´å‘¨æœŸç‰¹å¾
        features['hour'] = pd.to_datetime(features['timestamp']).dt.hour
        features['day_of_week'] = pd.to_datetime(features['timestamp']).dt.dayofweek
        features['is_weekend'] = (features['day_of_week'] >= 5).astype(int)
        features['is_asian_session'] = ((features['hour'] >= 0) & (features['hour'] < 8)).astype(int)
        features['is_european_session'] = ((features['hour'] >= 8) & (features['hour'] < 16)).astype(int)
        features['is_american_session'] = ((features['hour'] >= 16) & (features['hour'] <= 23)).astype(int)
        
        return features
    
    def generate_cross_timeframe_features(self, df_5m: pd.DataFrame) -> pd.DataFrame:
        """ç”Ÿæˆè·¨æ—¶é—´æ¡†æ¶ç‰¹å¾"""
        features = df_5m.copy()
        
        # èšåˆåˆ°15åˆ†é’Ÿå’Œ1å°æ—¶
        df_15m = self._aggregate_timeframe(df_5m, '15T')
        df_1h = self._aggregate_timeframe(df_5m, '1H')
        
        # 15åˆ†é’Ÿæ—¶é—´æ¡†æ¶ç‰¹å¾
        df_15m['rsi_15m'] = ta.RSI(df_15m['close'], timeperiod=14)
        df_15m['ema_10_15m'] = ta.EMA(df_15m['close'], timeperiod=10)
        df_15m['ema_20_15m'] = ta.EMA(df_15m['close'], timeperiod=20)
        df_15m['trend_15m'] = np.where(df_15m['ema_10_15m'] > df_15m['ema_20_15m'], 1, 0)
        
        # 1å°æ—¶æ—¶é—´æ¡†æ¶ç‰¹å¾
        df_1h['rsi_1h'] = ta.RSI(df_1h['close'], timeperiod=14)
        df_1h['ema_10_1h'] = ta.EMA(df_1h['close'], timeperiod=10)
        df_1h['ema_20_1h'] = ta.EMA(df_1h['close'], timeperiod=20)
        df_1h['trend_1h'] = np.where(df_1h['ema_10_1h'] > df_1h['ema_20_1h'], 1, 0)
        
        # å°†é«˜æ—¶é—´æ¡†æ¶ç‰¹å¾æ˜ å°„å›5åˆ†é’Ÿæ•°æ®
        features = self._map_higher_timeframe_features(features, df_15m, '15m')
        features = self._map_higher_timeframe_features(features, df_1h, '1h')
        
        # å¤šæ—¶é—´æ¡†æ¶ä¸€è‡´æ€§ç‰¹å¾
        features['rsi_alignment'] = (
            (features['rsi'] > 30).astype(int) +
            (features['rsi_15m'] > 30).astype(int) +
            (features['rsi_1h'] > 30).astype(int)
        )
        
        features['trend_alignment'] = (
            (features['close'] > features['close'].rolling(20).mean()).astype(int) +
            features['trend_15m'] +
            features['trend_1h']
        )
        
        # åŠ¨é‡æ”¶æ•›æ€§
        features['momentum_convergence'] = (
            features['close'].pct_change(5) * 
            features['close'].pct_change(15) * 
            features['close'].pct_change(60)
        )
        
        return features
    
    def _aggregate_timeframe(self, df: pd.DataFrame, timeframe: str) -> pd.DataFrame:
        """èšåˆåˆ°æŒ‡å®šæ—¶é—´æ¡†æ¶"""
        df_copy = df.copy()
        df_copy['timestamp'] = pd.to_datetime(df_copy['timestamp'])
        df_copy.set_index('timestamp', inplace=True)
        
        agg_dict = {
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum'
        }
        
        aggregated = df_copy.resample(timeframe).agg(agg_dict).dropna()
        aggregated.reset_index(inplace=True)
        
        return aggregated
    
    def _map_higher_timeframe_features(self, df_5m: pd.DataFrame, df_higher: pd.DataFrame, suffix: str) -> pd.DataFrame:
        """å°†é«˜æ—¶é—´æ¡†æ¶ç‰¹å¾æ˜ å°„åˆ°5åˆ†é’Ÿæ•°æ®"""
        df_5m_copy = df_5m.copy()
        df_5m_copy['timestamp'] = pd.to_datetime(df_5m_copy['timestamp'])
        df_higher_copy = df_higher.copy()
        df_higher_copy['timestamp'] = pd.to_datetime(df_higher_copy['timestamp'])
        
        # åˆ›å»ºæ—¶é—´åŒºé—´æ˜ å°„
        if suffix == '15m':
            df_5m_copy['time_group'] = df_5m_copy['timestamp'].dt.floor('15T')
        else:  # 1h
            df_5m_copy['time_group'] = df_5m_copy['timestamp'].dt.floor('1H')
        
        # åˆå¹¶ç‰¹å¾
        feature_cols = [col for col in df_higher_copy.columns if col not in ['open', 'high', 'low', 'close', 'volume', 'timestamp']]
        merge_df = df_higher_copy[['timestamp'] + feature_cols].rename(columns={'timestamp': 'time_group'})
        
        result = df_5m_copy.merge(merge_df, on='time_group', how='left')
        result.drop('time_group', axis=1, inplace=True)
        
        return result
    
    def generate_cross_asset_features(self, all_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """ç”Ÿæˆè·¨èµ„äº§ç‰¹å¾"""
        # ç¡®ä¿æ‰€æœ‰æ•°æ®æœ‰ç›¸åŒçš„æ—¶é—´æˆ³
        common_timestamps = None
        for symbol, df in all_data.items():
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            if common_timestamps is None:
                common_timestamps = set(df['timestamp'])
            else:
                common_timestamps = common_timestamps.intersection(set(df['timestamp']))
        
        common_timestamps = sorted(common_timestamps)
        
        # åˆ›å»ºä»·æ ¼çŸ©é˜µ
        price_matrix = pd.DataFrame(index=common_timestamps)
        return_matrix = pd.DataFrame(index=common_timestamps)
        
        for symbol, df in all_data.items():
            df_aligned = df[df['timestamp'].isin(common_timestamps)].set_index('timestamp')
            price_matrix[symbol] = df_aligned['close']
            return_matrix[symbol] = df_aligned['close'].pct_change()
        
        # è®¡ç®—ç›¸å…³æ€§ç‰¹å¾
        for symbol in all_data.keys():
            df = all_data[symbol].copy()
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # ä¸BTCçš„ç›¸å…³æ€§
            if 'BTCUSDT' in price_matrix.columns and symbol != 'BTCUSDT':
                rolling_corr = return_matrix[symbol].rolling(window=50).corr(return_matrix['BTCUSDT'])
                corr_df = rolling_corr.reset_index()
                corr_df.columns = ['timestamp', 'btc_correlation']
                df = df.merge(corr_df, on='timestamp', how='left')
            else:
                df['btc_correlation'] = 0.0
            
            # ä¸ETHçš„ç›¸å…³æ€§
            if 'ETHUSDT' in price_matrix.columns and symbol != 'ETHUSDT':
                rolling_corr = return_matrix[symbol].rolling(window=50).corr(return_matrix['ETHUSDT'])
                corr_df = rolling_corr.reset_index()
                corr_df.columns = ['timestamp', 'eth_correlation']
                df = df.merge(corr_df, on='timestamp', how='left')
            else:
                df['eth_correlation'] = 0.0
            
            # ç›¸å¯¹å¼ºåº¦ (ç›¸å¯¹äºå¸‚åœºå¹³å‡)
            market_avg_return = return_matrix.mean(axis=1)
            if symbol in return_matrix.columns:
                relative_strength = return_matrix[symbol] - market_avg_return
                rs_df = relative_strength.reset_index()
                rs_df.columns = ['timestamp', 'relative_strength']
                df = df.merge(rs_df, on='timestamp', how='left')
            else:
                df['relative_strength'] = 0.0
            
            # è¡Œä¸šbeta (ç®€åŒ–ç‰ˆ)
            if len(return_matrix.columns) > 1 and symbol in return_matrix.columns:
                market_return = return_matrix.drop(symbol, axis=1).mean(axis=1)
                rolling_beta = return_matrix[symbol].rolling(window=50).cov(market_return) / market_return.rolling(window=50).var()
                beta_df = rolling_beta.reset_index()
                beta_df.columns = ['timestamp', 'market_beta']
                df = df.merge(beta_df, on='timestamp', how='left')
            else:
                df['market_beta'] = 1.0
            
            all_data[symbol] = df
        
        return all_data

class LabelGenerator:
    """æ ‡ç­¾ç”Ÿæˆå™¨ - ä¸ºç›‘ç£å­¦ä¹ åˆ›å»ºé«˜è´¨é‡æ ‡ç­¾"""
    
    def __init__(self, config: FeatureEngineConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    def generate_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç”Ÿæˆå®Œæ•´çš„æ ‡ç­¾é›†åˆ"""
        labels = df.copy()
        
        # ä¸»æ ‡ç­¾ï¼šæœªæ¥æ”¶ç›Šç‡
        for horizon in self.config.prediction_horizons:
            labels[f'future_return_{horizon}m'] = (
                labels['close'].shift(-horizon) / labels['close'] - 1
            )
        
        # åˆ†ç±»æ ‡ç­¾ï¼šæ”¶ç›Šç‡åˆ†ç®±
        for horizon in self.config.prediction_horizons:
            return_col = f'future_return_{horizon}m'
            labels[f'return_class_{horizon}m'] = pd.qcut(
                labels[return_col], 
                q=5, 
                labels=['very_negative', 'negative', 'neutral', 'positive', 'very_positive']
            ).cat.codes
        
        # äºŒåˆ†ç±»æ ‡ç­¾ï¼šæ˜¯å¦ç›ˆåˆ©
        for horizon in self.config.prediction_horizons:
            return_col = f'future_return_{horizon}m'
            labels[f'is_profitable_{horizon}m'] = (labels[return_col] > 0).astype(int)
        
        # ç›®æ ‡è¾¾æˆæ ‡ç­¾
        for target in self.config.profit_targets:
            labels[f'hits_target_{target:.1%}'] = self._calculate_target_achievement(
                labels, target, self.config.max_holding_minutes
            )
        
        # é£é™©æ ‡ç­¾
        labels['hits_stop_loss'] = self._calculate_stop_loss_events(
            labels, self.config.stop_loss_threshold, self.config.max_holding_minutes
        )
        
        # æœ€ä¼˜å‡ºåœºæ—¶é—´æ ‡ç­¾
        labels['optimal_exit_time'] = self._calculate_optimal_exit_time(
            labels, self.config.max_holding_minutes
        )
        
        # æŒä»“æœŸé—´æœ€å¤§æ”¶ç›Š/å›æ’¤
        labels['max_profit_during_hold'] = self._calculate_max_profit_during_hold(
            labels, self.config.max_holding_minutes
        )
        labels['max_drawdown_during_hold'] = self._calculate_max_drawdown_during_hold(
            labels, self.config.max_holding_minutes
        )
        
        return labels
    
    def _calculate_target_achievement(self, df: pd.DataFrame, target: float, max_minutes: int) -> pd.Series:
        """è®¡ç®—æ˜¯å¦åœ¨æœ€å¤§æŒä»“æ—¶é—´å†…è¾¾åˆ°ç›®æ ‡æ”¶ç›Š"""
        result = pd.Series(0, index=df.index)
        
        for i in range(len(df) - max_minutes):
            entry_price = df.iloc[i]['close']
            
            for j in range(1, min(max_minutes + 1, len(df) - i)):
                current_price = df.iloc[i + j]['close']
                current_return = (current_price / entry_price) - 1
                
                if current_return >= target:
                    result.iloc[i] = 1
                    break
        
        return result
    
    def _calculate_stop_loss_events(self, df: pd.DataFrame, stop_threshold: float, max_minutes: int) -> pd.Series:
        """è®¡ç®—æ˜¯å¦è§¦å‘æ­¢æŸ"""
        result = pd.Series(0, index=df.index)
        
        for i in range(len(df) - max_minutes):
            entry_price = df.iloc[i]['close']
            
            for j in range(1, min(max_minutes + 1, len(df) - i)):
                current_price = df.iloc[i + j]['close']
                current_return = (current_price / entry_price) - 1
                
                if current_return <= -stop_threshold:
                    result.iloc[i] = 1
                    break
        
        return result
    
    def _calculate_optimal_exit_time(self, df: pd.DataFrame, max_minutes: int) -> pd.Series:
        """è®¡ç®—æœ€ä¼˜å‡ºåœºæ—¶é—´ï¼ˆæ”¶ç›Šæœ€å¤§åŒ–çš„æ—¶é—´ç‚¹ï¼‰"""
        result = pd.Series(np.nan, index=df.index)
        
        for i in range(len(df) - max_minutes):
            entry_price = df.iloc[i]['close']
            max_return = -float('inf')
            optimal_time = 0
            
            for j in range(1, min(max_minutes + 1, len(df) - i)):
                current_price = df.iloc[i + j]['close']
                current_return = (current_price / entry_price) - 1
                
                if current_return > max_return:
                    max_return = current_return
                    optimal_time = j
            
            result.iloc[i] = optimal_time
        
        return result
    
    def _calculate_max_profit_during_hold(self, df: pd.DataFrame, max_minutes: int) -> pd.Series:
        """è®¡ç®—æŒä»“æœŸé—´çš„æœ€å¤§æ”¶ç›Š"""
        result = pd.Series(np.nan, index=df.index)
        
        for i in range(len(df) - max_minutes):
            entry_price = df.iloc[i]['close']
            max_profit = -float('inf')
            
            for j in range(1, min(max_minutes + 1, len(df) - i)):
                current_price = df.iloc[i + j]['close']
                current_return = (current_price / entry_price) - 1
                max_profit = max(max_profit, current_return)
            
            result.iloc[i] = max_profit
        
        return result
    
    def _calculate_max_drawdown_during_hold(self, df: pd.DataFrame, max_minutes: int) -> pd.Series:
        """è®¡ç®—æŒä»“æœŸé—´çš„æœ€å¤§å›æ’¤"""
        result = pd.Series(np.nan, index=df.index)
        
        for i in range(len(df) - max_minutes):
            entry_price = df.iloc[i]['close']
            max_drawdown = 0
            peak_return = 0
            
            for j in range(1, min(max_minutes + 1, len(df) - i)):
                current_price = df.iloc[i + j]['close']
                current_return = (current_price / entry_price) - 1
                peak_return = max(peak_return, current_return)
                drawdown = peak_return - current_return
                max_drawdown = max(max_drawdown, drawdown)
            
            result.iloc[i] = max_drawdown
        
        return result

class FeatureEngineeringPipeline:
    """å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹ç®¡é“"""
    
    def __init__(self, config: FeatureEngineConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.feature_engine = DipMasterFeatureEngine(config)
        self.label_generator = LabelGenerator(config)
        self.quality_validator = FeatureQualityValidator(config)
        
        # ç‰¹å¾å…ƒæ•°æ®
        self.feature_metadata = {}
        
    def load_market_data(self, data_path: str) -> Dict[str, pd.DataFrame]:
        """åŠ è½½å¸‚åœºæ•°æ®"""
        all_data = {}
        data_dir = Path(data_path) / "market_data"
        
        for symbol in self.config.symbols:
            # å°è¯•åŠ è½½parquetæ–‡ä»¶
            parquet_file = data_dir / f"{symbol}_5m_2years.parquet"
            csv_file = data_dir / f"{symbol}_5m_2years.csv"
            
            if parquet_file.exists():
                df = pd.read_parquet(parquet_file)
                self.logger.info(f"Loaded {symbol} from parquet: {len(df)} records")
            elif csv_file.exists():
                df = pd.read_csv(csv_file)
                self.logger.info(f"Loaded {symbol} from CSV: {len(df)} records")
            else:
                self.logger.error(f"No data file found for {symbol}")
                continue
            
            # æ•°æ®é¢„å¤„ç†
            df = self._preprocess_data(df, symbol)
            all_data[symbol] = df
        
        return all_data
    
    def _preprocess_data(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """æ•°æ®é¢„å¤„ç†"""
        # ç¡®ä¿åˆ—åæ ‡å‡†åŒ–
        expected_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        if not all(col in df.columns for col in expected_columns):
            self.logger.error(f"Missing required columns for {symbol}")
            return pd.DataFrame()
        
        # æ—¶é—´æˆ³å¤„ç†
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        # ä»·æ ¼æ•°æ®ç±»å‹è½¬æ¢
        price_columns = ['open', 'high', 'low', 'close']
        for col in price_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        df['volume'] = pd.to_numeric(df['volume'], errors='coerce')
        
        # ç§»é™¤å¼‚å¸¸å€¼
        for col in price_columns:
            Q1 = df[col].quantile(0.01)
            Q3 = df[col].quantile(0.99)
            df[col] = df[col].clip(lower=Q1, upper=Q3)
        
        # æ·»åŠ ç¬¦å·æ ‡è¯†
        df['symbol'] = symbol
        df['symbol_encoded'] = hash(symbol) % 1000  # ç®€å•ç¼–ç 
        
        return df.dropna()
    
    def execute_feature_engineering(self, data_path: str) -> Dict:
        """æ‰§è¡Œå®Œæ•´çš„ç‰¹å¾å·¥ç¨‹ç®¡é“"""
        start_time = time.time()
        
        # 1. åŠ è½½æ•°æ®
        self.logger.info("Loading market data...")
        all_data = self.load_market_data(data_path)
        
        if not all_data:
            raise ValueError("No valid data loaded")
        
        # 2. ç”Ÿæˆæ ¸å¿ƒç‰¹å¾
        self.logger.info("Generating DipMaster core features...")
        for symbol in all_data.keys():
            all_data[symbol] = self.feature_engine.generate_dipmaster_core_features(
                all_data[symbol], symbol
            )
        
        # 3. ç”Ÿæˆå¾®è§‚ç»“æ„ç‰¹å¾
        self.logger.info("Generating microstructure features...")
        for symbol in all_data.keys():
            all_data[symbol] = self.feature_engine.generate_microstructure_features(
                all_data[symbol]
            )
        
        # 4. ç”Ÿæˆè·¨æ—¶é—´æ¡†æ¶ç‰¹å¾
        self.logger.info("Generating cross-timeframe features...")
        for symbol in all_data.keys():
            all_data[symbol] = self.feature_engine.generate_cross_timeframe_features(
                all_data[symbol]
            )
        
        # 5. ç”Ÿæˆè·¨èµ„äº§ç‰¹å¾
        self.logger.info("Generating cross-asset features...")
        all_data = self.feature_engine.generate_cross_asset_features(all_data)
        
        # 6. ç”Ÿæˆæ ‡ç­¾
        self.logger.info("Generating labels...")
        for symbol in all_data.keys():
            all_data[symbol] = self.label_generator.generate_labels(all_data[symbol])
        
        # 7. åˆå¹¶æ‰€æœ‰æ•°æ®
        self.logger.info("Combining all data...")
        combined_data = pd.concat(all_data.values(), ignore_index=True)
        
        # 8. ç‰¹å¾åå¤„ç†
        self.logger.info("Post-processing features...")
        combined_data = self._post_process_features(combined_data)
        
        # 9. æ•°æ®è´¨é‡éªŒè¯
        self.logger.info("Validating data quality...")
        quality_report = self._validate_data_quality(combined_data)
        
        execution_time = time.time() - start_time
        
        result = {
            'features': combined_data,
            'feature_metadata': self.feature_metadata,
            'quality_report': quality_report,
            'execution_time': execution_time,
            'config': asdict(self.config)
        }
        
        self.logger.info(f"Feature engineering completed in {execution_time:.2f} seconds")
        self.logger.info(f"Generated {len(combined_data)} samples with {len(combined_data.columns)} features")
        
        return result
    
    def _post_process_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾åå¤„ç†"""
        # å¡«å……ç¼ºå¤±å€¼
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        # å‰å‘å¡«å……æ—¶é—´åºåˆ—æ•°æ®
        df[numeric_columns] = df[numeric_columns].fillna(method='ffill')
        
        # å‰©ä½™ç¼ºå¤±å€¼ç”¨ä¸­ä½æ•°å¡«å……
        for col in numeric_columns:
            if df[col].isnull().any():
                df[col].fillna(df[col].median(), inplace=True)
        
        # å¼‚å¸¸å€¼å¤„ç† (Winsorization)
        for col in numeric_columns:
            if col not in ['timestamp', 'symbol_encoded', 'hour', 'day_of_week']:
                lower_percentile = self.config.outlier_percentile[0]
                upper_percentile = self.config.outlier_percentile[1]
                
                lower_bound = df[col].quantile(lower_percentile / 100)
                upper_bound = df[col].quantile(upper_percentile / 100)
                
                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
        
        # ç‰¹å¾æ ‡å‡†åŒ– (æ»šåŠ¨çª—å£)
        feature_columns = [col for col in numeric_columns 
                          if not col.startswith(('future_', 'is_profitable_', 'hits_', 'optimal_', 'max_')) 
                          and col not in ['timestamp', 'symbol_encoded', 'hour', 'day_of_week']]
        
        for col in feature_columns:
            # ä½¿ç”¨æ»šåŠ¨çª—å£è¿›è¡Œz-scoreæ ‡å‡†åŒ–
            rolling_mean = df[col].rolling(window=200, min_periods=50).mean()
            rolling_std = df[col].rolling(window=200, min_periods=50).std()
            df[f'{col}_zscore'] = (df[col] - rolling_mean) / (rolling_std + 1e-8)
        
        return df
    
    def _validate_data_quality(self, df: pd.DataFrame) -> Dict:
        """æ•°æ®è´¨é‡éªŒè¯"""
        quality_report = {}
        
        # åŸºç¡€ç»Ÿè®¡
        quality_report['basic_stats'] = {
            'total_samples': len(df),
            'total_features': len(df.columns),
            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,
            'date_range': {
                'start': str(df['timestamp'].min()),
                'end': str(df['timestamp'].max())
            }
        }
        
        # ç¼ºå¤±å€¼åˆ†æ
        missing_analysis = {}
        for col in df.columns:
            missing_pct = df[col].isnull().sum() / len(df)
            if missing_pct > 0:
                missing_analysis[col] = missing_pct
        
        quality_report['missing_values'] = missing_analysis
        
        # å‰è§†åå·®æ£€æµ‹
        quality_report['lookahead_bias_check'] = self.quality_validator.validate_no_lookahead_bias(
            df, self.feature_metadata
        )
        
        # ç‰¹å¾ç¨³å®šæ€§ (ç®€åŒ–ç‰ˆ)
        # å°†æ•°æ®åˆ†æˆå¤šä¸ªæ—¶é—´æ®µè¿›è¡ŒPSIè®¡ç®—
        df_sorted = df.sort_values('timestamp')
        n_periods = 4
        period_size = len(df_sorted) // n_periods
        
        time_periods = []
        for i in range(n_periods):
            start_idx = i * period_size
            end_idx = (i + 1) * period_size if i < n_periods - 1 else len(df_sorted)
            
            start_time = df_sorted.iloc[start_idx]['timestamp']
            end_time = df_sorted.iloc[end_idx - 1]['timestamp']
            time_periods.append((str(start_time), str(end_time)))
        
        stability_results = self.quality_validator.calculate_feature_stability(df_sorted, time_periods)
        quality_report['feature_stability'] = stability_results
        
        # å¤šé‡å…±çº¿æ€§æ£€æµ‹
        correlation_analysis = self.quality_validator.detect_multicollinearity(df)
        quality_report['multicollinearity'] = correlation_analysis
        
        return quality_report
    
    def save_feature_set(self, result: Dict, output_path: str) -> Dict[str, str]:
        """ä¿å­˜ç‰¹å¾é›†"""
        output_dir = Path(output_path)
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ç‰¹å¾æ•°æ® (Parquetæ ¼å¼)
        features_file = output_dir / f"dipmaster_v4_features_{timestamp}.parquet"
        result['features'].to_parquet(features_file, compression='snappy')
        
        # ä¿å­˜ç‰¹å¾é›†å…ƒæ•°æ® (JSONæ ¼å¼)
        metadata_file = output_dir / f"dipmaster_v4_featureset_{timestamp}.json"
        
        feature_set_metadata = {
            'version': '4.0.0',
            'strategy_name': 'DipMaster_Enhanced_V4',
            'created_timestamp': timestamp,
            'data_summary': {
                'total_samples': len(result['features']),
                'total_features': len(result['features'].columns),
                'symbols': result['config']['symbols'],
                'date_range': {
                    'start': str(result['features']['timestamp'].min()),
                    'end': str(result['features']['timestamp'].max())
                }
            },
            'feature_categories': {
                'dipmaster_core': [col for col in result['features'].columns if any(x in col for x in ['rsi', 'bb_', 'dipmaster_', 'volume_'])],
                'microstructure': [col for col in result['features'].columns if any(x in col for x in ['volatility_', 'momentum_', 'order_flow', 'buying_', 'selling_'])],
                'cross_timeframe': [col for col in result['features'].columns if any(x in col for x in ['_15m', '_1h', 'alignment', 'convergence'])],
                'cross_asset': [col for col in result['features'].columns if any(x in col for x in ['_correlation', 'relative_strength', 'market_beta'])],
                'labels': [col for col in result['features'].columns if any(x in col for x in ['future_', 'return_class', 'is_profitable', 'hits_', 'optimal_', 'max_'])]
            },
            'quality_metrics': result['quality_report'],
            'configuration': result['config'],
            'execution_metadata': {
                'execution_time_seconds': result['execution_time'],
                'feature_engineering_version': '4.0.0'
            }
        }
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(feature_set_metadata, f, indent=2, ensure_ascii=False, default=str)
        
        # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        quality_report_file = output_dir / f"dipmaster_v4_quality_report_{timestamp}.json"
        with open(quality_report_file, 'w', encoding='utf-8') as f:
            json.dump(result['quality_report'], f, indent=2, ensure_ascii=False, default=str)
        
        return {
            'features_file': str(features_file),
            'metadata_file': str(metadata_file),
            'quality_report_file': str(quality_report_file)
        }

# ä½¿ç”¨ç¤ºä¾‹å’Œä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•° - æ‰§è¡ŒDipMaster Enhanced V4ç‰¹å¾å·¥ç¨‹ç®¡é“"""
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»ºé…ç½®
    config = FeatureEngineConfig(
        symbols=[
            "BTCUSDT", "ETHUSDT", "SOLUSDT", "ADAUSDT", "XRPUSDT",
            "BNBUSDT", "DOGEUSDT", "SUIUSDT", "ICPUSDT", "ALGOUSDT", "IOTAUSDT"
        ],
        primary_timeframe="5m",
        analysis_timeframes=["5m", "15m", "1h"],
        prediction_horizons=[15, 30, 60],  # 15åˆ†é’Ÿï¼Œ30åˆ†é’Ÿï¼Œ1å°æ—¶
        profit_targets=[0.006, 0.012, 0.020]  # 0.6%, 1.2%, 2.0%
    )
    
    # åˆ›å»ºç‰¹å¾å·¥ç¨‹ç®¡é“
    pipeline = FeatureEngineeringPipeline(config)
    
    # æ‰§è¡Œç‰¹å¾å·¥ç¨‹
    try:
        result = pipeline.execute_feature_engineering("G:/Github/Quant/DipMaster-Trading-System/data")
        
        # ä¿å­˜ç»“æœ
        file_paths = pipeline.save_feature_set(result, "G:/Github/Quant/DipMaster-Trading-System/data")
        
        print("\n" + "="*80)
        print("DIPMASTER ENHANCED V4 ç‰¹å¾å·¥ç¨‹å®Œæˆ!")
        print("="*80)
        print(f"âœ… æ€»æ ·æœ¬æ•°: {result['quality_report']['basic_stats']['total_samples']:,}")
        print(f"âœ… æ€»ç‰¹å¾æ•°: {result['quality_report']['basic_stats']['total_features']:,}")
        print(f"âœ… æ‰§è¡Œæ—¶é—´: {result['execution_time']:.2f} ç§’")
        print(f"âœ… å†…å­˜ä½¿ç”¨: {result['quality_report']['basic_stats']['memory_usage_mb']:.1f} MB")
        print("\næ–‡ä»¶è¾“å‡º:")
        for file_type, file_path in file_paths.items():
            print(f"ğŸ“ {file_type}: {file_path}")
        
        # è´¨é‡è¯„ä¼°æ‘˜è¦
        print("\nè´¨é‡è¯„ä¼°æ‘˜è¦:")
        quality = result['quality_report']
        
        if quality['lookahead_bias_check']['has_lookahead_bias']:
            print("âš ï¸  æ£€æµ‹åˆ°å‰è§†åå·®é£é™©")
        else:
            print("âœ… æ— å‰è§†åå·®")
            
        if quality['multicollinearity']['multicollinearity_detected']:
            print(f"âš ï¸  æ£€æµ‹åˆ° {len(quality['multicollinearity']['high_correlation_pairs'])} å¯¹é«˜ç›¸å…³ç‰¹å¾")
        else:
            print("âœ… æ— å¤šé‡å…±çº¿æ€§é—®é¢˜")
        
        # ç‰¹å¾ç¨³å®šæ€§ç»Ÿè®¡
        stable_features = sum(1 for v in quality['feature_stability'].values() 
                             if v['stability_rating'] == 'stable')
        total_features = len(quality['feature_stability'])
        print(f"âœ… ç‰¹å¾ç¨³å®šæ€§: {stable_features}/{total_features} ({stable_features/total_features*100:.1f}%) ç¨³å®š")
        
        print("\nğŸ¯ ç‰¹å¾å·¥ç¨‹ç®¡é“å·²å‡†å¤‡å°±ç»ªï¼Œå¯ç”¨äºDipMaster Enhanced V4ç­–ç•¥è®­ç»ƒ!")
        
    except Exception as e:
        logging.error(f"Feature engineering failed: {e}")
        raise

if __name__ == "__main__":
    main()