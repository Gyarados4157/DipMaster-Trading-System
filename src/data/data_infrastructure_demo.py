"""
æ•°æ®åŸºç¡€è®¾æ–½æ¼”ç¤ºå’ŒéªŒè¯è„šæœ¬
å±•ç¤ºDipMaster Enhanced V4æ•°æ®åŸºç¡€è®¾æ–½çš„å®Œæ•´åŠŸèƒ½
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

async def demo_data_infrastructure():
    """æ¼”ç¤ºæ•°æ®åŸºç¡€è®¾æ–½åŠŸèƒ½"""
    logger = logging.getLogger(__name__)
    
    logger.info("=== DipMaster Enhanced V4 æ•°æ®åŸºç¡€è®¾æ–½æ¼”ç¤º ===")
    
    # 1. éªŒè¯MarketDataBundleé…ç½®
    bundle_path = project_root / 'data' / 'MarketDataBundle.json'
    
    if not bundle_path.exists():
        logger.error("MarketDataBundle.json ä¸å­˜åœ¨")
        return False
    
    with open(bundle_path, 'r', encoding='utf-8') as f:
        bundle_config = json.load(f)
    
    logger.info(f"âœ“ æ•°æ®åŒ…ç‰ˆæœ¬: {bundle_config['version']}")
    logger.info(f"âœ“ æ•°æ®åŒ…ID: {bundle_config['metadata']['bundle_id']}")
    logger.info(f"âœ“ äº¤æ˜“å¯¹æ•°é‡: {len(bundle_config['metadata']['symbols'])}")
    logger.info(f"âœ“ æ•°æ®è´¨é‡è¯„åˆ†: {bundle_config['metadata']['data_quality_score']}")
    logger.info(f"âœ“ æ€»æ•°æ®é‡: {bundle_config['metadata']['total_size_mb']} MB")
    
    # 2. éªŒè¯æ•°æ®æ–‡ä»¶å®Œæ•´æ€§
    logger.info("\n=== æ•°æ®æ–‡ä»¶å®Œæ•´æ€§éªŒè¯ ===")
    
    total_files = 0
    missing_files = 0
    total_records = 0
    
    for symbol in bundle_config['metadata']['symbols']:
        # æ£€æŸ¥ä¸»è¦æ•°æ®æ–‡ä»¶
        if symbol in ['BTCUSDT', 'ETHUSDT']:
            file_path = project_root / 'data' / 'market_data' / f'{symbol}_5m_2years.parquet'
            metadata_path = project_root / 'data' / 'market_data' / f'{symbol}_5m_2years_metadata.json'
        else:
            file_path = project_root / 'data' / 'market_data' / f'{symbol}_5m_2years.csv'
            metadata_path = project_root / 'data' / 'market_data' / f'{symbol}_5m_2years_metadata.json'
        
        total_files += 1
        
        if file_path.exists() and metadata_path.exists():
            try:
                # è¯»å–å…ƒæ•°æ®
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                records = metadata.get('records_count', 0)
                total_records += records
                
                logger.info(f"âœ“ {symbol}: {records:,}æ¡è®°å½•, {file_size_mb:.1f}MB")
                
            except Exception as e:
                logger.error(f"âœ— {symbol}: è¯»å–å¤±è´¥ - {e}")
                missing_files += 1
        else:
            logger.error(f"âœ— {symbol}: æ–‡ä»¶ç¼ºå¤±")
            missing_files += 1
    
    # 3. æ•°æ®è´¨é‡æ£€æŸ¥
    logger.info("\n=== æ•°æ®è´¨é‡æ£€æŸ¥ ===")
    
    # æ£€æŸ¥BTCUSDTæ•°æ®æ ·æœ¬
    btc_file = project_root / 'data' / 'market_data' / 'BTCUSDT_5m_2years.parquet'
    
    if btc_file.exists():
        try:
            df = pd.read_parquet(btc_file)
            
            logger.info(f"âœ“ BTCUSDTæ•°æ®åŠ è½½æˆåŠŸ: {len(df):,}æ¡è®°å½•")
            logger.info(f"âœ“ æ—¶é—´èŒƒå›´: {df['timestamp'].min()} - {df['timestamp'].max()}")
            logger.info(f"âœ“ æ•°æ®åˆ—: {list(df.columns)}")
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            null_count = df.isnull().sum().sum()
            logger.info(f"âœ“ ç©ºå€¼æ•°é‡: {null_count}")
            
            if 'close' in df.columns:
                price_stats = df['close'].describe()
                logger.info(f"âœ“ ä»·æ ¼ç»Ÿè®¡: æœ€å°å€¼={price_stats['min']:.2f}, æœ€å¤§å€¼={price_stats['max']:.2f}, å‡å€¼={price_stats['mean']:.2f}")
                
                # OHLCå…³ç³»æ£€æŸ¥
                ohlc_valid = (
                    (df['high'] >= df['open']) & 
                    (df['high'] >= df['low']) & 
                    (df['high'] >= df['close']) &
                    (df['low'] <= df['open']) & 
                    (df['low'] <= df['close'])
                ).all()
                
                logger.info(f"âœ“ OHLCå…³ç³»æœ‰æ•ˆæ€§: {ohlc_valid}")
            
        except Exception as e:
            logger.error(f"âœ— BTCUSDTæ•°æ®æ£€æŸ¥å¤±è´¥: {e}")
    
    # 4. æ€§èƒ½åŸºå‡†æµ‹è¯•
    logger.info("\n=== æ€§èƒ½åŸºå‡†æµ‹è¯• ===")
    
    try:
        # æµ‹è¯•æ•°æ®åŠ è½½é€Ÿåº¦
        start_time = datetime.now()
        df = pd.read_parquet(btc_file)
        load_time = (datetime.now() - start_time).total_seconds() * 1000
        
        logger.info(f"âœ“ æ•°æ®åŠ è½½é€Ÿåº¦: {load_time:.1f}ms ({len(df):,}æ¡è®°å½•)")
        
        # æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
        start_time = datetime.now()
        recent_data = df.tail(1000)  # æœ€è¿‘1000æ¡è®°å½•
        query_time = (datetime.now() - start_time).total_seconds() * 1000
        
        logger.info(f"âœ“ æŸ¥è¯¢æ€§èƒ½: {query_time:.1f}ms (1000æ¡è®°å½•)")
        
        # è®¡ç®—å­˜å‚¨æ•ˆç‡
        memory_usage = df.memory_usage(deep=True).sum() / (1024 * 1024)
        file_size = btc_file.stat().st_size / (1024 * 1024)
        compression_ratio = file_size / memory_usage
        
        logger.info(f"âœ“ å­˜å‚¨æ•ˆç‡: å‹ç¼©æ¯”={compression_ratio:.2f}, æ–‡ä»¶å¤§å°={file_size:.1f}MB, å†…å­˜ä½¿ç”¨={memory_usage:.1f}MB")
        
    except Exception as e:
        logger.error(f"âœ— æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
    
    # 5. æ€»ç»“æŠ¥å‘Š
    logger.info("\n=== åŸºç¡€è®¾æ–½çŠ¶æ€æ€»ç»“ ===")
    
    success_rate = ((total_files - missing_files) / total_files * 100) if total_files > 0 else 0
    
    logger.info(f"æ–‡ä»¶å®Œæ•´æ€§: {total_files - missing_files}/{total_files} ({success_rate:.1f}%)")
    logger.info(f"æ€»æ•°æ®è®°å½•: {total_records:,}")
    logger.info(f"é¢„ä¼°æ•°æ®è¦†ç›–: 2å¹´å†å²æ•°æ®")
    
    if success_rate >= 90:
        logger.info("âœ… æ•°æ®åŸºç¡€è®¾æ–½çŠ¶æ€: ä¼˜ç§€")
        status = "excellent"
    elif success_rate >= 70:
        logger.info("âš ï¸ æ•°æ®åŸºç¡€è®¾æ–½çŠ¶æ€: è‰¯å¥½")
        status = "good"
    else:
        logger.info("âŒ æ•°æ®åŸºç¡€è®¾æ–½çŠ¶æ€: éœ€è¦æ”¹è¿›")
        status = "needs_improvement"
    
    # 6. ç”ŸæˆçŠ¶æ€æŠ¥å‘Š
    report = {
        "validation_time": datetime.now().isoformat(),
        "infrastructure_status": status,
        "summary": {
            "total_files": total_files,
            "missing_files": missing_files,
            "success_rate": success_rate,
            "total_records": total_records,
            "data_quality_score": bundle_config['metadata']['data_quality_score']
        },
        "performance_benchmarks": {
            "data_load_time_ms": load_time if 'load_time' in locals() else None,
            "query_time_ms": query_time if 'query_time' in locals() else None,
            "compression_ratio": compression_ratio if 'compression_ratio' in locals() else None
        },
        "recommendations": []
    }
    
    if missing_files > 0:
        report["recommendations"].append(f"éœ€è¦ä¸‹è½½æˆ–ä¿®å¤{missing_files}ä¸ªç¼ºå¤±çš„æ•°æ®æ–‡ä»¶")
    
    if success_rate < 100:
        report["recommendations"].append("å»ºè®®æ‰§è¡Œå®Œæ•´çš„æ•°æ®è´¨é‡éªŒè¯")
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = project_root / 'data' / f'infrastructure_validation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nâœ… éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    return status == "excellent"

def show_usage_examples():
    """æ˜¾ç¤ºæ•°æ®åŸºç¡€è®¾æ–½ä½¿ç”¨ç¤ºä¾‹"""
    logger = logging.getLogger(__name__)
    
    logger.info("\n=== æ•°æ®åŸºç¡€è®¾æ–½ä½¿ç”¨ç¤ºä¾‹ ===")
    
    examples = [
        {
            "title": "åŠ è½½å†å²Kçº¿æ•°æ®",
            "code": """
# åŠ è½½BTCUSDT 5åˆ†é’ŸKçº¿æ•°æ®
import pandas as pd

df = pd.read_parquet('data/market_data/BTCUSDT_5m_2years.parquet')
print(f"æ•°æ®èŒƒå›´: {df['timestamp'].min()} - {df['timestamp'].max()}")
print(f"æ•°æ®é‡: {len(df):,} æ¡è®°å½•")
            """
        },
        {
            "title": "å®æ—¶æ•°æ®æµé›†æˆ",
            "code": """
# ä½¿ç”¨æ•°æ®åŸºç¡€è®¾æ–½çš„å®æ—¶æµ
from src.data.realtime_stream import RealtimeDataStream

config = {'realtime': {'buffer_size': 10000}}
stream = RealtimeDataStream(config)

# è®¢é˜…ä»·æ ¼æ›´æ–°
async def price_handler(data):
    print(f"ä»·æ ¼æ›´æ–°: {data['symbol']} = {data['price']}")

stream.subscribe('ticker_BTCUSDT', price_handler)
await stream.connect(['BTCUSDT', 'ETHUSDT'])
            """
        },
        {
            "title": "æ•°æ®è´¨é‡ç›‘æ§",
            "code": """
# ä½¿ç”¨æ•°æ®ç›‘æ§ç³»ç»Ÿ
from src.data.data_monitor import DataMonitor

monitor = DataMonitor(config)
await monitor.start_monitoring()

# è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€
status = await monitor.get_system_status()
print(f"æ•°æ®è´¨é‡è¯„åˆ†: {status['overall_health_score']}")
            """
        },
        {
            "title": "é«˜æ€§èƒ½æ•°æ®æŸ¥è¯¢",
            "code": """
# ä½¿ç”¨å­˜å‚¨ç®¡ç†å™¨è¿›è¡Œä¼˜åŒ–æŸ¥è¯¢
from src.data.storage_manager import StorageManager

storage = StorageManager(config)

# åŠ è½½æŒ‡å®šæ—¶é—´èŒƒå›´çš„æ•°æ®
df = await storage.load_kline_data(
    symbol='BTCUSDT',
    timeframe='5m',
    start_date='2024-01-01',
    end_date='2024-12-31',
    columns=['timestamp', 'open', 'high', 'low', 'close', 'volume']
)
            """
        }
    ]
    
    for i, example in enumerate(examples, 1):
        logger.info(f"\n{i}. {example['title']}:")
        logger.info(example['code'])

async def main():
    """ä¸»å‡½æ•°"""
    logger = logging.getLogger(__name__)
    
    try:
        # è¿è¡ŒåŸºç¡€è®¾æ–½éªŒè¯
        success = await demo_data_infrastructure()
        
        # æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹
        show_usage_examples()
        
        if success:
            logger.info("\nğŸ‰ DipMaster Enhanced V4 æ•°æ®åŸºç¡€è®¾æ–½éªŒè¯æˆåŠŸ!")
            logger.info("ç³»ç»Ÿå·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ç­–ç•¥å¼€å‘å’Œæµ‹è¯•ã€‚")
        else:
            logger.warning("\nâš ï¸ æ•°æ®åŸºç¡€è®¾æ–½å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå»ºè®®è¿›è¡Œä¿®å¤ã€‚")
        
        return success
        
    except Exception as e:
        logger.error(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)