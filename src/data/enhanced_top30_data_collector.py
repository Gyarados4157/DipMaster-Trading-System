"""
Enhanced Top 30 Altcoins Data Collection Infrastructure V2
å¢å¼ºç‰ˆå‰30å¸‚å€¼å±±å¯¨å¸æ•°æ®æ”¶é›†åŸºç¡€è®¾æ–½

æ–°å¢ç‰¹æ€§ï¼š
- æ”¯æŒ6ä¸ªæ—¶é—´æ¡†æ¶ï¼š1m, 5m, 15m, 1h, 4h, 1d
- é«˜åº¦å¹¶è¡ŒåŒ–ä¸‹è½½ï¼ˆæ¯æ‰¹æ¬¡5ä¸ªå¸ç§åŒæ—¶å¤„ç†ï¼‰
- æ™ºèƒ½æ•°æ®è´¨é‡è¯„ä¼°å’Œå¼‚å¸¸æ£€æµ‹
- è‡ªåŠ¨æ›¿æ¢ä½è´¨é‡å¸ç§
- å®æ—¶è¿›åº¦ç›‘æ§å’Œæ€§èƒ½ä¼˜åŒ–
- ç”Ÿæˆå¢å¼ºçš„MarketDataBundleé…ç½®
"""

import asyncio
import ccxt
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import logging
from pathlib import Path
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Tuple, Any
import warnings
import aiohttp
import ssl
from collections import defaultdict
warnings.filterwarnings('ignore')

@dataclass
class EnhancedCoinInfo:
    """å¢å¼ºå¸ç§ä¿¡æ¯"""
    symbol: str
    name: str
    category: str
    market_cap_rank: int
    priority: int
    daily_volume_usd: float
    exchange_support: List[str]
    liquidity_tier: str
    volatility_factor: float
    correlation_group: str

class EnhancedTop30DataCollector:
    """å¢å¼ºç‰ˆå‰30å¤§å±±å¯¨å¸æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self):
        self.logger = self.setup_logging()
        
        # å¢å¼ºç‰ˆå‰30å¤§å¸‚å€¼å±±å¯¨å¸é…ç½®ï¼ˆåŸºäº2025å¹´8æœˆæ•°æ®ï¼‰
        self.top30_altcoins = {
            # é¡¶çº§å±±å¯¨å¸ (Tier 1)
            "XRPUSDT": EnhancedCoinInfo("XRPUSDT", "XRP", "Payment", 3, 1, 2500000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§", 0.85, "Payment"),
            "BNBUSDT": EnhancedCoinInfo("BNBUSDT", "BNB", "Exchange", 4, 1, 1800000000, ["binance", "okx"], "é«˜æµåŠ¨æ€§", 0.75, "Exchange"),
            "SOLUSDT": EnhancedCoinInfo("SOLUSDT", "Solana", "Layer1", 5, 1, 3200000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§", 1.2, "SmartContract"),
            "DOGEUSDT": EnhancedCoinInfo("DOGEUSDT", "Dogecoin", "Meme", 6, 2, 1400000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§", 1.5, "Meme"),
            "ADAUSDT": EnhancedCoinInfo("ADAUSDT", "Cardano", "Layer1", 7, 1, 800000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§", 0.9, "SmartContract"),
            
            # é«˜æµåŠ¨æ€§å±±å¯¨å¸ (Tier 2)
            "TRXUSDT": EnhancedCoinInfo("TRXUSDT", "TRON", "Layer1", 8, 2, 450000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§", 0.8, "SmartContract"),
            "TONUSDT": EnhancedCoinInfo("TONUSDT", "Toncoin", "Layer1", 9, 2, 350000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 1.1, "SmartContract"),
            "AVAXUSDT": EnhancedCoinInfo("AVAXUSDT", "Avalanche", "Layer1", 10, 1, 600000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§", 1.3, "SmartContract"),
            "LINKUSDT": EnhancedCoinInfo("LINKUSDT", "Chainlink", "Oracle", 11, 1, 700000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§", 1.0, "Infrastructure"),
            "DOTUSDT": EnhancedCoinInfo("DOTUSDT", "Polkadot", "Layer0", 12, 2, 300000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 1.1, "Infrastructure"),
            
            # ä¸­ç­‰æµåŠ¨æ€§å±±å¯¨å¸ (Tier 3)
            "MATICUSDT": EnhancedCoinInfo("MATICUSDT", "Polygon", "Layer2", 13, 2, 400000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 1.0, "Layer2"),
            "LTCUSDT": EnhancedCoinInfo("LTCUSDT", "Litecoin", "Payment", 15, 2, 600000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§", 0.7, "Payment"),
            "NEARUSDT": EnhancedCoinInfo("NEARUSDT", "NEAR Protocol", "Layer1", 17, 2, 180000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 1.2, "SmartContract"),
            "APTUSDT": EnhancedCoinInfo("APTUSDT", "Aptos", "Layer1", 18, 2, 250000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 1.4, "SmartContract"),
            "UNIUSDT": EnhancedCoinInfo("UNIUSDT", "Uniswap", "DeFi", 19, 2, 300000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 1.1, "DeFi"),
            
            # å…¶ä»–é‡è¦å±±å¯¨å¸
            "ATOMUSDT": EnhancedCoinInfo("ATOMUSDT", "Cosmos", "Layer0", 20, 2, 150000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 1.0, "Infrastructure"),
            "XLMUSDT": EnhancedCoinInfo("XLMUSDT", "Stellar", "Payment", 21, 3, 120000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 0.8, "Payment"),
            "FILUSDT": EnhancedCoinInfo("FILUSDT", "Filecoin", "Storage", 25, 3, 150000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 1.2, "Storage"),
            "ARBUSDT": EnhancedCoinInfo("ARBUSDT", "Arbitrum", "Layer2", 26, 2, 180000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 1.3, "Layer2"),
            "OPUSDT": EnhancedCoinInfo("OPUSDT", "Optimism", "Layer2", 27, 2, 120000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 1.2, "Layer2"),
            
            # DeFiå’Œç‰¹æ®Šç”¨é€”å¸ç§
            "AAVEUSDT": EnhancedCoinInfo("AAVEUSDT", "Aave", "DeFi", 31, 2, 200000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 1.3, "DeFi"),
            "MKRUSDT": EnhancedCoinInfo("MKRUSDT", "Maker", "DeFi", 32, 3, 150000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 1.1, "DeFi"),
            "COMPUSDT": EnhancedCoinInfo("COMPUSDT", "Compound", "DeFi", 35, 3, 100000000, ["binance", "okx", "bybit"], "ä½æµåŠ¨æ€§", 1.4, "DeFi"),
            "VETUSDT": EnhancedCoinInfo("VETUSDT", "VeChain", "Supply Chain", 28, 3, 60000000, ["binance", "okx", "bybit"], "ä½æµåŠ¨æ€§", 0.9, "Enterprise"),
            "ALGOUSDT": EnhancedCoinInfo("ALGOUSDT", "Algorand", "Layer1", 29, 3, 100000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 1.0, "SmartContract"),
            
            # å¤‡ç”¨å¸ç§ï¼ˆå¦‚éœ€æ›¿æ¢ï¼‰
            "GRTUSDT": EnhancedCoinInfo("GRTUSDT", "The Graph", "Indexing", 30, 3, 80000000, ["binance", "okx", "bybit"], "ä½æµåŠ¨æ€§", 1.3, "Infrastructure"),
            "ICPUSDT": EnhancedCoinInfo("ICPUSDT", "Internet Computer", "Computing", 16, 3, 200000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 1.5, "Computing"),
            "SHIBUSDT": EnhancedCoinInfo("SHIBUSDT", "Shiba Inu", "Meme", 14, 3, 800000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 2.0, "Meme"),
            "PEPEUSDT": EnhancedCoinInfo("PEPEUSDT", "Pepe", "Meme", 33, 3, 300000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§", 2.5, "Meme"),
            "WIFUSDT": EnhancedCoinInfo("WIFUSDT", "dogwifhat", "Meme", 34, 3, 200000000, ["binance", "okx", "bybit"], "ä½æµåŠ¨æ€§", 3.0, "Meme"),
        }
        
        # å¢å¼ºç‰ˆæ—¶é—´æ¡†æ¶é…ç½®
        self.timeframes = {
            '1m': '1m',     # 1åˆ†é’ŸKçº¿ - è¶…é«˜é¢‘åˆ†æ
            '5m': '5m',     # 5åˆ†é’ŸKçº¿ - DipMasterä¸»è¦æ—¶é—´æ¡†æ¶
            '15m': '15m',   # 15åˆ†é’ŸKçº¿ - ä¸­æœŸåˆ†æ
            '1h': '1h',     # 1å°æ—¶Kçº¿ - è¶‹åŠ¿åˆ†æ
            '4h': '4h',     # 4å°æ—¶Kçº¿ - å®è§‚è¶‹åŠ¿
            '1d': '1d'      # æ—¥çº¿ - é•¿æœŸè¶‹åŠ¿
        }
        
        # æ•°æ®å­˜å‚¨è·¯å¾„
        self.data_path = Path("data/enhanced_market_data")
        self.data_path.mkdir(exist_ok=True, parents=True)
        
        # åˆå§‹åŒ–äº¤æ˜“æ‰€è¿æ¥
        self.exchanges = self.setup_exchanges()
        
        # å¢å¼ºçš„æ•°æ®è´¨é‡æ ‡å‡†
        self.quality_standards = {
            'completeness_threshold': 0.995,  # 99.5%ä»¥ä¸Šçš„æ•°æ®å®Œæ•´æ€§
            'max_gap_minutes': 15,            # æœ€å¤§æ•°æ®ç¼ºå¤±15åˆ†é’Ÿ
            'price_spike_threshold': 0.3,     # ä»·æ ¼å¼‚å¸¸æ³¢åŠ¨30%é˜ˆå€¼ï¼ˆæ›´ä¸¥æ ¼ï¼‰
            'volume_outlier_std': 4,          # æˆäº¤é‡å¼‚å¸¸æ ‡å‡†å·®å€æ•°ï¼ˆæ›´ä¸¥æ ¼ï¼‰
            'ohlc_consistency_tolerance': 0.0005,  # OHLCä¸€è‡´æ€§å®¹å¿åº¦
            'minimum_daily_volume': 50000000   # æœ€å°æ—¥æˆäº¤é‡5000ä¸‡USD
        }
        
        # å¢å¼ºçš„æ•°æ®æ”¶é›†é…ç½®
        self.collection_config = {
            'lookback_days': 730,       # 2å¹´å†å²æ•°æ®
            'batch_size': 1000,         # æ¯æ‰¹æ¬¡è·å–1000æ¡è®°å½•
            'rate_limit_delay': 0.05,   # è¯·æ±‚é—´éš”50msï¼ˆæ›´å¿«ï¼‰
            'retry_attempts': 5,        # å¤±è´¥é‡è¯•5æ¬¡
            'timeout_seconds': 45,      # è¯·æ±‚è¶…æ—¶45ç§’
            'parallel_symbols': 5,      # å¹¶è¡Œä¸‹è½½5ä¸ªå¸ç§
            'max_workers': 10           # æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        }
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = {
            'start_time': None,
            'download_times': {},
            'quality_scores': {},
            'errors': [],
            'retry_counts': defaultdict(int)
        }
        
    def setup_logging(self) -> logging.Logger:
        """è®¾ç½®å¢å¼ºæ—¥å¿—ç³»ç»Ÿ"""
        log_path = Path("logs")
        log_path.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('logs/enhanced_top30_data_collection.log'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    def setup_exchanges(self) -> Dict[str, ccxt.Exchange]:
        """è®¾ç½®å¢å¼ºçš„äº¤æ˜“æ‰€è¿æ¥"""
        exchanges = {}
        
        # Binance - ä¸»è¦æ•°æ®æº
        exchanges['binance'] = ccxt.binance({
            'apiKey': '',
            'secret': '',
            'sandbox': False,
            'rateLimit': 100,           # æ›´æ¿€è¿›çš„é™åˆ¶
            'enableRateLimit': True,
            'options': {'defaultType': 'spot'},
            'timeout': 30000            # 30ç§’è¶…æ—¶
        })
        
        return exchanges
    
    async def download_symbol_timeframe_data(self, 
                                          symbol: str, 
                                          timeframe: str, 
                                          exchange_name: str = 'binance') -> Tuple[str, str, pd.DataFrame, Dict]:
        """ä¸‹è½½å•ä¸ªå¸ç§å•ä¸ªæ—¶é—´æ¡†æ¶çš„æ•°æ®"""
        start_time = time.time()
        quality_info = {}
        
        try:
            self.logger.info(f"å¼€å§‹ä¸‹è½½ {symbol} {timeframe} æ•°æ®...")
            
            exchange = self.exchanges[exchange_name]
            
            # æ ¹æ®æ—¶é—´æ¡†æ¶è®¡ç®—è·å–å¤©æ•°
            timeframe_days = {
                '1m': 90,   # 1åˆ†é’Ÿæ•°æ®åªå–90å¤©ï¼ˆæ•°æ®é‡å¤§ï¼‰
                '5m': 730,  # 5åˆ†é’Ÿæ•°æ®å–2å¹´
                '15m': 730, # 15åˆ†é’Ÿæ•°æ®å–2å¹´
                '1h': 730,  # 1å°æ—¶æ•°æ®å–2å¹´
                '4h': 730,  # 4å°æ—¶æ•°æ®å–2å¹´
                '1d': 1095  # æ—¥çº¿æ•°æ®å–3å¹´
            }
            
            lookback_days = timeframe_days.get(timeframe, 730)
            
            # è®¡ç®—æ—¶é—´èŒƒå›´
            end_time = datetime.now()
            start_time_calc = end_time - timedelta(days=lookback_days)
            since = int(start_time_calc.timestamp() * 1000)
            
            # åˆ†æ‰¹ä¸‹è½½æ•°æ®
            all_ohlcv = []
            current_since = since
            batch_size = self.collection_config['batch_size']
            retry_count = 0
            
            while current_since < int(end_time.timestamp() * 1000):
                try:
                    # å¼‚æ­¥è·å–OHLCVæ•°æ®
                    ohlcv = await asyncio.to_thread(
                        exchange.fetch_ohlcv,
                        symbol, timeframe, current_since, batch_size
                    )
                    
                    if not ohlcv:
                        break
                    
                    all_ohlcv.extend(ohlcv)
                    current_since = ohlcv[-1][0] + 1
                    
                    # è¿›åº¦æ˜¾ç¤º
                    if len(all_ohlcv) % 5000 == 0:
                        self.logger.info(f"{symbol} {timeframe}: å·²æ”¶é›† {len(all_ohlcv)} æ¡è®°å½•")
                    
                    # é¿å…è§¦å‘é¢‘ç‡é™åˆ¶
                    await asyncio.sleep(self.collection_config['rate_limit_delay'])
                    
                except Exception as e:
                    retry_count += 1
                    self.performance_metrics['retry_counts'][f"{symbol}_{timeframe}"] += 1
                    
                    if retry_count >= self.collection_config['retry_attempts']:
                        self.logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {symbol} {timeframe}: {e}")
                        break
                    
                    self.logger.warning(f"æ‰¹æ¬¡ä¸‹è½½å¤±è´¥ {symbol} {timeframe} (é‡è¯• {retry_count}): {e}")
                    await asyncio.sleep(min(retry_count * 2, 10))  # æŒ‡æ•°é€€é¿
                    continue
            
            # è½¬æ¢ä¸ºDataFrame
            if not all_ohlcv:
                self.logger.error(f"æ²¡æœ‰è·å–åˆ° {symbol} {timeframe} æ•°æ®")
                return symbol, timeframe, pd.DataFrame(), {}
            
            df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df.set_index('timestamp', inplace=True)
            
            # æ•°æ®æ¸…ç†å’Œå»é‡
            df = df[~df.index.duplicated(keep='first')]
            df.sort_index(inplace=True)
            
            # ç§»é™¤é›¶æˆäº¤é‡çš„è®°å½•
            df = df[df['volume'] > 0]
            
            # æ•°æ®è´¨é‡è¯„ä¼°
            quality_score = self.assess_data_quality(df, symbol, timeframe)
            quality_info = {
                'quality_score': quality_score,
                'records_count': len(df),
                'date_range': {
                    'start': df.index.min().isoformat() if not df.empty else None,
                    'end': df.index.max().isoformat() if not df.empty else None
                },
                'download_time_seconds': time.time() - start_time,
                'retry_count': retry_count
            }
            
            self.logger.info(f"{symbol} {timeframe} ä¸‹è½½å®Œæˆ: {len(df)} æ¡è®°å½•, è´¨é‡è¯„åˆ†: {quality_score:.3f}")
            
            return symbol, timeframe, df, quality_info
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½ {symbol} {timeframe} æ•°æ®å¤±è´¥: {e}")
            self.performance_metrics['errors'].append(f"{symbol}_{timeframe}: {str(e)}")
            return symbol, timeframe, pd.DataFrame(), {}
    
    def assess_data_quality(self, df: pd.DataFrame, symbol: str, timeframe: str) -> float:
        """å¢å¼ºçš„æ•°æ®è´¨é‡è¯„ä¼°"""
        if df.empty:
            return 0.0
        
        quality_scores = {}
        
        try:
            # 1. å®Œæ•´æ€§æ£€æŸ¥ï¼ˆæ›´ä¸¥æ ¼ï¼‰
            total_expected = self.calculate_expected_records(timeframe)
            actual_records = len(df)
            quality_scores['completeness'] = min(1.0, actual_records / total_expected)
            
            # 2. ç¼ºå¤±å€¼æ£€æŸ¥
            missing_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
            quality_scores['no_missing'] = max(0, 1 - missing_ratio)
            
            # 3. OHLCä¸€è‡´æ€§æ£€æŸ¥ï¼ˆæ›´ä¸¥æ ¼ï¼‰
            ohlc_violations = 0
            
            # High >= max(Open, Close) ä¸” High >= Low
            high_violations = ((df['high'] < df[['open', 'close']].max(axis=1)) | 
                             (df['high'] < df['low'])).sum()
            
            # Low <= min(Open, Close) ä¸” Low <= High
            low_violations = ((df['low'] > df[['open', 'close']].min(axis=1)) | 
                            (df['low'] > df['high'])).sum()
            
            # ä»·æ ¼å¿…é¡»ä¸ºæ­£æ•°
            negative_prices = (df[['open', 'high', 'low', 'close']] <= 0).sum().sum()
            
            total_violations = high_violations + low_violations + negative_prices
            quality_scores['ohlc_consistency'] = max(0, 1 - (total_violations / (len(df) * 3)))
            
            # 4. ä»·æ ¼ç¨³å®šæ€§æ£€æŸ¥ï¼ˆæ£€æµ‹å¼‚å¸¸æ³¢åŠ¨ï¼‰
            price_changes = df['close'].pct_change().abs()
            extreme_changes = (price_changes > self.quality_standards['price_spike_threshold']).sum()
            quality_scores['price_stability'] = max(0, 1 - (extreme_changes / len(df)))
            
            # 5. æˆäº¤é‡åˆç†æ€§æ£€æŸ¥
            if df['volume'].std() > 0:
                volume_z_scores = np.abs((df['volume'] - df['volume'].mean()) / df['volume'].std())
                volume_outliers = (volume_z_scores > self.quality_standards['volume_outlier_std']).sum()
                quality_scores['volume_stability'] = max(0, 1 - (volume_outliers / len(df)))
            else:
                quality_scores['volume_stability'] = 0.5  # æˆäº¤é‡æ— å˜åŒ–ï¼Œç»™ä¸­ç­‰åˆ†æ•°
            
            # 6. æ—¶é—´è¿ç»­æ€§æ£€æŸ¥ï¼ˆæ›´ä¸¥æ ¼ï¼‰
            time_gaps = self.check_time_gaps(df, timeframe)
            quality_scores['time_continuity'] = time_gaps['continuity_score']
            
            # 7. æµåŠ¨æ€§æ£€æŸ¥ï¼ˆæ–°å¢ï¼‰
            avg_volume = df['volume'].mean()
            coin_info = self.top30_altcoins.get(symbol)
            if coin_info:
                expected_volume = coin_info.daily_volume_usd / 288  # å‡è®¾5åˆ†é’Ÿå‡åŒ€åˆ†å¸ƒ
                volume_ratio = min(1.0, avg_volume / expected_volume) if expected_volume > 0 else 0.5
                quality_scores['liquidity'] = volume_ratio
            else:
                quality_scores['liquidity'] = 0.8  # é»˜è®¤è¯„åˆ†
            
            # 8. ä»·æ ¼åˆç†æ€§æ£€æŸ¥ï¼ˆæ–°å¢ï¼‰
            price_mean = df['close'].mean()
            price_std = df['close'].std()
            if price_std > 0:
                cv = price_std / price_mean  # å˜å¼‚ç³»æ•°
                reasonable_cv = cv < 2.0  # å˜å¼‚ç³»æ•°å°äº2è§†ä¸ºåˆç†
                quality_scores['price_reasonableness'] = 1.0 if reasonable_cv else 0.5
            else:
                quality_scores['price_reasonableness'] = 0.3  # ä»·æ ¼æ— å˜åŒ–ï¼Œä½åˆ†
            
            # ç»¼åˆè´¨é‡è¯„åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
            weights = {
                'completeness': 0.20,
                'no_missing': 0.15,
                'ohlc_consistency': 0.20,
                'price_stability': 0.15,
                'volume_stability': 0.10,
                'time_continuity': 0.10,
                'liquidity': 0.05,
                'price_reasonableness': 0.05
            }
            
            overall_score = sum(quality_scores[key] * weights[key] for key in weights)
            
            # å­˜å‚¨è¯¦ç»†è´¨é‡æŒ‡æ ‡
            self.performance_metrics['quality_scores'][f"{symbol}_{timeframe}"] = quality_scores
            
            return overall_score
            
        except Exception as e:
            self.logger.error(f"è´¨é‡è¯„ä¼°å¤±è´¥ {symbol} {timeframe}: {e}")
            return 0.0
    
    def calculate_expected_records(self, timeframe: str) -> int:
        """è®¡ç®—æœŸæœ›çš„è®°å½•æ•°é‡ï¼ˆåŸºäºä¸åŒæ—¶é—´æ¡†æ¶ï¼‰"""
        timeframe_days = {
            '1m': 90,   # 1åˆ†é’Ÿæ•°æ®åªå–90å¤©
            '5m': 730,  # å…¶ä»–æ•°æ®å–2å¹´
            '15m': 730,
            '1h': 730,
            '4h': 730,
            '1d': 1095  # æ—¥çº¿æ•°æ®å–3å¹´
        }
        
        days = timeframe_days.get(timeframe, 730)
        
        records_per_day = {
            '1m': 1440,    # 24 * 60
            '5m': 288,     # 24 * 12
            '15m': 96,     # 24 * 4
            '1h': 24,      # 24
            '4h': 6,       # 6
            '1d': 1        # 1
        }
        
        return days * records_per_day.get(timeframe, 288)
    
    def check_time_gaps(self, df: pd.DataFrame, timeframe: str) -> Dict:
        """å¢å¼ºçš„æ—¶é—´é—´éš”è¿ç»­æ€§æ£€æŸ¥"""
        if len(df) < 2:
            return {'continuity_score': 0.0, 'gaps_count': 0, 'max_gap_minutes': 0}
        
        # è®¡ç®—æœŸæœ›çš„æ—¶é—´é—´éš”
        expected_interval = {
            '1m': timedelta(minutes=1),
            '5m': timedelta(minutes=5),
            '15m': timedelta(minutes=15),
            '1h': timedelta(hours=1),
            '4h': timedelta(hours=4),
            '1d': timedelta(days=1)
        }
        
        interval = expected_interval.get(timeframe, timedelta(minutes=5))
        
        # è®¡ç®—å®é™…æ—¶é—´é—´éš”
        time_diffs = df.index.to_series().diff().dropna()
        
        # è¯†åˆ«å¼‚å¸¸é—´éš”
        expected_seconds = interval.total_seconds()
        
        # å…è®¸çš„æœ€å¤§é—´éš”ï¼ˆè€ƒè™‘å‘¨æœ«å’ŒèŠ‚å‡æ—¥ï¼‰
        if timeframe in ['1d']:
            max_allowed_seconds = expected_seconds * 3  # æ—¥çº¿å…è®¸3å¤©é—´éš”
        elif timeframe in ['4h']:
            max_allowed_seconds = expected_seconds * 2  # 4å°æ—¶å…è®¸8å°æ—¶é—´éš”
        else:
            max_allowed_seconds = expected_seconds * 1.5  # å…¶ä»–å…è®¸1.5å€é—´éš”
        
        large_gaps = time_diffs[time_diffs.dt.total_seconds() > max_allowed_seconds]
        gaps_count = len(large_gaps)
        
        # æœ€å¤§é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
        max_gap_minutes = time_diffs.dt.total_seconds().max() / 60 if not time_diffs.empty else 0
        
        # è¿ç»­æ€§è¯„åˆ†ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        continuity_score = max(0, 1 - (gaps_count / len(time_diffs)) * 2)  # ä¹˜ä»¥2ä½¿è¯„åˆ†æ›´ä¸¥æ ¼
        
        return {
            'continuity_score': continuity_score,
            'gaps_count': gaps_count,
            'max_gap_minutes': max_gap_minutes,
            'large_gaps_count': gaps_count
        }
    
    def save_enhanced_data(self, df: pd.DataFrame, symbol: str, timeframe: str, quality_info: Dict) -> str:
        """ä¿å­˜å¢å¼ºæ•°æ®åˆ°æ–‡ä»¶"""
        try:
            file_path = self.data_path / f"{symbol}_{timeframe}_2years.parquet"
            
            # ä¿å­˜ä¸ºParquetæ ¼å¼ï¼ˆä½¿ç”¨zstdå‹ç¼©ï¼Œæ›´é«˜å‹ç¼©ç‡ï¼‰
            df.to_parquet(file_path, compression='zstd', index=True)
            
            # ç”Ÿæˆå¢å¼ºå…ƒæ•°æ®
            metadata = {
                'symbol': symbol,
                'timeframe': timeframe,
                'records_count': len(df),
                'data_quality': quality_info,
                'date_range': {
                    'start': df.index.min().isoformat() if not df.empty else None,
                    'end': df.index.max().isoformat() if not df.empty else None,
                    'span_days': (df.index.max() - df.index.min()).days if not df.empty else 0
                },
                'file_info': {
                    'file_size_mb': file_path.stat().st_size / (1024 * 1024),
                    'compression': 'zstd',
                    'format': 'parquet'
                },
                'statistics': {
                    'price_range': {
                        'min': float(df['low'].min()) if not df.empty else None,
                        'max': float(df['high'].max()) if not df.empty else None,
                        'avg': float(df['close'].mean()) if not df.empty else None
                    },
                    'volume_stats': {
                        'total': float(df['volume'].sum()) if not df.empty else None,
                        'avg': float(df['volume'].mean()) if not df.empty else None,
                        'max': float(df['volume'].max()) if not df.empty else None
                    }
                },
                'created_at': datetime.now().isoformat(),
                'collection_config': self.collection_config
            }
            
            metadata_path = self.data_path / f"{symbol}_{timeframe}_2years_metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2, default=str)
            
            self.logger.info(f"æ•°æ®å·²ä¿å­˜: {file_path} ({metadata['file_info']['file_size_mb']:.2f} MB)")
            return str(file_path)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥ {symbol} {timeframe}: {e}")
            return ""
    
    async def collect_symbol_batch(self, symbols_batch: List[str]) -> Dict:
        """å¹¶è¡Œæ”¶é›†ä¸€æ‰¹å¸ç§çš„æ‰€æœ‰æ—¶é—´æ¡†æ¶æ•°æ®"""
        batch_results = {}
        
        # ä¸ºæ¯ä¸ªå¸ç§åˆ›å»ºæ‰€æœ‰æ—¶é—´æ¡†æ¶çš„ä»»åŠ¡
        tasks = []
        for symbol in symbols_batch:
            for timeframe in self.timeframes.keys():
                task = self.download_symbol_timeframe_data(symbol, timeframe)
                tasks.append(task)
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        self.logger.info(f"å¼€å§‹å¹¶è¡Œä¸‹è½½ {len(symbols_batch)} ä¸ªå¸ç§çš„ {len(self.timeframes)} ä¸ªæ—¶é—´æ¡†æ¶æ•°æ®...")
        
        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for result in results:
                if isinstance(result, Exception):
                    self.logger.error(f"æ‰¹æ¬¡ä»»åŠ¡å¤±è´¥: {result}")
                    continue
                
                symbol, timeframe, df, quality_info = result
                
                if not df.empty:
                    # ä¿å­˜æ•°æ®
                    file_path = self.save_enhanced_data(df, symbol, timeframe, quality_info)
                    
                    if file_path:
                        if symbol not in batch_results:
                            batch_results[symbol] = {}
                        
                        batch_results[symbol][timeframe] = {
                            'file_path': file_path,
                            'quality_info': quality_info,
                            'success': True
                        }
                else:
                    self.logger.warning(f"ç©ºæ•°æ®: {symbol} {timeframe}")
                    if symbol not in batch_results:
                        batch_results[symbol] = {}
                    batch_results[symbol][timeframe] = {
                        'file_path': '',
                        'quality_info': {},
                        'success': False
                    }
        
        except Exception as e:
            self.logger.error(f"æ‰¹æ¬¡æ”¶é›†å¤±è´¥: {e}")
        
        return batch_results
    
    async def collect_all_enhanced_data(self) -> Dict[str, Any]:
        """æ”¶é›†æ‰€æœ‰å¸ç§çš„å¢å¼ºå†å²æ•°æ®"""
        self.logger.info("å¼€å§‹æ”¶é›†å‰30å¤§å±±å¯¨å¸çš„å¢å¼ºç‰ˆ2å¹´å†å²æ•°æ®...")
        self.performance_metrics['start_time'] = datetime.now()
        
        collection_start_time = datetime.now()
        all_results = {}
        failed_symbols = []
        
        # å°†å¸ç§åˆ†æ‰¹å¤„ç†
        symbols = list(self.top30_altcoins.keys())
        batch_size = self.collection_config['parallel_symbols']
        total_batches = len(symbols) // batch_size + (1 if len(symbols) % batch_size > 0 else 0)
        
        for i in range(0, len(symbols), batch_size):
            batch_num = i // batch_size + 1
            batch_symbols = symbols[i:i + batch_size]
            
            self.logger.info(f"å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹: {batch_symbols}")
            
            try:
                batch_results = await self.collect_symbol_batch(batch_symbols)
                all_results.update(batch_results)
                
                # è¿›åº¦æŠ¥å‘Š
                completed_symbols = len(all_results)
                progress = (completed_symbols / len(symbols)) * 100
                self.logger.info(f"æ‰¹æ¬¡å®Œæˆè¿›åº¦: {progress:.1f}% ({completed_symbols}/{len(symbols)})")
                
                # æ‰¹æ¬¡é—´çŸ­æš‚ä¼‘æ¯
                if batch_num < total_batches:
                    await asyncio.sleep(2)
                    
            except Exception as e:
                self.logger.error(f"æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {e}")
                failed_symbols.extend(batch_symbols)
        
        # ç”Ÿæˆæ”¶é›†æŠ¥å‘Š
        collection_end_time = datetime.now()
        collection_duration = collection_end_time - collection_start_time
        
        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥çš„æƒ…å†µ
        successful_symbols = []
        partially_successful = []
        failed_completely = []
        
        for symbol in symbols:
            if symbol in all_results:
                timeframe_success = sum(1 for tf_data in all_results[symbol].values() if tf_data.get('success', False))
                total_timeframes = len(self.timeframes)
                
                if timeframe_success == total_timeframes:
                    successful_symbols.append(symbol)
                elif timeframe_success > 0:
                    partially_successful.append(symbol)
                else:
                    failed_completely.append(symbol)
            else:
                failed_completely.append(symbol)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_files_created = sum(
            sum(1 for tf_data in symbol_data.values() if tf_data.get('success', False))
            for symbol_data in all_results.values()
        )
        
        total_quality_scores = []
        for symbol_data in all_results.values():
            for tf_data in symbol_data.values():
                if tf_data.get('success', False):
                    quality_score = tf_data.get('quality_info', {}).get('quality_score', 0)
                    total_quality_scores.append(quality_score)
        
        avg_quality = np.mean(total_quality_scores) if total_quality_scores else 0
        
        collection_report = {
            'collection_summary': {
                'start_time': collection_start_time.isoformat(),
                'end_time': collection_end_time.isoformat(),
                'duration_minutes': collection_duration.total_seconds() / 60,
                'total_symbols_attempted': len(symbols),
                'fully_successful_symbols': len(successful_symbols),
                'partially_successful_symbols': len(partially_successful),
                'failed_symbols': len(failed_completely),
                'total_files_created': total_files_created,
                'total_timeframes': len(self.timeframes),
                'expected_files': len(symbols) * len(self.timeframes),
                'success_rate': total_files_created / (len(symbols) * len(self.timeframes)),
                'average_quality_score': avg_quality
            },
            'symbol_results': {
                'fully_successful': successful_symbols,
                'partially_successful': partially_successful,
                'failed_completely': failed_completely
            },
            'data_collection': all_results,
            'performance_metrics': self.performance_metrics,
            'quality_standards': self.quality_standards,
            'collection_config': self.collection_config,
            'timeframes_collected': list(self.timeframes.keys())
        }
        
        self.logger.info(f"å¢å¼ºæ•°æ®æ”¶é›†å®Œæˆï¼")
        self.logger.info(f"è€—æ—¶: {collection_duration.total_seconds()/60:.1f} åˆ†é’Ÿ")
        self.logger.info(f"å®Œå…¨æˆåŠŸ: {len(successful_symbols)} å¸ç§")
        self.logger.info(f"éƒ¨åˆ†æˆåŠŸ: {len(partially_successful)} å¸ç§")
        self.logger.info(f"å®Œå…¨å¤±è´¥: {len(failed_completely)} å¸ç§")
        self.logger.info(f"æ€»æ–‡ä»¶æ•°: {total_files_created}/{len(symbols) * len(self.timeframes)}")
        self.logger.info(f"å¹³å‡è´¨é‡è¯„åˆ†: {avg_quality:.3f}")
        
        return collection_report

# ä¸»æ‰§è¡Œå‡½æ•°
async def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œå¢å¼ºç‰ˆæ•°æ®æ”¶é›†æµç¨‹"""
    collector = EnhancedTop30DataCollector()
    
    try:
        # æ‰§è¡Œå¢å¼ºç‰ˆæ•°æ®æ”¶é›†
        collection_report = await collector.collect_all_enhanced_data()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        reports_path = Path("data")
        reports_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜æ”¶é›†æŠ¥å‘Š
        collection_report_path = reports_path / f"Enhanced_Top30_Collection_Report_{timestamp}.json"
        with open(collection_report_path, 'w', encoding='utf-8') as f:
            json.dump(collection_report, f, ensure_ascii=False, indent=2, default=str)
        
        # ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
        print("\n" + "="*80)
        print("å¢å¼ºç‰ˆå‰30å¤§å±±å¯¨å¸æ•°æ®æ”¶é›†å®Œæˆ!")
        print("="*80)
        print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Š: {collection_report_path}")
        print(f"â±ï¸  æ”¶é›†è€—æ—¶: {collection_report.get('collection_summary', {}).get('duration_minutes', 0):.1f} åˆ†é’Ÿ")
        print(f"âœ… å®Œå…¨æˆåŠŸ: {collection_report.get('collection_summary', {}).get('fully_successful_symbols', 0)} å¸ç§")
        print(f"ğŸ”¸ éƒ¨åˆ†æˆåŠŸ: {collection_report.get('collection_summary', {}).get('partially_successful_symbols', 0)} å¸ç§")
        print(f"âŒ å®Œå…¨å¤±è´¥: {collection_report.get('collection_summary', {}).get('failed_symbols', 0)} å¸ç§")
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {collection_report.get('collection_summary', {}).get('total_files_created', 0)}")
        print(f"ğŸ† å¹³å‡è´¨é‡: {collection_report.get('collection_summary', {}).get('average_quality_score', 0):.3f}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {collection_report.get('collection_summary', {}).get('success_rate', 0)*100:.1f}%")
        print("="*80)
        
        return collection_report
        
    except Exception as e:
        collector.logger.error(f"å¢å¼ºæ•°æ®æ”¶é›†æµç¨‹å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())