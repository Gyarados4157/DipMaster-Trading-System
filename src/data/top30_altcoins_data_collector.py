"""
Top 30 Altcoins Data Collection Infrastructure
ä¸ºDipMasterç­–ç•¥ä¼˜åŒ–æ”¶é›†å‰30å¸‚å€¼å±±å¯¨å¸çš„å®Œæ•´å†å²æ•°æ®

æ”¯æŒï¼š
- å‰30å¤§å¸‚å€¼å±±å¯¨å¸ï¼ˆæ’é™¤BTCã€ETHã€ç¨³å®šå¸ï¼‰
- å¤šæ—¶é—´æ¡†æ¶ï¼š1åˆ†é’Ÿã€5åˆ†é’Ÿã€15åˆ†é’Ÿã€1å°æ—¶Kçº¿æ•°æ®
- 2å¹´å†å²æ•°æ®
- é«˜è´¨é‡æ•°æ®éªŒè¯å’Œæ¸…ç†
- è‡ªåŠ¨ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
"""

import asyncio
import ccxt
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import logging
from pathlib import Path
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

@dataclass
class CoinInfo:
    """å¸ç§ä¿¡æ¯"""
    symbol: str
    name: str
    category: str
    market_cap_rank: int
    priority: int
    daily_volume_usd: float
    exchange_support: List[str]
    liquidity_tier: str

class Top30AltcoinsDataCollector:
    """å‰30å¤§å±±å¯¨å¸æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self):
        self.logger = self.setup_logging()
        
        # å‰30å¤§å¸‚å€¼å±±å¯¨å¸é…ç½®ï¼ˆåŸºäº2025å¹´8æœˆæ•°æ®ï¼‰
        self.top30_altcoins = {
            # Top 10 å±±å¯¨å¸
            "XRPUSDT": CoinInfo("XRPUSDT", "XRP", "Payment", 3, 1, 2500000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§"),
            "BNBUSDT": CoinInfo("BNBUSDT", "BNB", "Exchange", 4, 1, 1800000000, ["binance", "okx"], "é«˜æµåŠ¨æ€§"),
            "SOLUSDT": CoinInfo("SOLUSDT", "Solana", "Layer1", 5, 1, 3200000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§"),
            "DOGEUSDT": CoinInfo("DOGEUSDT", "Dogecoin", "Meme", 6, 2, 1400000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§"),
            "ADAUSDT": CoinInfo("ADAUSDT", "Cardano", "Layer1", 7, 1, 800000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§"),
            "TRXUSDT": CoinInfo("TRXUSDT", "TRON", "Layer1", 8, 2, 450000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "TONUSDT": CoinInfo("TONUSDT", "Toncoin", "Layer1", 9, 2, 350000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "AVAXUSDT": CoinInfo("AVAXUSDT", "Avalanche", "Layer1", 10, 1, 600000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§"),
            "LINKUSDT": CoinInfo("LINKUSDT", "Chainlink", "Oracle", 11, 1, 700000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§"),
            "DOTUSDT": CoinInfo("DOTUSDT", "Polkadot", "Layer0", 12, 2, 300000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            
            # 11-20åå±±å¯¨å¸
            "MATICUSDT": CoinInfo("MATICUSDT", "Polygon", "Layer2", 13, 2, 400000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "SHIBUSDT": CoinInfo("SHIBUSDT", "Shiba Inu", "Meme", 14, 3, 800000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "LTCUSDT": CoinInfo("LTCUSDT", "Litecoin", "Payment", 15, 2, 600000000, ["binance", "okx", "bybit"], "é«˜æµåŠ¨æ€§"),
            "ICPUSDT": CoinInfo("ICPUSDT", "Internet Computer", "Computing", 16, 3, 200000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "NEARUSDT": CoinInfo("NEARUSDT", "NEAR Protocol", "Layer1", 17, 2, 180000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "APTUSDT": CoinInfo("APTUSDT", "Aptos", "Layer1", 18, 2, 250000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "UNIUSDT": CoinInfo("UNIUSDT", "Uniswap", "DeFi", 19, 2, 300000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "ATOMUSDT": CoinInfo("ATOMUSDT", "Cosmos", "Layer0", 20, 2, 150000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "XLMUSDT": CoinInfo("XLMUSDT", "Stellar", "Payment", 21, 3, 120000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "BCHUSDT": CoinInfo("BCHUSDT", "Bitcoin Cash", "Payment", 22, 3, 400000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            
            # 21-30åå±±å¯¨å¸
            "HBARUSDT": CoinInfo("HBARUSDT", "Hedera", "Enterprise", 23, 3, 80000000, ["binance", "okx", "bybit"], "ä½æµåŠ¨æ€§"),
            "ETCUSDT": CoinInfo("ETCUSDT", "Ethereum Classic", "Layer1", 24, 3, 200000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "FILUSDT": CoinInfo("FILUSDT", "Filecoin", "Storage", 25, 3, 150000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "ARBUSDT": CoinInfo("ARBUSDT", "Arbitrum", "Layer2", 26, 2, 180000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "OPUSDT": CoinInfo("OPUSDT", "Optimism", "Layer2", 27, 2, 120000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "VETUSDT": CoinInfo("VETUSDT", "VeChain", "Supply Chain", 28, 3, 60000000, ["binance", "okx", "bybit"], "ä½æµåŠ¨æ€§"),
            "ALGOUSDT": CoinInfo("ALGOUSDT", "Algorand", "Layer1", 29, 3, 100000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "GRTUSDT": CoinInfo("GRTUSDT", "The Graph", "Indexing", 30, 3, 80000000, ["binance", "okx", "bybit"], "ä½æµåŠ¨æ€§"),
            "AAVEUSDT": CoinInfo("AAVEUSDT", "Aave", "DeFi", 31, 2, 200000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
            "MKRUSDT": CoinInfo("MKRUSDT", "Maker", "DeFi", 32, 3, 150000000, ["binance", "okx", "bybit"], "ä¸­ç­‰æµåŠ¨æ€§"),
        }
        
        # æ—¶é—´æ¡†æ¶é…ç½®
        self.timeframes = {
            '1m': '1m',     # 1åˆ†é’ŸKçº¿ - é«˜é¢‘ç­–ç•¥åˆ†æ
            '5m': '5m',     # 5åˆ†é’ŸKçº¿ - DipMasterä¸»è¦æ—¶é—´æ¡†æ¶
            '15m': '15m',   # 15åˆ†é’ŸKçº¿ - ä¸­æœŸè¶‹åŠ¿åˆ†æ
            '1h': '1h',     # 1å°æ—¶Kçº¿ - é•¿æœŸè¶‹åŠ¿åˆ†æ
        }
        
        # æ•°æ®å­˜å‚¨è·¯å¾„
        self.data_path = Path("data/enhanced_market_data")
        self.data_path.mkdir(exist_ok=True, parents=True)
        
        # åˆå§‹åŒ–äº¤æ˜“æ‰€è¿æ¥
        self.exchanges = self.setup_exchanges()
        
        # æ•°æ®è´¨é‡æ ‡å‡†
        self.quality_standards = {
            'completeness_threshold': 0.99,  # 99%ä»¥ä¸Šçš„æ•°æ®å®Œæ•´æ€§
            'max_gap_minutes': 30,           # æœ€å¤§æ•°æ®ç¼ºå¤±30åˆ†é’Ÿ
            'price_spike_threshold': 0.5,    # ä»·æ ¼å¼‚å¸¸æ³¢åŠ¨50%é˜ˆå€¼
            'volume_outlier_std': 5,         # æˆäº¤é‡å¼‚å¸¸æ ‡å‡†å·®å€æ•°
            'ohlc_consistency_tolerance': 0.001  # OHLCä¸€è‡´æ€§å®¹å¿åº¦
        }
        
        # æ•°æ®æ”¶é›†é…ç½®
        self.collection_config = {
            'lookback_days': 730,    # 2å¹´å†å²æ•°æ®
            'batch_size': 1000,      # æ¯æ‰¹æ¬¡è·å–1000æ¡è®°å½•
            'rate_limit_delay': 0.1, # è¯·æ±‚é—´éš”100ms
            'retry_attempts': 3,     # å¤±è´¥é‡è¯•3æ¬¡
            'timeout_seconds': 30    # è¯·æ±‚è¶…æ—¶30ç§’
        }
        
    def setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('logs/top30_data_collection.log'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    def setup_exchanges(self) -> Dict[str, ccxt.Exchange]:
        """è®¾ç½®äº¤æ˜“æ‰€è¿æ¥"""
        exchanges = {}
        
        # Binance - ä¸»è¦æ•°æ®æº
        exchanges['binance'] = ccxt.binance({
            'apiKey': '',
            'secret': '',
            'sandbox': False,
            'rateLimit': 1200,
            'enableRateLimit': True,
            'options': {'defaultType': 'spot'}
        })
        
        # å¤‡ç”¨äº¤æ˜“æ‰€ï¼ˆå¦‚æœéœ€è¦ï¼‰
        exchanges['okx'] = ccxt.okx({
            'apiKey': '',
            'secret': '',
            'passphrase': '',
            'sandbox': False,
            'rateLimit': 1000,
            'enableRateLimit': True,
        })
        
        return exchanges
    
    async def download_symbol_data(self, 
                                 symbol: str, 
                                 timeframe: str, 
                                 exchange_name: str = 'binance') -> pd.DataFrame:
        """ä¸‹è½½å•ä¸ªå¸ç§çš„å†å²æ•°æ®"""
        try:
            self.logger.info(f"å¼€å§‹ä¸‹è½½ {symbol} {timeframe} æ•°æ®...")
            
            exchange = self.exchanges[exchange_name]
            
            # è®¡ç®—æ—¶é—´èŒƒå›´
            end_time = datetime.now()
            start_time = end_time - timedelta(days=self.collection_config['lookback_days'])
            since = int(start_time.timestamp() * 1000)
            
            # åˆ†æ‰¹ä¸‹è½½æ•°æ®
            all_ohlcv = []
            current_since = since
            batch_size = self.collection_config['batch_size']
            
            while current_since < int(end_time.timestamp() * 1000):
                try:
                    # å¼‚æ­¥è·å–OHLCVæ•°æ®
                    ohlcv = await asyncio.to_thread(
                        exchange.fetch_ohlcv,
                        symbol, timeframe, current_since, batch_size
                    )
                    
                    if not ohlcv:
                        break
                    
                    all_ohlcv.extend(ohlcv)
                    current_since = ohlcv[-1][0] + 1
                    
                    # è¿›åº¦æ˜¾ç¤º
                    progress = len(all_ohlcv)
                    if progress % 10000 == 0:
                        self.logger.info(f"{symbol} {timeframe}: å·²æ”¶é›† {progress} æ¡è®°å½•")
                    
                    # é¿å…è§¦å‘é¢‘ç‡é™åˆ¶
                    await asyncio.sleep(self.collection_config['rate_limit_delay'])
                    
                except Exception as e:
                    self.logger.warning(f"æ‰¹æ¬¡ä¸‹è½½å¤±è´¥ {symbol} {timeframe}: {e}")
                    await asyncio.sleep(1)
                    continue
            
            # è½¬æ¢ä¸ºDataFrame
            if not all_ohlcv:
                self.logger.error(f"æ²¡æœ‰è·å–åˆ° {symbol} {timeframe} æ•°æ®")
                return pd.DataFrame()
            
            df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df.set_index('timestamp', inplace=True)
            
            # æ•°æ®å»é‡å’Œæ’åº
            df = df[~df.index.duplicated(keep='first')]
            df.sort_index(inplace=True)
            
            # æ•°æ®è´¨é‡è¯„ä¼°
            quality_score = self.assess_data_quality(df, symbol, timeframe)
            
            self.logger.info(f"{symbol} {timeframe} ä¸‹è½½å®Œæˆ: {len(df)} æ¡è®°å½•, è´¨é‡è¯„åˆ†: {quality_score:.3f}")
            
            return df
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½ {symbol} {timeframe} æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def assess_data_quality(self, df: pd.DataFrame, symbol: str, timeframe: str) -> float:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        if df.empty:
            return 0.0
        
        quality_scores = {}
        
        try:
            # 1. å®Œæ•´æ€§æ£€æŸ¥
            total_expected = self.calculate_expected_records(timeframe)
            actual_records = len(df)
            quality_scores['completeness'] = min(1.0, actual_records / total_expected)
            
            # 2. ç¼ºå¤±å€¼æ£€æŸ¥
            missing_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
            quality_scores['no_missing'] = max(0, 1 - missing_ratio)
            
            # 3. OHLCä¸€è‡´æ€§æ£€æŸ¥
            ohlc_violations = 0
            total_checks = len(df)
            
            # High >= max(Open, Close)
            high_violations = (df['high'] < df[['open', 'close']].max(axis=1)).sum()
            # Low <= min(Open, Close)
            low_violations = (df['low'] > df[['open', 'close']].min(axis=1)).sum()
            
            ohlc_violations = high_violations + low_violations
            quality_scores['ohlc_consistency'] = max(0, 1 - (ohlc_violations / (total_checks * 2)))
            
            # 4. ä»·æ ¼å¼‚å¸¸æ£€æŸ¥ï¼ˆå¼‚å¸¸æ³¢åŠ¨ï¼‰
            price_changes = df['close'].pct_change().abs()
            extreme_changes = (price_changes > self.quality_standards['price_spike_threshold']).sum()
            quality_scores['price_stability'] = max(0, 1 - (extreme_changes / len(df)))
            
            # 5. æˆäº¤é‡å¼‚å¸¸æ£€æŸ¥
            volume_z_scores = np.abs((df['volume'] - df['volume'].mean()) / df['volume'].std())
            volume_outliers = (volume_z_scores > self.quality_standards['volume_outlier_std']).sum()
            quality_scores['volume_stability'] = max(0, 1 - (volume_outliers / len(df)))
            
            # 6. æ—¶é—´è¿ç»­æ€§æ£€æŸ¥
            time_gaps = self.check_time_gaps(df, timeframe)
            quality_scores['time_continuity'] = time_gaps['continuity_score']
            
            # ç»¼åˆè´¨é‡è¯„åˆ†
            overall_score = np.mean(list(quality_scores.values()))
            
            # è®°å½•è¯¦ç»†è´¨é‡æŒ‡æ ‡
            self.log_quality_metrics(symbol, timeframe, quality_scores, time_gaps)
            
            return overall_score
            
        except Exception as e:
            self.logger.error(f"è´¨é‡è¯„ä¼°å¤±è´¥ {symbol} {timeframe}: {e}")
            return 0.0
    
    def calculate_expected_records(self, timeframe: str) -> int:
        """è®¡ç®—æœŸæœ›çš„è®°å½•æ•°é‡"""
        days = self.collection_config['lookback_days']
        
        records_per_day = {
            '1m': 1440,    # 24 * 60
            '5m': 288,     # 24 * 12
            '15m': 96,     # 24 * 4
            '1h': 24       # 24
        }
        
        return days * records_per_day.get(timeframe, 288)
    
    def check_time_gaps(self, df: pd.DataFrame, timeframe: str) -> Dict:
        """æ£€æŸ¥æ—¶é—´é—´éš”çš„è¿ç»­æ€§"""
        if len(df) < 2:
            return {'continuity_score': 0.0, 'gaps_count': 0, 'max_gap_minutes': 0}
        
        # è®¡ç®—æœŸæœ›çš„æ—¶é—´é—´éš”
        expected_interval = {
            '1m': timedelta(minutes=1),
            '5m': timedelta(minutes=5),
            '15m': timedelta(minutes=15),
            '1h': timedelta(hours=1)
        }
        
        interval = expected_interval.get(timeframe, timedelta(minutes=5))
        
        # è®¡ç®—å®é™…æ—¶é—´é—´éš”
        time_diffs = df.index.to_series().diff().dropna()
        
        # è¯†åˆ«å¼‚å¸¸é—´éš”ï¼ˆè¶…è¿‡æœŸæœ›é—´éš”çš„2å€ï¼‰
        expected_seconds = interval.total_seconds()
        max_allowed_seconds = expected_seconds * 2
        
        large_gaps = time_diffs[time_diffs.dt.total_seconds() > max_allowed_seconds]
        gaps_count = len(large_gaps)
        
        # æœ€å¤§é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
        max_gap_minutes = time_diffs.dt.total_seconds().max() / 60 if not time_diffs.empty else 0
        
        # è¿ç»­æ€§è¯„åˆ†
        continuity_score = max(0, 1 - (gaps_count / len(time_diffs)))
        
        return {
            'continuity_score': continuity_score,
            'gaps_count': gaps_count,
            'max_gap_minutes': max_gap_minutes,
            'large_gaps': large_gaps.tolist()
        }
    
    def log_quality_metrics(self, symbol: str, timeframe: str, scores: Dict, gaps: Dict):
        """è®°å½•è´¨é‡æŒ‡æ ‡"""
        self.logger.info(f"{symbol} {timeframe} è´¨é‡æŒ‡æ ‡:")
        self.logger.info(f"  å®Œæ•´æ€§: {scores.get('completeness', 0):.3f}")
        self.logger.info(f"  æ— ç¼ºå¤±: {scores.get('no_missing', 0):.3f}")
        self.logger.info(f"  OHLCä¸€è‡´æ€§: {scores.get('ohlc_consistency', 0):.3f}")
        self.logger.info(f"  ä»·æ ¼ç¨³å®šæ€§: {scores.get('price_stability', 0):.3f}")
        self.logger.info(f"  æˆäº¤é‡ç¨³å®šæ€§: {scores.get('volume_stability', 0):.3f}")
        self.logger.info(f"  æ—¶é—´è¿ç»­æ€§: {scores.get('time_continuity', 0):.3f}")
        self.logger.info(f"  æ—¶é—´é—´éš”æ•°: {gaps.get('gaps_count', 0)}")
        self.logger.info(f"  æœ€å¤§é—´éš”: {gaps.get('max_gap_minutes', 0):.1f} åˆ†é’Ÿ")
    
    def save_data(self, df: pd.DataFrame, symbol: str, timeframe: str) -> str:
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            file_path = self.data_path / f"{symbol}_{timeframe}_2years.parquet"
            
            # ä¿å­˜ä¸ºParquetæ ¼å¼ï¼ˆé«˜æ•ˆå‹ç¼©ï¼‰
            df.to_parquet(file_path, compression='snappy', index=True)
            
            # ç”Ÿæˆå…ƒæ•°æ®
            metadata = {
                'symbol': symbol,
                'timeframe': timeframe,
                'records_count': len(df),
                'date_range': {
                    'start': df.index.min().isoformat() if not df.empty else None,
                    'end': df.index.max().isoformat() if not df.empty else None
                },
                'file_size_mb': file_path.stat().st_size / (1024 * 1024),
                'created_at': datetime.now().isoformat()
            }
            
            metadata_path = self.data_path / f"{symbol}_{timeframe}_2years_metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2, default=str)
            
            self.logger.info(f"æ•°æ®å·²ä¿å­˜: {file_path}")
            return str(file_path)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥ {symbol} {timeframe}: {e}")
            return ""
    
    async def collect_all_data(self) -> Dict[str, Any]:
        """æ”¶é›†æ‰€æœ‰å¸ç§çš„å†å²æ•°æ®"""
        self.logger.info("å¼€å§‹æ”¶é›†å‰30å¤§å±±å¯¨å¸çš„2å¹´å†å²æ•°æ®...")
        
        collection_start_time = datetime.now()
        successful_downloads = {}
        failed_downloads = []
        quality_reports = {}
        
        # åˆ›å»ºä¸‹è½½ä»»åŠ¡
        total_tasks = len(self.top30_altcoins) * len(self.timeframes)
        completed_tasks = 0
        
        for symbol, coin_info in self.top30_altcoins.items():
            self.logger.info(f"å¤„ç†å¸ç§: {coin_info.name} ({symbol})")
            
            symbol_data = {}
            
            for timeframe in self.timeframes.keys():
                try:
                    # ä¸‹è½½æ•°æ®
                    df = await self.download_symbol_data(symbol, timeframe)
                    
                    if not df.empty:
                        # ä¿å­˜æ•°æ®
                        file_path = self.save_data(df, symbol, timeframe)
                        
                        if file_path:
                            symbol_data[timeframe] = {
                                'file_path': file_path,
                                'records_count': len(df),
                                'quality_score': self.assess_data_quality(df, symbol, timeframe),
                                'date_range': {
                                    'start': df.index.min().isoformat(),
                                    'end': df.index.max().isoformat()
                                }
                            }
                    else:
                        failed_downloads.append(f"{symbol}_{timeframe}")
                        
                except Exception as e:
                    self.logger.error(f"å¤„ç†å¤±è´¥ {symbol} {timeframe}: {e}")
                    failed_downloads.append(f"{symbol}_{timeframe}")
                
                completed_tasks += 1
                progress = (completed_tasks / total_tasks) * 100
                self.logger.info(f"æ€»ä½“è¿›åº¦: {progress:.1f}% ({completed_tasks}/{total_tasks})")
            
            if symbol_data:
                successful_downloads[symbol] = {
                    'coin_info': asdict(coin_info),
                    'timeframes': symbol_data
                }
        
        collection_end_time = datetime.now()
        collection_duration = collection_end_time - collection_start_time
        
        # ç”Ÿæˆæ”¶é›†æŠ¥å‘Š
        collection_report = {
            'collection_summary': {
                'start_time': collection_start_time.isoformat(),
                'end_time': collection_end_time.isoformat(),
                'duration_minutes': collection_duration.total_seconds() / 60,
                'total_symbols': len(self.top30_altcoins),
                'successful_symbols': len(successful_downloads),
                'failed_downloads': failed_downloads,
                'total_files_created': sum(
                    len(data['timeframes']) for data in successful_downloads.values()
                )
            },
            'data_collection': successful_downloads,
            'quality_standards': self.quality_standards,
            'collection_config': self.collection_config
        }
        
        self.logger.info(f"æ•°æ®æ”¶é›†å®Œæˆï¼è€—æ—¶: {collection_duration.total_seconds()/60:.1f} åˆ†é’Ÿ")
        self.logger.info(f"æˆåŠŸ: {len(successful_downloads)} å¸ç§, å¤±è´¥: {len(failed_downloads)} ä»»åŠ¡")
        
        return collection_report
    
    def analyze_market_characteristics(self, collection_report: Dict) -> Dict:
        """åˆ†æå¸‚åœºç‰¹å¾"""
        self.logger.info("åˆ†æå¸‚åœºç‰¹å¾...")
        
        analysis_results = {
            'liquidity_distribution': self.analyze_liquidity_tiers(),
            'category_distribution': self.analyze_category_distribution(),
            'volume_analysis': self.analyze_volume_patterns(),
            'quality_summary': self.analyze_quality_distribution(collection_report),
            'correlation_potential': self.estimate_correlation_patterns(),
            'trading_suitability': self.assess_trading_suitability()
        }
        
        return analysis_results
    
    def analyze_liquidity_tiers(self) -> Dict:
        """æµåŠ¨æ€§åˆ†å±‚åˆ†æ"""
        tiers = {}
        for symbol, coin_info in self.top30_altcoins.items():
            tier = coin_info.liquidity_tier
            if tier not in tiers:
                tiers[tier] = []
            tiers[tier].append({
                'symbol': symbol,
                'name': coin_info.name,
                'volume_usd': coin_info.daily_volume_usd
            })
        
        return tiers
    
    def analyze_category_distribution(self) -> Dict:
        """ç±»åˆ«åˆ†å¸ƒåˆ†æ"""
        categories = {}
        for symbol, coin_info in self.top30_altcoins.items():
            category = coin_info.category
            if category not in categories:
                categories[category] = []
            categories[category].append({
                'symbol': symbol,
                'name': coin_info.name,
                'rank': coin_info.market_cap_rank
            })
        
        return categories
    
    def analyze_volume_patterns(self) -> Dict:
        """æˆäº¤é‡æ¨¡å¼åˆ†æ"""
        volume_stats = {}
        total_volume = sum(coin.daily_volume_usd for coin in self.top30_altcoins.values())
        
        for symbol, coin_info in self.top30_altcoins.items():
            volume_share = coin_info.daily_volume_usd / total_volume
            volume_stats[symbol] = {
                'daily_volume_usd': coin_info.daily_volume_usd,
                'volume_share': volume_share,
                'volume_tier': 'High' if volume_share > 0.05 else 'Medium' if volume_share > 0.02 else 'Low'
            }
        
        return volume_stats
    
    def analyze_quality_distribution(self, collection_report: Dict) -> Dict:
        """è´¨é‡åˆ†å¸ƒåˆ†æ"""
        quality_scores = []
        quality_by_symbol = {}
        
        for symbol, data in collection_report.get('data_collection', {}).items():
            symbol_scores = []
            for timeframe, tf_data in data.get('timeframes', {}).items():
                score = tf_data.get('quality_score', 0)
                symbol_scores.append(score)
                quality_scores.append(score)
            
            if symbol_scores:
                quality_by_symbol[symbol] = {
                    'avg_quality': np.mean(symbol_scores),
                    'min_quality': min(symbol_scores),
                    'max_quality': max(symbol_scores)
                }
        
        return {
            'overall_avg_quality': np.mean(quality_scores) if quality_scores else 0,
            'quality_distribution': {
                'high_quality': len([s for s in quality_scores if s > 0.95]),
                'medium_quality': len([s for s in quality_scores if 0.9 <= s <= 0.95]),
                'low_quality': len([s for s in quality_scores if s < 0.9])
            },
            'quality_by_symbol': quality_by_symbol
        }
    
    def estimate_correlation_patterns(self) -> Dict:
        """ä¼°ç®—ç›¸å…³æ€§æ¨¡å¼"""
        correlation_estimates = {
            'high_correlation_groups': [
                ['BTCUSDT', 'LTCUSDT', 'BCHUSDT'],  # Bitcoin forks
                ['ETHUSDT', 'ADAUSDT', 'SOLUSDT', 'AVAXUSDT'],  # Smart contract platforms
                ['UNIUSDT', 'AAVEUSDT', 'MKRUSDT'],  # DeFi tokens
                ['ARBUSDT', 'OPUSDT', 'MATICUSDT']   # Layer 2 solutions
            ],
            'low_correlation_candidates': [
                ['XRPUSDT', 'DOGEUSDT'],  # Different use cases
                ['FILUSDT', 'HBARUSDT'],  # Enterprise/storage
                ['LINKUSDT', 'GRTUSDT'],  # Infrastructure
                ['TONUSDT', 'NEARUSDT']   # Alternative Layer 1s
            ]
        }
        
        return correlation_estimates
    
    def assess_trading_suitability(self) -> Dict:
        """è¯„ä¼°äº¤æ˜“é€‚ç”¨æ€§"""
        suitability_scores = {}
        
        for symbol, coin_info in self.top30_altcoins.items():
            # è¯„ä¼°æ ‡å‡†
            liquidity_score = 1.0 if coin_info.liquidity_tier == "é«˜æµåŠ¨æ€§" else 0.7 if coin_info.liquidity_tier == "ä¸­ç­‰æµåŠ¨æ€§" else 0.4
            volume_score = min(1.0, coin_info.daily_volume_usd / 1000000000)  # åŸºäº10äº¿USDæ ‡å‡†åŒ–
            rank_score = max(0.3, 1 - (coin_info.market_cap_rank - 3) / 30)  # æ’åè¶Šé å‰è¶Šå¥½
            priority_score = (4 - coin_info.priority) / 3  # ä¼˜å…ˆçº§è½¬æ¢ä¸ºè¯„åˆ†
            
            overall_score = (liquidity_score * 0.3 + volume_score * 0.3 + 
                           rank_score * 0.2 + priority_score * 0.2)
            
            suitability_scores[symbol] = {
                'overall_score': overall_score,
                'liquidity_score': liquidity_score,
                'volume_score': volume_score,
                'rank_score': rank_score,
                'priority_score': priority_score,
                'recommendation': 'Excellent' if overall_score > 0.8 else 
                               'Good' if overall_score > 0.6 else 
                               'Fair' if overall_score > 0.4 else 'Poor'
            }
        
        return suitability_scores
    
    def create_market_data_bundle(self, collection_report: Dict, market_analysis: Dict) -> Dict:
        """åˆ›å»ºMarketDataBundle_Top30.json"""
        
        timestamp = datetime.now().isoformat()
        
        # ç­›é€‰é«˜è´¨é‡æ•°æ®
        high_quality_symbols = []
        for symbol, data in collection_report.get('data_collection', {}).items():
            avg_quality = np.mean([
                tf_data.get('quality_score', 0) 
                for tf_data in data.get('timeframes', {}).values()
            ])
            if avg_quality > self.quality_standards['completeness_threshold']:
                high_quality_symbols.append(symbol)
        
        bundle = {
            "version": timestamp,
            "metadata": {
                "bundle_id": f"top30_altcoins_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "strategy_name": "DipMaster_Top30_Altcoins_V1",
                "description": "å‰30å¤§å¸‚å€¼å±±å¯¨å¸å®Œæ•´å†å²æ•°æ®é›† - ä¸ºDipMasterç­–ç•¥ä¼˜åŒ–å‡†å¤‡",
                "total_symbols": len(self.top30_altcoins),
                "high_quality_symbols": len(high_quality_symbols),
                "data_coverage": "2å¹´å†å²æ•°æ® (2023-2025)",
                "timeframes": list(self.timeframes.keys()),
                "exchanges": ["binance"],
                "collection_date": timestamp,
                "data_quality_standard": self.quality_standards
            },
            
            "symbol_specifications": {
                symbol: {
                    "coin_info": asdict(coin_info),
                    "trading_suitability": market_analysis['trading_suitability'].get(symbol, {}),
                    "data_quality": collection_report.get('data_collection', {}).get(symbol, {})
                }
                for symbol, coin_info in self.top30_altcoins.items()
            },
            
            "market_analysis": market_analysis,
            
            "data_files": {
                symbol: {
                    timeframe: {
                        "file_path": f"data/enhanced_market_data/{symbol}_{timeframe}_2years.parquet",
                        "metadata_path": f"data/enhanced_market_data/{symbol}_{timeframe}_2years_metadata.json",
                        "format": "parquet",
                        "compression": "snappy"
                    }
                    for timeframe in self.timeframes.keys()
                }
                for symbol in self.top30_altcoins.keys()
            },
            
            "quality_assurance": {
                "standards": self.quality_standards,
                "validation_rules": {
                    "minimum_completeness": "99%",
                    "maximum_gap_minutes": 30,
                    "price_spike_detection": "50% threshold",
                    "volume_outlier_detection": "5 sigma",
                    "ohlc_consistency_check": "enabled"
                },
                "quality_distribution": market_analysis.get('quality_summary', {}),
                "recommended_symbols": high_quality_symbols
            },
            
            "usage_recommendations": {
                "excellent_quality": [
                    symbol for symbol, data in market_analysis.get('trading_suitability', {}).items()
                    if data.get('recommendation') == 'Excellent'
                ],
                "portfolio_size": min(15, len(high_quality_symbols)),
                "correlation_groups": market_analysis.get('correlation_potential', {}).get('high_correlation_groups', []),
                "low_correlation_pairs": market_analysis.get('correlation_potential', {}).get('low_correlation_candidates', []),
                "risk_considerations": [
                    "ç›‘æ§æ•°æ®è´¨é‡å˜åŒ–",
                    "æ³¨æ„æµåŠ¨æ€§åˆ†å±‚å·®å¼‚", 
                    "è€ƒè™‘ç±»åˆ«ç›¸å…³æ€§",
                    "å®šæœŸéªŒè¯æˆäº¤é‡",
                    "å…³æ³¨å¸‚åœºåˆ¶åº¦å˜åŒ–"
                ]
            },
            
            "performance_metrics": {
                "collection_time_minutes": collection_report.get('collection_summary', {}).get('duration_minutes', 0),
                "success_rate": len(collection_report.get('data_collection', {})) / len(self.top30_altcoins),
                "average_quality_score": market_analysis.get('quality_summary', {}).get('overall_avg_quality', 0),
                "total_data_points": sum(
                    sum(tf_data.get('records_count', 0) for tf_data in data.get('timeframes', {}).values())
                    for data in collection_report.get('data_collection', {}).values()
                ),
                "estimated_storage_mb": len(self.top30_altcoins) * len(self.timeframes) * 50  # ä¼°ç®—
            },
            
            "timestamp": timestamp
        }
        
        return bundle

# ä¸»æ‰§è¡Œå‡½æ•°
async def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œå®Œæ•´çš„æ•°æ®æ”¶é›†æµç¨‹"""
    collector = Top30AltcoinsDataCollector()
    
    try:
        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰æ•°æ®
        collection_report = await collector.collect_all_data()
        
        # ç¬¬äºŒæ­¥ï¼šå¸‚åœºç‰¹å¾åˆ†æ
        market_analysis = collector.analyze_market_characteristics(collection_report)
        
        # ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºæ•°æ®æŸé…ç½®
        market_data_bundle = collector.create_market_data_bundle(collection_report, market_analysis)
        
        # ä¿å­˜æŠ¥å‘Š
        reports_path = Path("data")
        reports_path.mkdir(exist_ok=True)
        
        # ä¿å­˜æ”¶é›†æŠ¥å‘Š
        collection_report_path = reports_path / f"Top30_Collection_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(collection_report_path, 'w', encoding='utf-8') as f:
            json.dump(collection_report, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜å¸‚åœºåˆ†æ
        market_analysis_path = reports_path / f"Top30_Market_Analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(market_analysis_path, 'w', encoding='utf-8') as f:
            json.dump(market_analysis, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜æ•°æ®æŸé…ç½®
        bundle_path = reports_path / "MarketDataBundle_Top30.json"
        with open(bundle_path, 'w', encoding='utf-8') as f:
            json.dump(market_data_bundle, f, ensure_ascii=False, indent=2, default=str)
        
        # ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
        print("\n" + "="*80)
        print("å‰30å¤§å±±å¯¨å¸æ•°æ®æ”¶é›†å®Œæˆ!")
        print("="*80)
        print(f"ğŸ“Š æ•°æ®æ”¶é›†æŠ¥å‘Š: {collection_report_path}")
        print(f"ğŸ“ˆ å¸‚åœºåˆ†ææŠ¥å‘Š: {market_analysis_path}")
        print(f"ğŸ¯ æ•°æ®æŸé…ç½®: {bundle_path}")
        print(f"â±ï¸  æ”¶é›†è€—æ—¶: {collection_report.get('collection_summary', {}).get('duration_minutes', 0):.1f} åˆ†é’Ÿ")
        print(f"âœ… æˆåŠŸå¸ç§: {collection_report.get('collection_summary', {}).get('successful_symbols', 0)}/{len(collector.top30_altcoins)}")
        print(f"ğŸ† å¹³å‡è´¨é‡: {market_analysis.get('quality_summary', {}).get('overall_avg_quality', 0):.3f}")
        print("="*80)
        
        return market_data_bundle
        
    except Exception as e:
        collector.logger.error(f"æ•°æ®æ”¶é›†æµç¨‹å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())