#!/usr/bin/env python3
"""
DipMasteræŒç»­è®­ç»ƒä¼˜åŒ–ç³»ç»Ÿ
ç›®æ ‡: èƒœç‡85%+, å¤æ™®æ¯”ç‡>1.5, æœ€å¤§å›æ’¤<3%, å¹´åŒ–æ”¶ç›Š>15%
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ åº“
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import lightgbm as lgb
import xgboost as xgb
try:
    import catboost as cb
except ImportError:
    cb = None

# è¶…å‚æ•°ä¼˜åŒ–
import optuna
from optuna.samplers import TPESampler

# å›æµ‹å’Œæ€§èƒ½åˆ†æ
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# æœ¬åœ°æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# æ•°æ®é¢„å¤„ç†å’ŒéªŒè¯
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
import pickle
import joblib
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    target_win_rate: float = 0.85
    target_sharpe: float = 1.5
    target_max_drawdown: float = 0.03
    target_annual_return: float = 0.15
    
    # äº¤æ˜“æˆæœ¬
    commission_rate: float = 0.001  # 0.1% commission
    slippage_rate: float = 0.0005   # 0.05% slippage
    
    # è®­ç»ƒå‚æ•°
    cv_folds: int = 5
    embargo_hours: int = 2
    test_size: float = 0.2
    
    # æ¨¡å‹å‚æ•°
    optuna_trials: int = 100
    max_features_per_model: int = 30
    ensemble_models: List[str] = None
    
    def __post_init__(self):
        if self.ensemble_models is None:
            self.ensemble_models = ['lgbm', 'xgb', 'rf', 'lr']

class PurgedKFold:
    """æ—¶åºäº¤å‰éªŒè¯withæ•°æ®æ¸…æ´—"""
    
    def __init__(self, n_splits=5, embargo_hours=2):
        self.n_splits = n_splits
        self.embargo_td = timedelta(hours=embargo_hours)
        
    def split(self, X, y=None, groups=None):
        """ç”Ÿæˆè®­ç»ƒ/éªŒè¯åˆ†å‰²"""
        if 'timestamp' not in X.columns:
            raise ValueError("éœ€è¦timestampåˆ—è¿›è¡Œæ—¶åºåˆ†å‰²")
            
        timestamps = pd.to_datetime(X['timestamp'])
        indices = np.arange(len(X))
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        n_samples = len(X)
        fold_size = n_samples // self.n_splits
        
        for i in range(self.n_splits):
            # è®­ç»ƒé›†ç»“æŸç‚¹
            train_end_idx = fold_size * (i + 1)
            if train_end_idx >= n_samples:
                continue
                
            # éªŒè¯é›†å¼€å§‹ç‚¹ï¼ˆæ·»åŠ embargoæœŸï¼‰
            train_end_time = timestamps.iloc[train_end_idx]
            val_start_time = train_end_time + self.embargo_td
            
            # æ‰¾åˆ°éªŒè¯é›†å®é™…å¼€å§‹ç´¢å¼•
            val_start_idx = timestamps[timestamps >= val_start_time].index
            if len(val_start_idx) == 0:
                continue
            val_start_idx = val_start_idx[0]
            
            # éªŒè¯é›†ç»“æŸç‚¹
            val_end_idx = min(train_end_idx + fold_size, n_samples)
            
            train_indices = indices[:train_end_idx]
            val_indices = indices[val_start_idx:val_end_idx]
            
            if len(train_indices) > 100 and len(val_indices) > 50:
                yield train_indices, val_indices

class ContinuousTrainingSystem:
    """æŒç»­è®­ç»ƒä¼˜åŒ–ç³»ç»Ÿ"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.performance_history = []
        self.best_params = {}
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("results/continuous_training")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(42)
        
    def load_multi_symbol_data(self, data_dir: str) -> Dict[str, pd.DataFrame]:
        """åŠ è½½å¤šå¸ç§ç‰¹å¾æ•°æ®"""
        logger.info("åŠ è½½å¤šå¸ç§ç‰¹å¾æ•°æ®...")
        
        data_files = list(Path(data_dir).glob("features_*_optimized_*.parquet"))
        datasets = {}
        
        for file_path in data_files:
            try:
                # ä»æ–‡ä»¶åæå–å¸ç§
                symbol = file_path.stem.split('_')[1]  # features_BTCUSDT_optimized_...
                
                df = pd.read_parquet(file_path)
                if len(df) > 1000:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®
                    datasets[symbol] = df
                    logger.info(f"åŠ è½½ {symbol}: {len(df)} samples, {len(df.columns)} features")
                    
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½ {file_path}: {e}")
                
        logger.info(f"æˆåŠŸåŠ è½½ {len(datasets)} ä¸ªå¸ç§çš„æ•°æ®")
        return datasets
        
    def prepare_features_and_labels(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
        """å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾"""
        # ç§»é™¤æ—¶é—´æˆ³å’ŒåŸå§‹ä»·æ ¼æ•°æ®
        feature_cols = [col for col in df.columns if col not in [
            'timestamp', 'target_return', 'target_binary', 
            'target_return_12p', 'target_return_24p', 'target_return_48p',
            'target_profitable_12p', 'target_profitable_24p', 'target_profitable_48p'
        ]]
        
        # é€‰æ‹©ä¸»è¦ç›®æ ‡ï¼ˆ12æœŸæ ‡ç­¾æœ€ç›¸å…³ï¼‰
        X = df[feature_cols].copy()
        y_return = df['target_return_12p'].copy() if 'target_return_12p' in df.columns else df.get('target_return', pd.Series())
        y_binary = df['target_profitable_12p'].copy() if 'target_profitable_12p' in df.columns else df.get('target_binary', pd.Series())
        
        # æ¸…ç†æ•°æ®
        mask = ~(y_return.isna() | y_binary.isna())
        X = X[mask].fillna(method='ffill').fillna(0)
        y_return = y_return[mask]
        y_binary = y_binary[mask]
        
        logger.info(f"ç‰¹å¾ç»´åº¦: {X.shape}, æ­£æ ·æœ¬æ¯”ä¾‹: {y_binary.mean():.3f}")
        
        return X, y_return, y_binary
    
    def optimize_hyperparameters(self, X, y, model_type: str) -> Dict:
        """ä½¿ç”¨Optunaä¼˜åŒ–è¶…å‚æ•°"""
        logger.info(f"ä¼˜åŒ– {model_type} è¶…å‚æ•°...")
        
        def objective(trial):
            if model_type == 'lgbm':
                params = {
                    'objective': 'binary',
                    'metric': 'binary_logloss',
                    'boosting_type': 'gbdt',
                    'num_leaves': trial.suggest_int('num_leaves', 10, 100),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                    'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
                    'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
                    'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                    'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),
                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),
                    'verbose': -1
                }
                
            elif model_type == 'xgb':
                params = {
                    'objective': 'binary:logistic',
                    'eval_metric': 'logloss',
                    'max_depth': trial.suggest_int('max_depth', 3, 10),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                    'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
                    'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),
                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),
                }
                
            elif model_type == 'rf':
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 100, 500),
                    'max_depth': trial.suggest_int('max_depth', 5, 20),
                    'min_samples_split': trial.suggest_int('min_samples_split', 5, 20),
                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 10),
                    'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.5, 0.8]),
                }
                
            else:  # logistic regression
                params = {
                    'C': trial.suggest_float('C', 0.01, 10, log=True),
                    'penalty': trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet']),
                    'solver': 'saga',
                    'max_iter': 1000
                }
                if params['penalty'] == 'elasticnet':
                    params['l1_ratio'] = trial.suggest_float('l1_ratio', 0, 1)
            
            # äº¤å‰éªŒè¯è¯„ä¼°
            cv_scores = []
            kfold = PurgedKFold(n_splits=3, embargo_hours=self.config.embargo_hours)
            
            for train_idx, val_idx in kfold.split(X):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                
                # æ ‡å‡†åŒ–
                scaler = RobustScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)
                
                try:
                    if model_type == 'lgbm':
                        model = lgb.LGBMClassifier(**params)
                    elif model_type == 'xgb':
                        model = xgb.XGBClassifier(**params)
                    elif model_type == 'rf':
                        model = RandomForestClassifier(**params, random_state=42)
                    else:
                        model = LogisticRegression(**params, random_state=42)
                    
                    model.fit(X_train_scaled, y_train)
                    y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]
                    auc_score = roc_auc_score(y_val, y_pred_proba)
                    cv_scores.append(auc_score)
                    
                except Exception as e:
                    logger.warning(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
                    return 0.5
                    
            return np.mean(cv_scores) if cv_scores else 0.5
        
        # è¿è¡Œä¼˜åŒ–
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=42),
            pruner=optuna.pruners.MedianPruner(n_startup_trials=10)
        )
        
        study.optimize(objective, n_trials=self.config.optuna_trials)
        
        logger.info(f"{model_type} æœ€ä½³AUC: {study.best_value:.4f}")
        return study.best_params
    
    def train_ensemble_model(self, X, y_return, y_binary) -> Dict:
        """è®­ç»ƒé›†æˆæ¨¡å‹"""
        logger.info("è®­ç»ƒé›†æˆæ¨¡å‹...")
        
        # ä¼˜åŒ–å„ä¸ªæ¨¡å‹çš„è¶…å‚æ•°
        optimized_params = {}
        for model_type in self.config.ensemble_models:
            if model_type in ['lgbm', 'xgb', 'rf', 'lr']:
                optimized_params[model_type] = self.optimize_hyperparameters(X, y_binary, model_type)
        
        # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        models = {}
        scalers = {}
        
        # åˆ†å‰²æ•°æ®
        train_size = int(len(X) * (1 - self.config.test_size))
        X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]
        y_binary_train, y_binary_test = y_binary.iloc[:train_size], y_binary.iloc[train_size:]
        y_return_train, y_return_test = y_return.iloc[:train_size], y_return.iloc[train_size:]
        
        # æ ‡å‡†åŒ–
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        scalers['main'] = scaler
        
        # è®­ç»ƒå„ä¸ªæ¨¡å‹
        for model_type in self.config.ensemble_models:
            try:
                logger.info(f"è®­ç»ƒ {model_type} æ¨¡å‹...")
                params = optimized_params.get(model_type, {})
                
                if model_type == 'lgbm':
                    model = lgb.LGBMClassifier(**params)
                elif model_type == 'xgb':
                    model = xgb.XGBClassifier(**params)
                elif model_type == 'rf':
                    model = RandomForestClassifier(**params, random_state=42)
                else:  # lr
                    model = LogisticRegression(**params, random_state=42)
                
                model.fit(X_train_scaled, y_binary_train)
                models[model_type] = model
                
                # è®¡ç®—ç‰¹å¾é‡è¦æ€§
                if hasattr(model, 'feature_importances_'):
                    importance = model.feature_importances_
                elif hasattr(model, 'coef_'):
                    importance = np.abs(model.coef_[0])
                else:
                    importance = np.ones(len(X_train.columns))
                
                self.feature_importance[model_type] = dict(zip(X_train.columns, importance))
                
            except Exception as e:
                logger.error(f"è®­ç»ƒ {model_type} å¤±è´¥: {e}")
                
        # é›†æˆé¢„æµ‹
        ensemble_predictions = self._ensemble_predict(models, scalers, X_test)
        
        # å›æµ‹è¯„ä¼°
        backtest_results = self._comprehensive_backtest(
            predictions=ensemble_predictions,
            actual_returns=y_return_test,
            actual_binary=y_binary_test,
            timestamps=X_test.get('timestamp', pd.Series(range(len(X_test))))
        )
        
        return {
            'models': models,
            'scalers': scalers,
            'optimized_params': optimized_params,
            'backtest_results': backtest_results,
            'test_data': {
                'X_test': X_test,
                'y_return_test': y_return_test,
                'y_binary_test': y_binary_test,
                'predictions': ensemble_predictions
            }
        }
    
    def _ensemble_predict(self, models: Dict, scalers: Dict, X_test: pd.DataFrame) -> np.ndarray:
        """é›†æˆé¢„æµ‹"""
        X_test_scaled = scalers['main'].transform(X_test)
        
        predictions = []
        weights = []
        
        for model_name, model in models.items():
            try:
                pred = model.predict_proba(X_test_scaled)[:, 1]
                predictions.append(pred)
                
                # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®æƒé‡
                if model_name in ['lgbm', 'xgb']:
                    weights.append(0.4)  # æ ‘æ¨¡å‹æƒé‡è¾ƒé«˜
                elif model_name == 'rf':
                    weights.append(0.15)
                else:  # lr
                    weights.append(0.05)
                    
            except Exception as e:
                logger.warning(f"{model_name} é¢„æµ‹å¤±è´¥: {e}")
        
        if not predictions:
            return np.zeros(len(X_test))
            
        # åŠ æƒå¹³å‡
        weights = np.array(weights)
        weights = weights / weights.sum()  # å½’ä¸€åŒ–æƒé‡
        
        ensemble_pred = np.average(predictions, axis=0, weights=weights)
        return ensemble_pred
    
    def _comprehensive_backtest(self, predictions: np.ndarray, actual_returns: pd.Series, 
                               actual_binary: pd.Series, timestamps: pd.Series) -> Dict:
        """ç»¼åˆå›æµ‹åˆ†æ"""
        logger.info("æ‰§è¡Œç»¼åˆå›æµ‹åˆ†æ...")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥ä¾¿è®¡ç®—
        pred_proba = predictions
        actual_ret = actual_returns.values
        actual_bin = actual_binary.values
        
        # åŠ¨æ€é˜ˆå€¼ä¼˜åŒ–
        thresholds = np.linspace(0.1, 0.9, 50)
        best_threshold = 0.5
        best_sharpe = -999
        
        threshold_results = []
        
        for threshold in thresholds:
            signals = (pred_proba >= threshold).astype(int)
            
            if signals.sum() == 0:  # æ²¡æœ‰ä¿¡å·
                continue
                
            # åªåœ¨æœ‰ä¿¡å·æ—¶è¿›è¡Œäº¤æ˜“
            signal_returns = actual_ret[signals == 1]
            
            if len(signal_returns) < 10:  # ä¿¡å·å¤ªå°‘
                continue
            
            # åº”ç”¨äº¤æ˜“æˆæœ¬
            gross_returns = signal_returns
            net_returns = gross_returns - self.config.commission_rate - self.config.slippage_rate
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            win_rate = (signal_returns > 0).mean()
            avg_return = net_returns.mean()
            std_return = net_returns.std()
            sharpe_ratio = avg_return / std_return if std_return > 0 else 0
            
            # ç´¯ç§¯æ”¶ç›Š
            cum_returns = (1 + pd.Series(net_returns)).cumprod()
            max_drawdown = (cum_returns / cum_returns.expanding().max() - 1).min()
            
            result = {
                'threshold': threshold,
                'win_rate': win_rate,
                'avg_return': avg_return,
                'sharpe_ratio': sharpe_ratio,
                'max_drawdown': max_drawdown,
                'num_trades': len(signal_returns),
                'annual_return': avg_return * 252 * 24 * 12  # å‡è®¾5åˆ†é’Ÿæ•°æ®
            }
            
            threshold_results.append(result)
            
            # æ›´æ–°æœ€ä½³é˜ˆå€¼
            if sharpe_ratio > best_sharpe:
                best_sharpe = sharpe_ratio
                best_threshold = threshold
        
        # ä½¿ç”¨æœ€ä½³é˜ˆå€¼è¿›è¡Œæœ€ç»ˆå›æµ‹
        best_signals = (pred_proba >= best_threshold).astype(int)
        signal_indices = np.where(best_signals == 1)[0]
        
        if len(signal_indices) > 0:
            signal_returns = actual_ret[signal_indices]
            net_returns = signal_returns - self.config.commission_rate - self.config.slippage_rate
            
            # è®¡ç®—æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
            final_metrics = {
                'best_threshold': best_threshold,
                'win_rate': (signal_returns > 0).mean(),
                'avg_return_per_trade': net_returns.mean(),
                'sharpe_ratio': net_returns.mean() / net_returns.std() if net_returns.std() > 0 else 0,
                'num_trades': len(signal_returns),
                'total_return': net_returns.sum(),
                'max_drawdown': (pd.Series(net_returns).cumsum().expanding().max() - 
                               pd.Series(net_returns).cumsum()).max(),
                'annual_return': net_returns.mean() * 252 * 24 * 12,  # å¹´åŒ–æ”¶ç›Š
                'profit_factor': (signal_returns[signal_returns > 0].sum() / 
                                -signal_returns[signal_returns < 0].sum() 
                                if (signal_returns < 0).any() else np.inf),
                'threshold_analysis': threshold_results
            }
            
            # ç›®æ ‡è¾¾æˆæƒ…å†µ
            targets_achieved = {
                'win_rate_target': final_metrics['win_rate'] >= self.config.target_win_rate,
                'sharpe_target': final_metrics['sharpe_ratio'] >= self.config.target_sharpe,
                'drawdown_target': abs(final_metrics['max_drawdown']) <= self.config.target_max_drawdown,
                'return_target': final_metrics['annual_return'] >= self.config.target_annual_return
            }
            
            final_metrics['targets_achieved'] = targets_achieved
            final_metrics['all_targets_met'] = all(targets_achieved.values())
            
        else:
            final_metrics = {'error': 'No valid signals generated'}
        
        return final_metrics
    
    def continuous_optimization_loop(self, data_dir: str, hours_interval: int = 2):
        """æŒç»­ä¼˜åŒ–å¾ªç¯"""
        logger.info(f"å¯åŠ¨æŒç»­ä¼˜åŒ–å¾ªç¯ï¼Œæ¯{hours_interval}å°æ—¶é‡è®­ç»ƒ")
        
        iteration = 0
        
        while True:
            try:
                logger.info(f"\n=== ä¼˜åŒ–è¿­ä»£ {iteration + 1} ===")
                
                # åŠ è½½æœ€æ–°æ•°æ®
                datasets = self.load_multi_symbol_data(data_dir)
                
                if not datasets:
                    logger.error("æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡æ­¤æ¬¡è¿­ä»£")
                    continue
                
                # åˆå¹¶å¤šå¸ç§æ•°æ®è¿›è¡Œè®­ç»ƒ
                combined_results = {}
                
                for symbol, data in datasets.items():
                    logger.info(f"\n--- å¤„ç† {symbol} ---")
                    
                    try:
                        X, y_return, y_binary = self.prepare_features_and_labels(data)
                        
                        if len(X) < 1000:
                            logger.warning(f"{symbol} æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
                            continue
                        
                        # è®­ç»ƒæ¨¡å‹
                        result = self.train_ensemble_model(X, y_return, y_binary)
                        combined_results[symbol] = result
                        
                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                        backtest = result['backtest_results']
                        
                        if isinstance(backtest, dict) and 'all_targets_met' in backtest:
                            if backtest['all_targets_met']:
                                logger.info(f"ğŸ‰ {symbol} è¾¾åˆ°æ‰€æœ‰ç›®æ ‡æŒ‡æ ‡!")
                                self._save_successful_model(symbol, result, iteration)
                            else:
                                logger.info(f"âš ï¸ {symbol} å°šæœªè¾¾åˆ°ç›®æ ‡ï¼Œç»§ç»­ä¼˜åŒ–...")
                                self._log_performance_gap(symbol, backtest)
                        
                    except Exception as e:
                        logger.error(f"å¤„ç† {symbol} æ—¶å‡ºé”™: {e}")
                        continue
                
                # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
                self._generate_iteration_report(combined_results, iteration)
                
                # æ€§èƒ½å†å²è®°å½•
                self.performance_history.append({
                    'iteration': iteration,
                    'timestamp': datetime.now(),
                    'results': combined_results
                })
                
                iteration += 1
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å¸ç§è¾¾åˆ°ç›®æ ‡
                targets_met = any(
                    result.get('backtest_results', {}).get('all_targets_met', False)
                    for result in combined_results.values()
                )
                
                if targets_met:
                    logger.info("ğŸ† æ£€æµ‹åˆ°è¾¾æ ‡æ¨¡å‹ï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
                    self._generate_final_report()
                    break
                
                # ç­‰å¾…ä¸‹æ¬¡è¿­ä»£
                logger.info(f"ç­‰å¾… {hours_interval} å°æ—¶åç»§ç»­ä¸‹æ¬¡ä¼˜åŒ–...")
                import time
                time.sleep(hours_interval * 3600)
                
            except KeyboardInterrupt:
                logger.info("ç”¨æˆ·ä¸­æ–­ï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
                self._generate_final_report()
                break
            except Exception as e:
                logger.error(f"è¿­ä»£ {iteration} å‡ºç°é”™è¯¯: {e}")
                continue
    
    def _save_successful_model(self, symbol: str, result: Dict, iteration: int):
        """ä¿å­˜æˆåŠŸçš„æ¨¡å‹"""
        model_dir = self.output_dir / f"successful_models/{symbol}_iteration_{iteration}"
        model_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        for model_name, model in result['models'].items():
            joblib.dump(model, model_dir / f"{model_name}_model.pkl")
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        joblib.dump(result['scalers'], model_dir / "scalers.pkl")
        
        # ä¿å­˜ç»“æœ
        with open(model_dir / "results.json", 'w') as f:
            # è½¬æ¢ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
            serializable_result = self._make_json_serializable(result)
            json.dump(serializable_result, f, indent=2)
        
        logger.info(f"âœ… æˆåŠŸæ¨¡å‹å·²ä¿å­˜è‡³: {model_dir}")
    
    def _make_json_serializable(self, obj):
        """å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()
                   if k not in ['models', 'scalers', 'test_data']}  # æ’é™¤ä¸å¯åºåˆ—åŒ–å¯¹è±¡
        elif isinstance(obj, (list, tuple)):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.int64, np.int32, np.float64, np.float32)):
            return obj.item()
        elif isinstance(obj, pd.Series):
            return obj.tolist()
        elif hasattr(obj, '__dict__'):
            return str(obj)
        else:
            return obj
    
    def _log_performance_gap(self, symbol: str, backtest: Dict):
        """è®°å½•æ€§èƒ½å·®è·"""
        targets = backtest.get('targets_achieved', {})
        metrics = {
            'win_rate': backtest.get('win_rate', 0),
            'sharpe_ratio': backtest.get('sharpe_ratio', 0),
            'max_drawdown': abs(backtest.get('max_drawdown', 0)),
            'annual_return': backtest.get('annual_return', 0)
        }
        
        logger.info(f"{symbol} å½“å‰æ€§èƒ½:")
        logger.info(f"  èƒœç‡: {metrics['win_rate']:.3f} (ç›®æ ‡: {self.config.target_win_rate:.3f}) {'âœ“' if targets.get('win_rate_target', False) else 'âœ—'}")
        logger.info(f"  å¤æ™®æ¯”ç‡: {metrics['sharpe_ratio']:.3f} (ç›®æ ‡: {self.config.target_sharpe:.3f}) {'âœ“' if targets.get('sharpe_target', False) else 'âœ—'}")
        logger.info(f"  æœ€å¤§å›æ’¤: {metrics['max_drawdown']:.3f} (ç›®æ ‡: <{self.config.target_max_drawdown:.3f}) {'âœ“' if targets.get('drawdown_target', False) else 'âœ—'}")
        logger.info(f"  å¹´åŒ–æ”¶ç›Š: {metrics['annual_return']:.3f} (ç›®æ ‡: >{self.config.target_annual_return:.3f}) {'âœ“' if targets.get('return_target', False) else 'âœ—'}")
    
    def _generate_iteration_report(self, results: Dict, iteration: int):
        """ç”Ÿæˆè¿­ä»£æŠ¥å‘Š"""
        report_path = self.output_dir / f"iteration_{iteration}_report.json"
        
        with open(report_path, 'w') as f:
            serializable_results = self._make_json_serializable(results)
            json.dump({
                'iteration': iteration,
                'timestamp': datetime.now().isoformat(),
                'results': serializable_results
            }, f, indent=2)
        
        logger.info(f"ğŸ“Š è¿­ä»£ {iteration} æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæ€§èƒ½åˆ†ææŠ¥å‘Š"""
        logger.info("ç”Ÿæˆæœ€ç»ˆæ€§èƒ½åˆ†ææŠ¥å‘Š...")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_iterations': len(self.performance_history),
            'performance_summary': {},
            'successful_models': [],
            'recommendations': []
        }
        
        # åˆ†ææˆåŠŸæ¨¡å‹ç›®å½•
        successful_models_dir = self.output_dir / "successful_models"
        if successful_models_dir.exists():
            for model_dir in successful_models_dir.iterdir():
                if model_dir.is_dir():
                    try:
                        with open(model_dir / "results.json", 'r') as f:
                            model_result = json.load(f)
                            report['successful_models'].append({
                                'path': str(model_dir),
                                'symbol': model_dir.name.split('_')[0],
                                'iteration': model_dir.name.split('_')[-1],
                                'performance': model_result.get('backtest_results', {})
                            })
                    except Exception as e:
                        logger.warning(f"æ— æ³•è¯»å–æ¨¡å‹ç»“æœ: {e}")
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_report = self._create_html_report(report)
        html_path = self.output_dir / "final_performance_report.html"
        
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_report)
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_path = self.output_dir / "final_performance_report.json"
        with open(json_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"ğŸ“ˆ æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ:")
        logger.info(f"  HTML: {html_path}")
        logger.info(f"  JSON: {json_path}")
        
        return report
    
    def _create_html_report(self, report: Dict) -> str:
        """åˆ›å»ºHTMLæ ¼å¼çš„æŠ¥å‘Š"""
        html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DipMasteræŒç»­è®­ç»ƒä¼˜åŒ–æŠ¥å‘Š</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        h1 {{ color: #2c3e50; text-align: center; margin-bottom: 30px; }}
        h2 {{ color: #34495e; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
        .summary {{ background: #ecf0f1; padding: 20px; border-radius: 5px; margin: 20px 0; }}
        .model-card {{
            border: 1px solid #bdc3c7;
            padding: 20px;
            margin: 15px 0;
            border-radius: 5px;
            background: #ffffff;
        }}
        .success {{ border-left: 4px solid #27ae60; background: #d5f5d8; }}
        .metrics {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 15px 0;
        }}
        .metric {{
            text-align: center;
            padding: 15px;
            background: #f8f9fa;
            border-radius: 5px;
        }}
        .metric-value {{ font-size: 24px; font-weight: bold; color: #2980b9; }}
        .metric-label {{ font-size: 14px; color: #7f8c8d; }}
        .status-good {{ color: #27ae60; }}
        .status-bad {{ color: #e74c3c; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background: #3498db; color: white; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¯ DipMasteræŒç»­è®­ç»ƒä¼˜åŒ–æŠ¥å‘Š</h1>
        
        <div class="summary">
            <h2>ğŸ“Š æ‰§è¡Œæ€»ç»“</h2>
            <p><strong>æŠ¥å‘Šç”Ÿæˆæ—¶é—´:</strong> {report['timestamp']}</p>
            <p><strong>æ€»è¿­ä»£æ¬¡æ•°:</strong> {report['total_iterations']}</p>
            <p><strong>æˆåŠŸæ¨¡å‹æ•°é‡:</strong> {len(report['successful_models'])}</p>
        </div>
        
        <h2>ğŸ† æˆåŠŸæ¨¡å‹</h2>
        {self._generate_success_models_html(report['successful_models'])}
        
        <h2>ğŸ¯ ç›®æ ‡è¾¾æˆæƒ…å†µ</h2>
        <div class="metrics">
            <div class="metric">
                <div class="metric-label">ç›®æ ‡èƒœç‡</div>
                <div class="metric-value">â‰¥85%</div>
            </div>
            <div class="metric">
                <div class="metric-label">ç›®æ ‡å¤æ™®æ¯”ç‡</div>
                <div class="metric-value">â‰¥1.5</div>
            </div>
            <div class="metric">
                <div class="metric-label">ç›®æ ‡æœ€å¤§å›æ’¤</div>
                <div class="metric-value">â‰¤3%</div>
            </div>
            <div class="metric">
                <div class="metric-label">ç›®æ ‡å¹´åŒ–æ”¶ç›Š</div>
                <div class="metric-value">â‰¥15%</div>
            </div>
        </div>
        
        <div style="margin-top: 40px; padding: 20px; background: #f8f9fa; border-radius: 5px;">
            <p><strong>æ³¨æ„:</strong> æœ¬æŠ¥å‘ŠåŸºäºå†å²æ•°æ®å›æµ‹ç”Ÿæˆï¼Œå®é™…äº¤æ˜“ç»“æœå¯èƒ½ä¸å›æµ‹ç»“æœå­˜åœ¨å·®å¼‚ã€‚</p>
            <p><strong>é£é™©æç¤º:</strong> åŠ å¯†è´§å¸äº¤æ˜“å­˜åœ¨é«˜é£é™©ï¼Œè¯·è°¨æ…æŠ•èµ„ã€‚</p>
        </div>
    </div>
</body>
</html>
        """
        return html
    
    def _generate_success_models_html(self, successful_models: List) -> str:
        """ç”ŸæˆæˆåŠŸæ¨¡å‹çš„HTML"""
        if not successful_models:
            return "<p>æš‚æ— è¾¾åˆ°ç›®æ ‡çš„æ¨¡å‹ã€‚</p>"
        
        html = ""
        for model in successful_models:
            performance = model.get('performance', {})
            
            html += f"""
            <div class="model-card success">
                <h3>âœ… {model['symbol']} - è¿­ä»£ {model['iteration']}</h3>
                <div class="metrics">
                    <div class="metric">
                        <div class="metric-value status-good">{performance.get('win_rate', 0):.1%}</div>
                        <div class="metric-label">èƒœç‡</div>
                    </div>
                    <div class="metric">
                        <div class="metric-value status-good">{performance.get('sharpe_ratio', 0):.2f}</div>
                        <div class="metric-label">å¤æ™®æ¯”ç‡</div>
                    </div>
                    <div class="metric">
                        <div class="metric-value status-good">{abs(performance.get('max_drawdown', 0)):.1%}</div>
                        <div class="metric-label">æœ€å¤§å›æ’¤</div>
                    </div>
                    <div class="metric">
                        <div class="metric-value status-good">{performance.get('annual_return', 0):.1%}</div>
                        <div class="metric-label">å¹´åŒ–æ”¶ç›Š</div>
                    </div>
                </div>
                <p><strong>äº¤æ˜“æ¬¡æ•°:</strong> {performance.get('num_trades', 0)}</p>
                <p><strong>ç›ˆäºæ¯”:</strong> {performance.get('profit_factor', 0):.2f}</p>
                <p><strong>æ¨¡å‹è·¯å¾„:</strong> {model['path']}</p>
            </div>
            """
        
        return html

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    config = TrainingConfig()
    
    # åˆ›å»ºè®­ç»ƒç³»ç»Ÿ
    training_system = ContinuousTrainingSystem(config)
    
    # æ•°æ®ç›®å½•
    data_dir = "data/continuous_optimization"
    
    logger.info("ğŸš€ å¯åŠ¨DipMasteræŒç»­è®­ç»ƒä¼˜åŒ–ç³»ç»Ÿ")
    logger.info(f"ç›®æ ‡: èƒœç‡â‰¥{config.target_win_rate:.1%}, å¤æ™®â‰¥{config.target_sharpe}, å›æ’¤â‰¤{config.target_max_drawdown:.1%}, å¹´åŒ–â‰¥{config.target_annual_return:.1%}")
    
    try:
        # è¿è¡ŒæŒç»­ä¼˜åŒ–å¾ªç¯
        training_system.continuous_optimization_loop(data_dir, hours_interval=2)
        
    except Exception as e:
        logger.error(f"ç³»ç»Ÿè¿è¡Œå‡ºé”™: {e}")
        raise
    
    logger.info("âœ… æŒç»­è®­ç»ƒä¼˜åŒ–ç³»ç»Ÿè¿è¡Œå®Œæˆ")

if __name__ == "__main__":
    main()