"""
Complete ML Pipeline for DipMaster Strategy
Integrates all components for end-to-end model training and backtesting.
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

import numpy as np
import pandas as pd
import json
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
import logging
import joblib
from dataclasses import asdict

# Import our custom modules
from time_series_validator import PurgedWalkForwardCV, RobustTimeSeriesValidator
from ensemble_model_trainer import EnsembleModelTrainer
from feature_engineering_pipeline import DipMasterFeatureEngineer
from realistic_backtester import RealisticBacktester, TradingCosts, RiskLimits, BacktestResult

warnings.filterwarnings('ignore')

class CompleteDipMasterMLPipeline:
    """
    Complete end-to-end ML pipeline for DipMaster strategy
    """
    
    def __init__(self,
                 output_dir: str = "results/ml_pipeline",
                 random_state: int = 42,
                 config: Dict[str, Any] = None):
        """
        Initialize complete ML pipeline
        
        Args:
            output_dir: Output directory for results
            random_state: Random state for reproducibility
            config: Configuration dictionary
        """
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.random_state = random_state
        
        # Configuration
        default_config = {
            'cv_splits': 5,
            'embargo_hours': 24,
            'optimization_trials': 50,
            'confidence_threshold': 0.6,
            'initial_capital': 10000.0,
            'lookback_windows': [5, 10, 20, 50],
            'target_returns': [0.006, 0.008, 0.012],
            'max_holding_minutes': 180,
            'models': ['lightgbm', 'xgboost', 'random_forest', 'logistic_regression']
        }\n        \n        self.config = {**default_config, **(config or {})}\n        \n        # Initialize components\n        self.feature_engineer = DipMasterFeatureEngineer(\n            lookback_windows=self.config['lookback_windows'],\n            target_returns=self.config['target_returns'],\n            max_holding_minutes=self.config['max_holding_minutes']\n        )\n        \n        self.ensemble_trainer = EnsembleModelTrainer(\n            models=self.config['models'],\n            cv_splits=self.config['cv_splits'],\n            embargo_hours=self.config['embargo_hours'],\n            optimization_trials=self.config['optimization_trials']\n        )\n        \n        self.validator = RobustTimeSeriesValidator(\n            primary_cv=PurgedWalkForwardCV(\n                n_splits=self.config['cv_splits'],\n                embargo_hours=self.config['embargo_hours']\n            )\n        )\n        \n        # Trading costs and risk limits\n        self.trading_costs = TradingCosts(\n            maker_fee=0.0002,\n            taker_fee=0.0004,\n            slippage_base=0.0001,\n            slippage_impact=0.00005,\n            funding_rate_8h=0.0001\n        )\n        \n        self.risk_limits = RiskLimits(\n            max_position_size=1000.0,\n            max_concurrent_positions=3,\n            daily_loss_limit=-500.0,\n            position_timeout_minutes=self.config['max_holding_minutes']\n        )\n        \n        # Results storage\n        self.raw_data = None\n        self.processed_data = None\n        self.trained_models = None\n        self.validation_results = {}\n        self.backtest_results = None\n        \n        # Setup logging\n        self.logger = self._setup_logging()\n        \n    def _setup_logging(self) -> logging.Logger:\n        \"\"\"Setup logging configuration\"\"\"\n        logger = logging.getLogger('CompleteDipMasterMLPipeline')\n        logger.setLevel(logging.INFO)\n        \n        if not logger.handlers:\n            # File handler\n            log_file = self.output_dir / f\"ml_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n            file_handler = logging.FileHandler(log_file)\n            file_formatter = logging.Formatter(\n                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n            )\n            file_handler.setFormatter(file_formatter)\n            logger.addHandler(file_handler)\n            \n            # Console handler\n            console_handler = logging.StreamHandler()\n            console_formatter = logging.Formatter(\n                '%(asctime)s - %(levelname)s - %(message)s'\n            )\n            console_handler.setFormatter(console_formatter)\n            logger.addHandler(console_handler)\n        \n        return logger\n    \n    def load_market_data(self, data_path: str) -> pd.DataFrame:\n        \"\"\"\n        Load market data from file\n        \n        Args:\n            data_path: Path to market data file (CSV or Parquet)\n            \n        Returns:\n            Market data DataFrame with datetime index\n        \"\"\"\n        \n        self.logger.info(f\"Loading market data from {data_path}\")\n        \n        data_path = Path(data_path)\n        \n        if data_path.suffix == '.parquet':\n            self.raw_data = pd.read_parquet(data_path)\n        elif data_path.suffix == '.csv':\n            self.raw_data = pd.read_csv(data_path, index_col=0, parse_dates=True)\n        else:\n            raise ValueError(f\"Unsupported file format: {data_path.suffix}\")\n        \n        # Ensure datetime index\n        if not isinstance(self.raw_data.index, pd.DatetimeIndex):\n            self.raw_data.index = pd.to_datetime(self.raw_data.index)\n        \n        # Sort by timestamp\n        self.raw_data = self.raw_data.sort_index()\n        \n        # Basic validation\n        required_columns = ['open', 'high', 'low', 'close', 'volume']\n        missing_columns = [col for col in required_columns if col not in self.raw_data.columns]\n        if missing_columns:\n            raise ValueError(f\"Missing required columns: {missing_columns}\")\n        \n        self.logger.info(f\"Loaded {len(self.raw_data)} rows, {self.raw_data.shape[1]} columns\")\n        self.logger.info(f\"Date range: {self.raw_data.index.min()} to {self.raw_data.index.max()}\")\n        \n        return self.raw_data\n    \n    def engineer_features(self) -> pd.DataFrame:\n        \"\"\"\n        Perform comprehensive feature engineering\n        \n        Returns:\n            Enhanced DataFrame with engineered features and labels\n        \"\"\"\n        \n        if self.raw_data is None:\n            raise ValueError(\"Market data not loaded. Call load_market_data() first.\")\n        \n        self.logger.info(\"Starting feature engineering pipeline\")\n        \n        # Run feature engineering\n        self.processed_data = self.feature_engineer.engineer_features(self.raw_data)\n        \n        # Validate features\n        validation_results = self.feature_engineer.validate_features(self.processed_data)\n        \n        # Save validation results\n        validation_file = self.output_dir / f\"feature_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n        with open(validation_file, 'w') as f:\n            json.dump(validation_results, f, indent=2, default=str)\n        \n        self.logger.info(f\"Feature engineering complete: {self.processed_data.shape}\")\n        self.logger.info(f\"Feature validation saved to: {validation_file}\")\n        \n        return self.processed_data\n    \n    def prepare_training_data(self, \n                             train_start: str = None,\n                             train_end: str = None,\n                             test_start: str = None,\n                             test_end: str = None) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Prepare training and test datasets\n        \n        Args:\n            train_start: Training start date (YYYY-MM-DD)\n            train_end: Training end date (YYYY-MM-DD)\n            test_start: Test start date (YYYY-MM-DD)\n            test_end: Test end date (YYYY-MM-DD)\n            \n        Returns:\n            Dictionary with train and test datasets\n        \"\"\"\n        \n        if self.processed_data is None:\n            raise ValueError(\"Features not engineered. Call engineer_features() first.\")\n        \n        self.logger.info(\"Preparing training data splits\")\n        \n        # Auto-determine splits if not provided\n        if not all([train_start, train_end, test_start, test_end]):\n            total_len = len(self.processed_data)\n            train_end_idx = int(total_len * 0.8)  # 80% for training\n            \n            train_data = self.processed_data.iloc[:train_end_idx]\n            test_data = self.processed_data.iloc[train_end_idx:]\n        else:\n            # Use provided date ranges\n            train_mask = (self.processed_data.index >= train_start) & (self.processed_data.index <= train_end)\n            test_mask = (self.processed_data.index >= test_start) & (self.processed_data.index <= test_end)\n            \n            train_data = self.processed_data[train_mask]\n            test_data = self.processed_data[test_mask]\n        \n        # Identify feature columns (exclude labels)\n        label_keywords = ['future_', 'profitable_', 'hits_', 'dipmaster_primary_label']\n        feature_columns = [\n            col for col in self.processed_data.columns\n            if not any(keyword in col for keyword in label_keywords)\n        ]\n        \n        # Primary label (DipMaster 15-minute exit with 0.8% target)\n        primary_label = 'dipmaster_primary_label'\n        if primary_label not in self.processed_data.columns:\n            # Fallback to simple profitable label\n            primary_label = 'profitable_0.8%_15m'\n            if primary_label not in self.processed_data.columns:\n                raise ValueError(\"No suitable primary label found\")\n        \n        # Prepare datasets\n        datasets = {\n            'train': {\n                'X': train_data[feature_columns],\n                'y': train_data[primary_label]\n            },\n            'test': {\n                'X': test_data[feature_columns],\n                'y': test_data[primary_label]\n            }\n        }\n        \n        # Remove rows with NaN labels\n        for dataset_name in ['train', 'test']:\n            valid_mask = ~datasets[dataset_name]['y'].isna()\n            datasets[dataset_name]['X'] = datasets[dataset_name]['X'][valid_mask]\n            datasets[dataset_name]['y'] = datasets[dataset_name]['y'][valid_mask]\n        \n        self.logger.info(f\"Training data: {len(datasets['train']['X'])} samples\")\n        self.logger.info(f\"Test data: {len(datasets['test']['X'])} samples\")\n        self.logger.info(f\"Features: {len(feature_columns)}\")\n        self.logger.info(f\"Primary label: {primary_label}\")\n        \n        # Log label distribution\n        train_label_dist = datasets['train']['y'].value_counts()\n        test_label_dist = datasets['test']['y'].value_counts()\n        \n        self.logger.info(f\"Training label distribution: {train_label_dist.to_dict()}\")\n        self.logger.info(f\"Test label distribution: {test_label_dist.to_dict()}\")\n        \n        return datasets\n    \n    def train_models(self, datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n        \"\"\"\n        Train ensemble of models with rigorous validation\n        \n        Args:\n            datasets: Training and test datasets\n            \n        Returns:\n            Trained models and validation results\n        \"\"\"\n        \n        self.logger.info(\"Starting model training\")\n        \n        # Train ensemble\n        self.trained_models = self.ensemble_trainer.train_ensemble(\n            X=datasets['train']['X'],\n            y=datasets['train']['y']\n        )\n        \n        # Comprehensive validation on training data (cross-validation)\n        self.logger.info(\"Performing comprehensive validation\")\n        \n        validation_results = {}\n        \n        for model_name, model in self.trained_models['models'].items():\n            self.logger.info(f\"Validating {model_name}\")\n            \n            model_results = self.validator.validate_model(\n                model=model,\n                X=datasets['train']['X'],\n                y=datasets['train']['y']\n            )\n            \n            validation_results[model_name] = model_results\n            \n            self.logger.info(f\"{model_name} - CV Score: {model_results['mean_cv_score']:.4f} Â± {model_results['std_cv_score']:.4f}\")\n            self.logger.info(f\"{model_name} - Overfitting Ratio: {model_results['overfitting_ratio']:.2f}\")\n        \n        # Test ensemble on out-of-sample data\n        self.logger.info(\"Evaluating ensemble on test set\")\n        \n        ensemble_evaluation = self.ensemble_trainer.evaluate_ensemble(\n            ensemble_predict_func=self.trained_models['ensemble_predict'],\n            X_test=datasets['test']['X'],\n            y_test=datasets['test']['y']\n        )\n        \n        validation_results['ensemble'] = ensemble_evaluation\n        \n        self.logger.info(f\"Ensemble Test AUC: {ensemble_evaluation['auc_score']:.4f}\")\n        self.logger.info(f\"Ensemble Test Accuracy: {ensemble_evaluation['accuracy']:.1%}\")\n        self.logger.info(f\"Ensemble Test Precision: {ensemble_evaluation['precision']:.1%}\")\n        \n        # Store validation results\n        self.validation_results = validation_results\n        \n        # Save trained models\n        models_file = self.output_dir / f\"trained_models_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib\"\n        self.ensemble_trainer.save_ensemble(str(models_file))\n        \n        self.logger.info(f\"Models saved to: {models_file}\")\n        \n        return {\n            'trained_models': self.trained_models,\n            'validation_results': validation_results,\n            'models_file': str(models_file)\n        }\n    \n    def generate_trading_signals(self, \n                                datasets: Dict[str, pd.DataFrame],\n                                confidence_threshold: float = None) -> pd.DataFrame:\n        \"\"\"\n        Generate trading signals for backtesting\n        \n        Args:\n            datasets: Training and test datasets\n            confidence_threshold: Minimum confidence for signals\n            \n        Returns:\n            DataFrame with trading signals\n        \"\"\"\n        \n        if self.trained_models is None:\n            raise ValueError(\"Models not trained. Call train_models() first.\")\n        \n        if confidence_threshold is None:\n            confidence_threshold = self.config['confidence_threshold']\n        \n        self.logger.info(f\"Generating trading signals (confidence >= {confidence_threshold})\")\n        \n        # Generate predictions on test set\n        test_predictions = self.trained_models['ensemble_predict'](datasets['test']['X'])\n        \n        # Create signals DataFrame\n        signals_df = pd.DataFrame({\n            'timestamp': datasets['test']['X'].index,\n            'symbol': 'BTCUSDT',  # Default symbol\n            'signal': test_predictions,\n            'confidence': test_predictions,  # Use prediction as confidence\n            'predicted_return': test_predictions * 0.015  # Scale to expected return\n        })\n        \n        # Filter by confidence threshold\n        signals_df = signals_df[signals_df['confidence'] >= confidence_threshold].copy()\n        \n        # Add additional signal metadata\n        signals_df['strategy'] = 'DipMaster'\n        signals_df['model_version'] = 'v1.0.0'\n        signals_df['signal_strength'] = pd.cut(\n            signals_df['confidence'], \n            bins=[0, 0.6, 0.8, 1.0], \n            labels=['WEAK', 'MODERATE', 'STRONG']\n        )\n        \n        self.logger.info(f\"Generated {len(signals_df)} trading signals\")\n        self.logger.info(f\"Signal strength distribution: {signals_df['signal_strength'].value_counts().to_dict()}\")\n        \n        return signals_df\n    \n    def run_backtest(self, \n                    signals_df: pd.DataFrame,\n                    market_data: pd.DataFrame = None,\n                    initial_capital: float = None) -> BacktestResult:\n        \"\"\"\n        Run comprehensive backtest\n        \n        Args:\n            signals_df: Trading signals\n            market_data: Market data for backtesting\n            initial_capital: Starting capital\n            \n        Returns:\n            BacktestResult object\n        \"\"\"\n        \n        if initial_capital is None:\n            initial_capital = self.config['initial_capital']\n        \n        if market_data is None:\n            # Use processed data for backtesting\n            market_data = self.processed_data[['open', 'high', 'low', 'close', 'volume', 'volatility_20']].copy()\n        \n        self.logger.info(\"Starting backtest simulation\")\n        self.logger.info(f\"Initial capital: ${initial_capital:,.2f}\")\n        \n        # Initialize backtester\n        backtester = RealisticBacktester(\n            initial_capital=initial_capital,\n            costs=self.trading_costs,\n            risk_limits=self.risk_limits\n        )\n        \n        # Run backtest\n        self.backtest_results = backtester.run_backtest(\n            signals_df=signals_df,\n            market_data=market_data\n        )\n        \n        # Log key results\n        self.logger.info(\"Backtest Results:\")\n        self.logger.info(f\"  Total Return: {self.backtest_results.total_return:.2%}\")\n        self.logger.info(f\"  Sharpe Ratio: {self.backtest_results.sharpe_ratio:.2f}\")\n        self.logger.info(f\"  Win Rate: {self.backtest_results.win_rate:.1%}\")\n        self.logger.info(f\"  Max Drawdown: {self.backtest_results.max_drawdown:.2%}\")\n        self.logger.info(f\"  Profit Factor: {self.backtest_results.profit_factor:.2f}\")\n        self.logger.info(f\"  Total Trades: {self.backtest_results.total_trades}\")\n        self.logger.info(f\"  Total Costs: ${self.backtest_results.total_fees + self.backtest_results.total_slippage + self.backtest_results.total_funding:.2f}\")\n        \n        # Generate HTML report\n        report_file = self.output_dir / f\"backtest_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html\"\n        backtester.generate_html_report(self.backtest_results, str(report_file))\n        \n        self.logger.info(f\"Backtest report saved to: {report_file}\")\n        \n        return self.backtest_results\n    \n    def save_alpha_signals(self, signals_df: pd.DataFrame, filename: str = None) -> str:\n        \"\"\"\n        Save alpha signals in standard format\n        \n        Args:\n            signals_df: Trading signals DataFrame\n            filename: Output filename\n            \n        Returns:\n            Path to saved file\n        \"\"\"\n        \n        if filename is None:\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            filename = f\"AlphaSignal_DipMasterV4_{timestamp}.json\"\n        \n        output_path = self.output_dir / filename\n        \n        # Calculate performance metrics for signal validation\n        if self.validation_results and 'ensemble' in self.validation_results:\n            ensemble_metrics = self.validation_results['ensemble']\n        else:\n            ensemble_metrics = {}\n        \n        # Prepare alpha signal format\n        alpha_signal = {\n            \"signal_uri\": str(output_path.with_suffix('.parquet')),\n            \"schema\": [\"timestamp\", \"symbol\", \"score\", \"confidence\", \"predicted_return\"],\n            \"model_version\": \"DipMaster_Enhanced_V4_1.0.0\",\n            \"retrain_policy\": \"weekly\",\n            \"feature_importance\": self.trained_models.get('feature_importance', {}) if self.trained_models else {},\n            \"validation_metrics\": ensemble_metrics,\n            \"generation_metadata\": {\n                \"generated_timestamp\": datetime.now().isoformat(),\n                \"total_signals\": len(signals_df),\n                \"confident_signals\": len(signals_df[signals_df['confidence'] >= 0.6]),\n                \"signal_period\": {\n                    \"start\": signals_df['timestamp'].min().isoformat(),\n                    \"end\": signals_df['timestamp'].max().isoformat()\n                },\n                \"configuration\": self.config,\n                \"model_performance\": {\n                    \"expected_win_rate\": \"85%+\",\n                    \"target_sharpe\": \"2.0+\",\n                    \"max_drawdown_limit\": \"5%\"\n                }\n            }\n        }\n        \n        # Add backtest results if available\n        if self.backtest_results:\n            alpha_signal[\"backtest_performance\"] = {\n                \"total_return\": self.backtest_results.total_return,\n                \"sharpe_ratio\": self.backtest_results.sharpe_ratio,\n                \"win_rate\": self.backtest_results.win_rate,\n                \"max_drawdown\": self.backtest_results.max_drawdown,\n                \"profit_factor\": self.backtest_results.profit_factor,\n                \"total_trades\": self.backtest_results.total_trades,\n                \"statistical_significance\": self.backtest_results.statistical_significance\n            }\n        \n        # Save JSON\n        with open(output_path, 'w') as f:\n            json.dump(alpha_signal, f, indent=2, default=str)\n        \n        # Save signals as parquet\n        signals_df.to_parquet(output_path.with_suffix('.parquet'))\n        \n        self.logger.info(f\"Alpha signals saved to {output_path}\")\n        \n        return str(output_path)\n    \n    def run_complete_pipeline(self, \n                             data_path: str,\n                             train_start: str = None,\n                             train_end: str = None,\n                             test_start: str = None,\n                             test_end: str = None) -> Dict[str, Any]:\n        \"\"\"\n        Run the complete ML pipeline from start to finish\n        \n        Args:\n            data_path: Path to market data file\n            train_start: Training start date (optional)\n            train_end: Training end date (optional)\n            test_start: Test start date (optional)\n            test_end: Test end date (optional)\n            \n        Returns:\n            Complete pipeline results\n        \"\"\"\n        \n        self.logger.info(\"=== Starting Complete DipMaster ML Pipeline ===\")\n        start_time = datetime.now()\n        \n        try:\n            # 1. Load market data\n            self.logger.info(\"Step 1: Loading market data\")\n            self.load_market_data(data_path)\n            \n            # 2. Feature engineering\n            self.logger.info(\"Step 2: Feature engineering\")\n            self.engineer_features()\n            \n            # 3. Prepare training data\n            self.logger.info(\"Step 3: Preparing training data\")\n            datasets = self.prepare_training_data(train_start, train_end, test_start, test_end)\n            \n            # 4. Train models\n            self.logger.info(\"Step 4: Training models\")\n            training_results = self.train_models(datasets)\n            \n            # 5. Generate signals\n            self.logger.info(\"Step 5: Generating trading signals\")\n            signals_df = self.generate_trading_signals(datasets)\n            \n            # 6. Run backtest\n            self.logger.info(\"Step 6: Running backtest\")\n            backtest_result = self.run_backtest(signals_df)\n            \n            # 7. Save results\n            self.logger.info(\"Step 7: Saving results\")\n            alpha_signals_path = self.save_alpha_signals(signals_df)\n            \n            # Performance assessment\n            performance_targets = {\n                'win_rate_target': 0.80,\n                'sharpe_target': 1.5,\n                'max_drawdown_limit': 0.05,\n                'profit_factor_target': 1.5\n            }\n            \n            target_achievement = {\n                'win_rate_achieved': backtest_result.win_rate >= performance_targets['win_rate_target'],\n                'sharpe_achieved': backtest_result.sharpe_ratio >= performance_targets['sharpe_target'],\n                'drawdown_ok': abs(backtest_result.max_drawdown) <= performance_targets['max_drawdown_limit'],\n                'profit_factor_achieved': backtest_result.profit_factor >= performance_targets['profit_factor_target']\n            }\n            \n            all_targets_met = all(target_achievement.values())\n            \n            # Final summary\n            end_time = datetime.now()\n            execution_time = (end_time - start_time).total_seconds()\n            \n            summary = {\n                'pipeline_status': 'COMPLETED',\n                'execution_time_seconds': execution_time,\n                'targets_achieved': all_targets_met,\n                'target_details': target_achievement,\n                'performance_metrics': {\n                    'total_return': backtest_result.total_return,\n                    'sharpe_ratio': backtest_result.sharpe_ratio,\n                    'win_rate': backtest_result.win_rate,\n                    'max_drawdown': backtest_result.max_drawdown,\n                    'profit_factor': backtest_result.profit_factor,\n                    'total_trades': backtest_result.total_trades,\n                    'statistical_significance': backtest_result.statistical_significance.get('is_significant', False)\n                },\n                'model_performance': {\n                    name: {\n                        'cv_score': results.get('mean_cv_score', 0),\n                        'overfitting_ratio': results.get('overfitting_ratio', 0)\n                    }\n                    for name, results in self.validation_results.items() if name != 'ensemble'\n                },\n                'cost_breakdown': {\n                    'total_fees': backtest_result.total_fees,\n                    'total_slippage': backtest_result.total_slippage,\n                    'total_funding': backtest_result.total_funding\n                },\n                'data_statistics': {\n                    'total_samples': len(self.processed_data),\n                    'training_samples': len(datasets['train']['X']),\n                    'test_samples': len(datasets['test']['X']),\n                    'features_engineered': self.processed_data.shape[1],\n                    'signals_generated': len(signals_df)\n                },\n                'output_files': {\n                    'alpha_signals': alpha_signals_path,\n                    'trained_models': training_results['models_file']\n                }\n            }\n            \n            # Save summary\n            summary_path = self.output_dir / f\"pipeline_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n            with open(summary_path, 'w') as f:\n                json.dump(summary, f, indent=2, default=str)\n            \n            self.logger.info(\"=== Pipeline Completed Successfully ===\")\n            self.logger.info(f\"Execution time: {execution_time:.1f} seconds\")\n            self.logger.info(f\"All targets achieved: {all_targets_met}\")\n            self.logger.info(f\"Summary saved to: {summary_path}\")\n            \n            return summary\n            \n        except Exception as e:\n            self.logger.error(f\"Pipeline failed with error: {str(e)}\")\n            self.logger.error(f\"Error type: {type(e).__name__}\")\n            \n            # Save error summary\n            error_summary = {\n                'pipeline_status': 'FAILED',\n                'error_message': str(e),\n                'error_type': type(e).__name__,\n                'execution_time_seconds': (datetime.now() - start_time).total_seconds(),\n                'timestamp': datetime.now().isoformat()\n            }\n            \n            error_path = self.output_dir / f\"pipeline_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n            with open(error_path, 'w') as f:\n                json.dump(error_summary, f, indent=2, default=str)\n            \n            raise\n\n\ndef main():\n    \"\"\"Main execution function for testing\"\"\"\n    \n    # Configuration\n    config = {\n        'cv_splits': 3,  # Reduced for faster execution\n        'optimization_trials': 20,  # Reduced for faster execution\n        'confidence_threshold': 0.6,\n        'initial_capital': 10000.0,\n        'models': ['lightgbm', 'xgboost']  # Reduced for faster execution\n    }\n    \n    # Initialize pipeline\n    pipeline = CompleteDipMasterMLPipeline(\n        output_dir=\"results/ml_pipeline\",\n        config=config\n    )\n    \n    # Sample data path (replace with actual path)\n    sample_data_path = \"data/features_BTCUSDT_sample.parquet\"\n    \n    print(\"DipMaster Complete ML Pipeline\")\n    print(\"==============================\")\n    print(f\"Configuration: {config}\")\n    print(f\"Data path: {sample_data_path}\")\n    \n    # Run pipeline (commented out as we don't have actual data)\n    # results = pipeline.run_complete_pipeline(sample_data_path)\n    # \n    # print(\"\\n=== FINAL RESULTS ===\")\n    # print(f\"Targets Achieved: {results['targets_achieved']}\")\n    # print(f\"Win Rate: {results['performance_metrics']['win_rate']:.1%}\")\n    # print(f\"Sharpe Ratio: {results['performance_metrics']['sharpe_ratio']:.2f}\")\n    # print(f\"Max Drawdown: {results['performance_metrics']['max_drawdown']:.2%}\")\n    # print(f\"Total Trades: {results['performance_metrics']['total_trades']}\")\n    \n    print(\"\\nPipeline initialized successfully!\")\n    print(\"To run with actual data, uncomment the execution lines in main()\")\n    \n    return pipeline\n\n\nif __name__ == \"__main__\":\n    pipeline = main()