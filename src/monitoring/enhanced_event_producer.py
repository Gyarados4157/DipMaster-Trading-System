#!/usr/bin/env python3
"""
Enhanced Event Producer for DipMaster Trading System
Â¢ûÂº∫Âûã‰∫ã‰ª∂Áîü‰∫ßËÄÖ - ‰∏ì‰∏öÁ∫ß‰∫§ÊòìÁ≥ªÁªü‰∫ã‰ª∂ÊµÅÁÆ°ÁêÜ

Features:
- Comprehensive event schema validation
- High-performance batch publishing
- Advanced retry mechanisms with exponential backoff
- Event correlation and traceability
- Metrics and monitoring integration
- Dead letter queue with intelligent retry
"""

import asyncio
import json
import time
import logging
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass, asdict, field
from enum import Enum
import uuid
from datetime import datetime, timezone, timedelta
import hashlib
import threading
from collections import defaultdict, deque

from .kafka_event_producer import (
    KafkaEventProducer, EventType, ExecutionReportEvent, 
    RiskMetricsEvent, AlertEvent, StrategyPerformanceEvent
)

logger = logging.getLogger(__name__)


@dataclass
class SystemHealthEvent:
    """System health event schema."""
    timestamp: str
    component: str
    health_score: float
    status: str  # "healthy", "degraded", "unhealthy"
    cpu_usage: float
    memory_usage: float
    disk_usage: float
    network_latency: float
    active_connections: int
    error_rate: float
    uptime_seconds: float
    details: Dict[str, Any] = field(default_factory=dict)


@dataclass
class TradeSignalEvent:
    """Trade signal event schema."""
    timestamp: str
    signal_id: str
    strategy: str
    symbol: str
    signal_type: str  # "BUY", "SELL", "HOLD"
    confidence: float
    price: float
    technical_indicators: Dict[str, float]
    market_conditions: Dict[str, Any]
    expected_entry: Optional[float] = None
    expected_exit: Optional[float] = None
    risk_score: Optional[float] = None
    correlation_id: Optional[str] = None


@dataclass
class PositionUpdateEvent:
    """Position update event schema."""
    timestamp: str
    position_id: str
    symbol: str
    side: str
    quantity: float
    entry_price: float
    current_price: float
    unrealized_pnl: float
    realized_pnl: float
    holding_time_minutes: int
    status: str  # "open", "closed", "partial"
    strategy: str
    risk_metrics: Dict[str, float]
    correlation_id: Optional[str] = None


@dataclass
class MarketDataEvent:
    """Market data event schema."""
    timestamp: str
    symbol: str
    price: float
    volume: float
    bid: float
    ask: float
    spread_bps: float
    volatility: float
    technical_indicators: Dict[str, float]
    market_depth: Dict[str, Any]
    quality_score: float
    source: str


class EventPriority(Enum):
    """Event priority levels."""
    LOW = 1
    NORMAL = 2
    HIGH = 3
    CRITICAL = 4
    EMERGENCY = 5


@dataclass
class EventMetadata:
    """Event metadata for traceability."""
    event_id: str
    correlation_id: str
    trace_id: str
    priority: EventPriority
    created_at: datetime
    source_component: str
    tags: Dict[str, str] = field(default_factory=dict)
    retry_count: int = 0
    max_retries: int = 3


class EventBuffer:
    """High-performance event buffer with batching support."""
    
    def __init__(self, max_size: int = 1000, flush_interval: float = 1.0):
        self.max_size = max_size
        self.flush_interval = flush_interval
        self.buffer = deque()
        self.last_flush = time.time()
        self.lock = threading.Lock()
        self.total_events = 0
        self.total_bytes = 0
    
    def add_event(self, topic: str, event_data: Dict[str, Any], metadata: EventMetadata) -> bool:
        """Add event to buffer."""
        with self.lock:
            if len(self.buffer) >= self.max_size:
                return False  # Buffer full
            
            buffered_event = {
                'topic': topic,
                'event_data': event_data,
                'metadata': metadata,
                'buffered_at': time.time()
            }
            
            self.buffer.append(buffered_event)
            self.total_events += 1
            self.total_bytes += len(json.dumps(event_data))
            
            return True
    
    def should_flush(self) -> bool:
        """Check if buffer should be flushed."""
        with self.lock:
            if not self.buffer:
                return False
            
            # Flush if buffer is full or time interval exceeded
            return (len(self.buffer) >= self.max_size or 
                   time.time() - self.last_flush >= self.flush_interval)
    
    def get_batch(self, max_batch_size: int = 100) -> List[Dict[str, Any]]:
        """Get batch of events for publishing."""
        with self.lock:
            batch_size = min(len(self.buffer), max_batch_size)
            batch = []
            
            for _ in range(batch_size):
                if self.buffer:
                    batch.append(self.buffer.popleft())
            
            if batch:
                self.last_flush = time.time()
            
            return batch
    
    def get_stats(self) -> Dict[str, Any]:
        """Get buffer statistics."""
        with self.lock:
            return {
                'current_size': len(self.buffer),
                'max_size': self.max_size,
                'total_events': self.total_events,
                'total_bytes': self.total_bytes,
                'utilization': len(self.buffer) / self.max_size * 100
            }


class EnhancedEventProducer:
    """
    Enhanced event producer with advanced features for trading system monitoring.
    
    Provides high-performance, reliable event publishing with comprehensive
    monitoring, tracing, and recovery capabilities.
    """
    
    def __init__(self,
                 kafka_producer: KafkaEventProducer,
                 config: Optional[Dict[str, Any]] = None):
        """
        Initialize enhanced event producer.
        
        Args:
            kafka_producer: Base Kafka producer instance
            config: Configuration parameters
        """
        self.kafka_producer = kafka_producer
        self.config = config or {}
        
        # Event buffering
        self.event_buffer = EventBuffer(
            max_size=self.config.get('buffer_max_size', 1000),
            flush_interval=self.config.get('buffer_flush_interval', 1.0)
        )
        
        # Event correlation and tracing
        self.correlation_cache = {}
        self.trace_cache = {}
        
        # Performance tracking
        self.event_stats = defaultdict(int)
        self.latency_stats = defaultdict(list)
        self.error_stats = defaultdict(int)
        
        # Background processing
        self.processing_active = False
        self.background_task = None
        
        # Event schemas for validation
        self.event_schemas = {
            EventType.EXECUTION_REPORT.value: ExecutionReportEvent,
            EventType.RISK_METRICS.value: RiskMetricsEvent,
            EventType.ALERTS.value: AlertEvent,
            EventType.STRATEGY_PERFORMANCE.value: StrategyPerformanceEvent,
            EventType.SYSTEM_HEALTH.value: SystemHealthEvent,
            EventType.TRADE_SIGNAL.value: TradeSignalEvent,
            EventType.POSITION_UPDATE.value: PositionUpdateEvent,
            EventType.MARKET_DATA.value: MarketDataEvent
        }
        
        logger.info("üöÄ EnhancedEventProducer initialized")
    
    async def start_processing(self):
        """Start background event processing."""
        if self.processing_active:
            logger.warning("‚ö†Ô∏è Event processing already active")
            return
        
        self.processing_active = True
        self.background_task = asyncio.create_task(self._background_processor())
        logger.info("‚úÖ Started background event processing")
    
    async def stop_processing(self):
        """Stop background event processing."""
        if not self.processing_active:
            return
        
        self.processing_active = False
        if self.background_task:
            self.background_task.cancel()
            try:
                await self.background_task
            except asyncio.CancelledError:
                pass
        
        # Flush remaining events
        await self._flush_buffer()
        logger.info("üõë Stopped background event processing")
    
    async def _background_processor(self):
        """Background task for processing buffered events."""
        while self.processing_active:
            try:
                if self.event_buffer.should_flush():
                    await self._flush_buffer()
                
                await asyncio.sleep(0.1)  # 100ms processing interval
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"‚ùå Error in background processor: {e}")
                await asyncio.sleep(1)  # Error backoff
    
    async def _flush_buffer(self):
        """Flush buffered events to Kafka."""
        batch = self.event_buffer.get_batch()
        if not batch:
            return
        
        # Group events by topic for efficient publishing
        topic_batches = defaultdict(list)
        for event in batch:
            topic_batches[event['topic']].append(event)
        
        # Publish batches
        for topic, events in topic_batches.items():
            await self._publish_batch(topic, events)
    
    async def _publish_batch(self, topic: str, events: List[Dict[str, Any]]):
        """Publish batch of events to specific topic."""
        for event in events:
            try:
                start_time = time.time()
                
                success = await self.kafka_producer.publish_event(
                    topic=event['topic'],
                    event_data=event['event_data'],
                    key=event['metadata'].correlation_id,
                    headers={
                        'event-id': event['metadata'].event_id,
                        'correlation-id': event['metadata'].correlation_id,
                        'trace-id': event['metadata'].trace_id,
                        'priority': event['metadata'].priority.name,
                        'source': event['metadata'].source_component
                    }
                )
                
                # Record metrics
                latency = (time.time() - start_time) * 1000  # ms
                self.latency_stats[topic].append(latency)
                
                if success:
                    self.event_stats[f"{topic}_success"] += 1
                else:
                    self.event_stats[f"{topic}_failed"] += 1
                    await self._handle_failed_event(event)
                
            except Exception as e:
                logger.error(f"‚ùå Failed to publish event to {topic}: {e}")
                self.error_stats[topic] += 1
                await self._handle_failed_event(event)
    
    async def _handle_failed_event(self, event: Dict[str, Any]):
        """Handle failed event with retry logic."""
        metadata = event['metadata']
        metadata.retry_count += 1
        
        if metadata.retry_count <= metadata.max_retries:
            # Add back to buffer for retry with exponential backoff
            await asyncio.sleep(2 ** metadata.retry_count)  # Exponential backoff
            self.event_buffer.add_event(event['topic'], event['event_data'], metadata)
        else:
            logger.error(f"‚ùå Event {metadata.event_id} exceeded max retries, dropping")
    
    async def publish_execution_report(self,
                                     symbol: str,
                                     execution_id: str,
                                     signal_id: str,
                                     side: str,
                                     quantity: float,
                                     price: float,
                                     slippage_bps: float,
                                     latency_ms: float,
                                     venue: str,
                                     status: str,
                                     **kwargs) -> bool:
        """Publish execution report event."""
        
        event = ExecutionReportEvent(
            timestamp=datetime.now(timezone.utc).isoformat(),
            execution_id=execution_id,
            signal_id=signal_id,
            symbol=symbol,
            side=side,
            quantity=quantity,
            price=price,
            slippage_bps=slippage_bps,
            latency_ms=latency_ms,
            venue=venue,
            status=status,
            **kwargs
        )
        
        return await self._publish_event(
            topic=EventType.EXECUTION_REPORT.value,
            event_data=asdict(event),
            correlation_id=signal_id,
            priority=EventPriority.HIGH,
            source_component="order_executor"
        )
    
    async def publish_risk_metrics(self,
                                 portfolio_id: str,
                                 var_95: float,
                                 var_99: float,
                                 expected_shortfall: float,
                                 sharpe_ratio: float,
                                 max_drawdown: float,
                                 leverage: float,
                                 correlation_stability: float,
                                 active_positions: int,
                                 total_exposure: float,
                                 unrealized_pnl: float,
                                 **kwargs) -> bool:
        """Publish risk metrics event."""
        
        event = RiskMetricsEvent(
            timestamp=datetime.now(timezone.utc).isoformat(),
            portfolio_id=portfolio_id,
            var_95=var_95,
            var_99=var_99,
            expected_shortfall=expected_shortfall,
            sharpe_ratio=sharpe_ratio,
            max_drawdown=max_drawdown,
            leverage=leverage,
            correlation_stability=correlation_stability,
            active_positions=active_positions,
            total_exposure=total_exposure,
            unrealized_pnl=unrealized_pnl,
            **kwargs
        )
        
        priority = EventPriority.CRITICAL if max_drawdown > 15.0 else EventPriority.HIGH
        
        return await self._publish_event(
            topic=EventType.RISK_METRICS.value,
            event_data=asdict(event),
            correlation_id=portfolio_id,
            priority=priority,
            source_component="risk_manager"
        )
    
    async def publish_alert(self,
                          alert_id: str,
                          severity: str,
                          category: str,
                          message: str,
                          affected_systems: List[str],
                          recommended_action: str,
                          auto_remediation: bool = False,
                          source: str = "monitoring_system",
                          **kwargs) -> bool:
        """Publish alert event."""
        
        event = AlertEvent(
            timestamp=datetime.now(timezone.utc).isoformat(),
            alert_id=alert_id,
            severity=severity,
            category=category,
            message=message,
            affected_systems=affected_systems,
            recommended_action=recommended_action,
            auto_remediation=auto_remediation,
            source=source,
            **kwargs
        )
        
        # Alert priority based on severity
        priority_map = {
            "INFO": EventPriority.LOW,
            "WARNING": EventPriority.NORMAL,
            "ERROR": EventPriority.HIGH,
            "CRITICAL": EventPriority.CRITICAL,
            "EMERGENCY": EventPriority.EMERGENCY
        }
        priority = priority_map.get(severity.upper(), EventPriority.NORMAL)
        
        return await self._publish_event(
            topic=EventType.ALERTS.value,
            event_data=asdict(event),
            correlation_id=alert_id,
            priority=priority,
            source_component=source
        )
    
    async def publish_strategy_performance(self,
                                         strategy_name: str,
                                         timeframe: str,
                                         win_rate: float,
                                         total_trades: int,
                                         winning_trades: int,
                                         losing_trades: int,
                                         avg_win: float,
                                         avg_loss: float,
                                         profit_factor: float,
                                         sharpe_ratio: float,
                                         max_drawdown: float,
                                         total_pnl: float,
                                         active_positions: int,
                                         signals_generated: int,
                                         signals_executed: int,
                                         execution_rate: float) -> bool:
        """Publish strategy performance event."""
        
        event = StrategyPerformanceEvent(
            timestamp=datetime.now(timezone.utc).isoformat(),
            strategy_name=strategy_name,
            timeframe=timeframe,
            win_rate=win_rate,
            total_trades=total_trades,
            winning_trades=winning_trades,
            losing_trades=losing_trades,
            avg_win=avg_win,
            avg_loss=avg_loss,
            profit_factor=profit_factor,
            sharpe_ratio=sharpe_ratio,
            max_drawdown=max_drawdown,
            total_pnl=total_pnl,
            active_positions=active_positions,
            signals_generated=signals_generated,
            signals_executed=signals_executed,
            execution_rate=execution_rate
        )
        
        return await self._publish_event(
            topic=EventType.STRATEGY_PERFORMANCE.value,
            event_data=asdict(event),
            correlation_id=strategy_name,
            priority=EventPriority.NORMAL,
            source_component="strategy_engine"
        )
    
    async def publish_system_health(self,
                                  component: str,
                                  health_score: float,
                                  status: str,
                                  cpu_usage: float,
                                  memory_usage: float,
                                  disk_usage: float,
                                  network_latency: float,
                                  active_connections: int,
                                  error_rate: float,
                                  uptime_seconds: float,
                                  details: Optional[Dict[str, Any]] = None) -> bool:
        """Publish system health event."""
        
        event = SystemHealthEvent(
            timestamp=datetime.now(timezone.utc).isoformat(),
            component=component,
            health_score=health_score,
            status=status,
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            disk_usage=disk_usage,
            network_latency=network_latency,
            active_connections=active_connections,
            error_rate=error_rate,
            uptime_seconds=uptime_seconds,
            details=details or {}
        )
        
        priority = EventPriority.CRITICAL if health_score < 50 else EventPriority.NORMAL
        
        return await self._publish_event(
            topic=EventType.SYSTEM_HEALTH.value,
            event_data=asdict(event),
            correlation_id=component,
            priority=priority,
            source_component="system_monitor"
        )
    
    async def publish_trade_signal(self,
                                 signal_id: str,
                                 strategy: str,
                                 symbol: str,
                                 signal_type: str,
                                 confidence: float,
                                 price: float,
                                 technical_indicators: Dict[str, float],
                                 market_conditions: Dict[str, Any],
                                 **kwargs) -> bool:
        """Publish trade signal event."""
        
        event = TradeSignalEvent(
            timestamp=datetime.now(timezone.utc).isoformat(),
            signal_id=signal_id,
            strategy=strategy,
            symbol=symbol,
            signal_type=signal_type,
            confidence=confidence,
            price=price,
            technical_indicators=technical_indicators,
            market_conditions=market_conditions,
            **kwargs
        )
        
        return await self._publish_event(
            topic=EventType.TRADE_SIGNAL.value,
            event_data=asdict(event),
            correlation_id=signal_id,
            priority=EventPriority.HIGH,
            source_component="signal_detector"
        )
    
    async def publish_position_update(self,
                                    position_id: str,
                                    symbol: str,
                                    side: str,
                                    quantity: float,
                                    entry_price: float,
                                    current_price: float,
                                    unrealized_pnl: float,
                                    realized_pnl: float,
                                    holding_time_minutes: int,
                                    status: str,
                                    strategy: str,
                                    risk_metrics: Dict[str, float],
                                    **kwargs) -> bool:
        """Publish position update event."""
        
        event = PositionUpdateEvent(
            timestamp=datetime.now(timezone.utc).isoformat(),
            position_id=position_id,
            symbol=symbol,
            side=side,
            quantity=quantity,
            entry_price=entry_price,
            current_price=current_price,
            unrealized_pnl=unrealized_pnl,
            realized_pnl=realized_pnl,
            holding_time_minutes=holding_time_minutes,
            status=status,
            strategy=strategy,
            risk_metrics=risk_metrics,
            **kwargs
        )
        
        priority = EventPriority.HIGH if status == "closed" else EventPriority.NORMAL
        
        return await self._publish_event(
            topic=EventType.POSITION_UPDATE.value,
            event_data=asdict(event),
            correlation_id=position_id,
            priority=priority,
            source_component="position_manager"
        )
    
    async def _publish_event(self,
                           topic: str,
                           event_data: Dict[str, Any],
                           correlation_id: str,
                           priority: EventPriority,
                           source_component: str,
                           tags: Optional[Dict[str, str]] = None) -> bool:
        """Internal method to publish event with metadata."""
        
        # Generate metadata
        metadata = EventMetadata(
            event_id=str(uuid.uuid4()),
            correlation_id=correlation_id,
            trace_id=self._get_or_create_trace_id(correlation_id),
            priority=priority,
            created_at=datetime.now(timezone.utc),
            source_component=source_component,
            tags=tags or {}
        )
        
        # Validate event schema if available
        if topic in self.event_schemas:
            try:
                # Schema validation would go here
                pass
            except Exception as e:
                logger.error(f"‚ùå Event schema validation failed for {topic}: {e}")
                return False
        
        # Add to buffer or publish immediately based on priority
        if priority in [EventPriority.CRITICAL, EventPriority.EMERGENCY]:
            # High-priority events bypass buffer
            return await self._publish_immediately(topic, event_data, metadata)
        else:
            # Buffer for batch processing
            return self.event_buffer.add_event(topic, event_data, metadata)
    
    async def _publish_immediately(self,
                                 topic: str,
                                 event_data: Dict[str, Any],
                                 metadata: EventMetadata) -> bool:
        """Publish event immediately for high-priority events."""
        try:
            return await self.kafka_producer.publish_event(
                topic=topic,
                event_data=event_data,
                key=metadata.correlation_id,
                headers={
                    'event-id': metadata.event_id,
                    'correlation-id': metadata.correlation_id,
                    'trace-id': metadata.trace_id,
                    'priority': metadata.priority.name,
                    'source': metadata.source_component
                }
            )
        except Exception as e:
            logger.error(f"‚ùå Failed to publish high-priority event: {e}")
            return False
    
    def _get_or_create_trace_id(self, correlation_id: str) -> str:
        """Get or create trace ID for correlation."""
        if correlation_id not in self.trace_cache:
            # Create trace ID based on correlation ID
            trace_id = hashlib.md5(f"trace_{correlation_id}_{time.time()}".encode()).hexdigest()[:16]
            self.trace_cache[correlation_id] = trace_id
            
            # Limit cache size
            if len(self.trace_cache) > 10000:
                # Remove oldest entries
                oldest_keys = list(self.trace_cache.keys())[:1000]
                for key in oldest_keys:
                    del self.trace_cache[key]
        
        return self.trace_cache[correlation_id]
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get performance statistics."""
        # Calculate latency percentiles
        latency_percentiles = {}
        for topic, latencies in self.latency_stats.items():
            if latencies:
                latencies_sorted = sorted(latencies[-1000:])  # Last 1000 events
                latency_percentiles[topic] = {
                    'p50': latencies_sorted[len(latencies_sorted) // 2],
                    'p95': latencies_sorted[int(len(latencies_sorted) * 0.95)],
                    'p99': latencies_sorted[int(len(latencies_sorted) * 0.99)]
                }
        
        return {
            'event_stats': dict(self.event_stats),
            'error_stats': dict(self.error_stats),
            'latency_percentiles': latency_percentiles,
            'buffer_stats': self.event_buffer.get_stats(),
            'kafka_stats': self.kafka_producer.get_producer_stats() if self.kafka_producer else {}
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """Perform health check."""
        try:
            kafka_health = await self.kafka_producer.health_check() if self.kafka_producer else {'status': 'unknown'}
            buffer_stats = self.event_buffer.get_stats()
            
            # Determine overall health
            if kafka_health['status'] == 'healthy' and buffer_stats['utilization'] < 80:
                status = 'healthy'
            elif kafka_health['status'] in ['healthy', 'degraded'] and buffer_stats['utilization'] < 95:
                status = 'degraded'
            else:
                status = 'unhealthy'
            
            return {
                'status': status,
                'kafka_health': kafka_health,
                'buffer_utilization': buffer_stats['utilization'],
                'processing_active': self.processing_active,
                'total_events_processed': sum(self.event_stats.values()),
                'total_errors': sum(self.error_stats.values())
            }
            
        except Exception as e:
            return {'status': 'error', 'error': str(e)}


# Factory function
def create_enhanced_event_producer(kafka_producer: KafkaEventProducer,
                                 config: Optional[Dict[str, Any]] = None) -> EnhancedEventProducer:
    """Create and configure enhanced event producer."""
    return EnhancedEventProducer(kafka_producer, config)