#!/usr/bin/env python3
"""
DipMaster Trading System - Kafka Event Schemas and Event Stream Producer
Kafkaäº‹ä»¶æµ - äº‹ä»¶Schemaå®šä¹‰å’Œå®æ—¶äº‹ä»¶æµç”Ÿäº§ç³»ç»Ÿ

Author: DipMaster Trading System Monitoring Agent
Date: 2025-08-18
Version: 1.0.0
"""

import asyncio
import json
import time
import logging
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, field, asdict
from datetime import datetime, timezone
from enum import Enum
from abc import ABC, abstractmethod
import uuid

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class EventVersion(Enum):
    """äº‹ä»¶ç‰ˆæœ¬æšä¸¾"""
    V1 = "v1"
    V2 = "v2"


class KafkaTopics(Enum):
    """Kafkaä¸»é¢˜æšä¸¾"""
    EXECUTION_REPORTS = "exec.reports.v1"
    RISK_METRICS = "risk.metrics.v1"
    ALERTS = "alerts.v1"
    STRATEGY_PERFORMANCE = "strategy.performance.v1"
    SYSTEM_HEALTH = "system.health.v1"
    TRADE_SIGNALS = "trade.signals.v1"
    POSITION_UPDATES = "position.updates.v1"
    MARKET_DATA_QUALITY = "market.data.quality.v1"


@dataclass
class BaseKafkaEvent:
    """Kafkaäº‹ä»¶åŸºç±»"""
    event_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    version: str = EventVersion.V1.value
    source: str = "dipmaster-monitoring"
    
    def to_kafka_message(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºKafkaæ¶ˆæ¯æ ¼å¼"""
        data = asdict(self)
        # è½¬æ¢datetimeä¸ºISOå­—ç¬¦ä¸²
        for key, value in data.items():
            if isinstance(value, datetime):
                data[key] = value.isoformat()
        return data


@dataclass
class ExecutionReportEvent(BaseKafkaEvent):
    """äº¤æ˜“æ‰§è¡ŒæŠ¥å‘Šäº‹ä»¶ - Topic: exec.reports.v1"""
    execution_id: str = ""
    signal_id: str = ""
    symbol: str = ""
    side: str = ""  # BUY, SELL
    quantity: float = 0.0
    price: float = 0.0
    slippage_bps: float = 0.0
    latency_ms: float = 0.0
    venue: str = "binance"
    status: str = "FILLED"  # FILLED, PARTIAL, REJECTED, CANCELLED
    strategy: str = "dipmaster"
    order_type: str = "MARKET"
    commission: float = 0.0
    commission_asset: str = "USDT"
    
    def to_kafka_message(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºKafkaæ¶ˆæ¯æ ¼å¼"""
        message = super().to_kafka_message()
        message.update({
            'topic': KafkaTopics.EXECUTION_REPORTS.value,
            'key': f"{self.symbol}_{self.execution_id}",
            'execution_quality': {
                'slippage_bps': self.slippage_bps,
                'latency_ms': self.latency_ms,
                'venue_quality_score': self._calculate_venue_quality_score()
            }
        })
        return message
    
    def _calculate_venue_quality_score(self) -> float:
        """è®¡ç®—äº¤æ˜“æ‰€è´¨é‡è¯„åˆ†"""
        # åŸºäºæ»‘ç‚¹å’Œå»¶è¿Ÿè®¡ç®—è´¨é‡è¯„åˆ†
        slippage_penalty = min(self.slippage_bps / 10.0, 50.0)  # æœ€å¤§æ‰£50åˆ†
        latency_penalty = min(self.latency_ms / 100.0, 30.0)    # æœ€å¤§æ‰£30åˆ†
        return max(0.0, 100.0 - slippage_penalty - latency_penalty)


@dataclass
class RiskMetricsEvent(BaseKafkaEvent):
    """é£é™©æŒ‡æ ‡äº‹ä»¶ - Topic: risk.metrics.v1"""
    portfolio_id: str = "main_portfolio"
    var_95: float = 0.0
    var_99: float = 0.0
    expected_shortfall: float = 0.0
    sharpe_ratio: float = 0.0
    max_drawdown: float = 0.0
    current_drawdown: float = 0.0
    leverage: float = 0.0
    correlation_stability: float = 0.0
    portfolio_value: float = 0.0
    daily_pnl: float = 0.0
    position_count: int = 0
    risk_utilization: float = 0.0
    
    def to_kafka_message(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºKafkaæ¶ˆæ¯æ ¼å¼"""
        message = super().to_kafka_message()
        message.update({
            'topic': KafkaTopics.RISK_METRICS.value,
            'key': f"{self.portfolio_id}_{int(self.timestamp.timestamp())}",
            'risk_assessment': {
                'risk_score': self._calculate_risk_score(),
                'risk_level': self._get_risk_level(),
                'breach_alerts': self._check_risk_breaches()
            }
        })
        return message
    
    def _calculate_risk_score(self) -> float:
        """è®¡ç®—æ•´ä½“é£é™©è¯„åˆ† (0-100)"""
        # åŸºäºå¤šä¸ªé£é™©æŒ‡æ ‡çš„ç»¼åˆè¯„åˆ†
        var_score = min(self.var_95 / 100000.0 * 50, 50)  # VaRè¯„åˆ†ï¼Œæœ€å¤§50åˆ†
        drawdown_score = min(self.current_drawdown * 100, 30)  # å›æ’¤è¯„åˆ†ï¼Œæœ€å¤§30åˆ†
        leverage_score = min(self.leverage / 3.0 * 20, 20)     # æ æ†è¯„åˆ†ï¼Œæœ€å¤§20åˆ†
        
        return min(100.0, var_score + drawdown_score + leverage_score)
    
    def _get_risk_level(self) -> str:
        """è·å–é£é™©ç­‰çº§"""
        risk_score = self._calculate_risk_score()
        if risk_score < 30:
            return "LOW"
        elif risk_score < 60:
            return "MEDIUM"
        elif risk_score < 80:
            return "HIGH"
        else:
            return "CRITICAL"
    
    def _check_risk_breaches(self) -> List[str]:
        """æ£€æŸ¥é£é™©é™åˆ¶è¿è§„"""
        breaches = []
        
        # é¢„è®¾çš„é£é™©é™åˆ¶
        if self.var_95 > 200000:
            breaches.append("VAR_95_EXCEEDED")
        if self.var_99 > 300000:
            breaches.append("VAR_99_EXCEEDED")
        if self.current_drawdown > 0.20:
            breaches.append("MAX_DRAWDOWN_EXCEEDED")
        if self.leverage > 3.0:
            breaches.append("MAX_LEVERAGE_EXCEEDED")
        
        return breaches


@dataclass
class AlertEvent(BaseKafkaEvent):
    """å‘Šè­¦äº‹ä»¶ - Topic: alerts.v1"""
    alert_id: str = ""
    severity: str = "INFO"  # INFO, WARNING, CRITICAL, EMERGENCY
    category: str = ""
    message: str = ""
    affected_systems: List[str] = field(default_factory=list)
    recommended_action: str = ""
    auto_remediation: bool = False
    tags: Dict[str, str] = field(default_factory=dict)
    details: Dict[str, Any] = field(default_factory=dict)
    resolved: bool = False
    resolution_time: Optional[datetime] = None
    
    def to_kafka_message(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºKafkaæ¶ˆæ¯æ ¼å¼"""
        message = super().to_kafka_message()
        message.update({
            'topic': KafkaTopics.ALERTS.value,
            'key': self.alert_id,
            'alert_metadata': {
                'priority_score': self._calculate_priority_score(),
                'escalation_required': self._should_escalate(),
                'similar_alerts_count': 0  # éœ€è¦ä»å†å²æ•°æ®è®¡ç®—
            }
        })
        return message
    
    def _calculate_priority_score(self) -> int:
        """è®¡ç®—å‘Šè­¦ä¼˜å…ˆçº§è¯„åˆ† (1-10)"""
        severity_scores = {
            "INFO": 1,
            "WARNING": 4,
            "CRITICAL": 8,
            "EMERGENCY": 10
        }
        return severity_scores.get(self.severity, 1)
    
    def _should_escalate(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å‡çº§"""
        return self.severity in ["CRITICAL", "EMERGENCY"] and not self.resolved


@dataclass
class StrategyPerformanceEvent(BaseKafkaEvent):
    """ç­–ç•¥æ€§èƒ½äº‹ä»¶ - Topic: strategy.performance.v1"""
    strategy_name: str = "dipmaster"
    time_window: str = "1h"  # 1h, 4h, 1d, 1w
    win_rate: float = 0.0
    profit_factor: float = 0.0
    sharpe_ratio: float = 0.0
    max_drawdown: float = 0.0
    total_trades: int = 0
    winning_trades: int = 0
    losing_trades: int = 0
    total_pnl: float = 0.0
    avg_win: float = 0.0
    avg_loss: float = 0.0
    avg_holding_time_minutes: float = 0.0
    best_trade: float = 0.0
    worst_trade: float = 0.0
    
    def to_kafka_message(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºKafkaæ¶ˆæ¯æ ¼å¼"""
        message = super().to_kafka_message()
        message.update({
            'topic': KafkaTopics.STRATEGY_PERFORMANCE.value,
            'key': f"{self.strategy_name}_{self.time_window}_{int(self.timestamp.timestamp())}",
            'performance_assessment': {
                'performance_score': self._calculate_performance_score(),
                'performance_grade': self._get_performance_grade(),
                'benchmark_comparison': self._compare_to_benchmark()
            }
        })
        return message
    
    def _calculate_performance_score(self) -> float:
        """è®¡ç®—æ€§èƒ½è¯„åˆ† (0-100)"""
        # åŸºäºå¤šä¸ªæ€§èƒ½æŒ‡æ ‡çš„ç»¼åˆè¯„åˆ†
        win_rate_score = min(self.win_rate * 100, 40)  # èƒœç‡è¯„åˆ†ï¼Œæœ€å¤§40åˆ†
        sharpe_score = min(max(self.sharpe_ratio, 0) * 20, 30)  # å¤æ™®æ¯”ç‡è¯„åˆ†ï¼Œæœ€å¤§30åˆ†
        profit_factor_score = min(max(self.profit_factor - 1, 0) * 30, 30)  # ç›ˆäºæ¯”è¯„åˆ†ï¼Œæœ€å¤§30åˆ†
        
        return win_rate_score + sharpe_score + profit_factor_score
    
    def _get_performance_grade(self) -> str:
        """è·å–æ€§èƒ½ç­‰çº§"""
        score = self._calculate_performance_score()
        if score >= 80:
            return "EXCELLENT"
        elif score >= 60:
            return "GOOD"
        elif score >= 40:
            return "AVERAGE"
        else:
            return "POOR"
    
    def _compare_to_benchmark(self) -> Dict[str, float]:
        """ä¸åŸºå‡†å¯¹æ¯”"""
        # DipMasterç­–ç•¥åŸºå‡†æŒ‡æ ‡
        benchmark = {
            'win_rate': 0.55,
            'sharpe_ratio': 1.5,
            'profit_factor': 1.5,
            'max_drawdown': 0.15
        }
        
        return {
            'win_rate_vs_benchmark': self.win_rate - benchmark['win_rate'],
            'sharpe_vs_benchmark': self.sharpe_ratio - benchmark['sharpe_ratio'],
            'profit_factor_vs_benchmark': self.profit_factor - benchmark['profit_factor'],
            'drawdown_vs_benchmark': benchmark['max_drawdown'] - self.max_drawdown  # è¶Šå°è¶Šå¥½
        }


@dataclass
class SystemHealthEvent(BaseKafkaEvent):
    """ç³»ç»Ÿå¥åº·äº‹ä»¶ - Topic: system.health.v1"""
    component_name: str = ""
    health_score: float = 100.0
    status: str = "healthy"  # healthy, degraded, unhealthy
    cpu_usage_percent: float = 0.0
    memory_usage_percent: float = 0.0
    disk_usage_percent: float = 0.0
    network_latency_ms: float = 0.0
    api_response_time_ms: float = 0.0
    error_rate_percent: float = 0.0
    uptime_seconds: float = 0.0
    active_connections: int = 0
    
    def to_kafka_message(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºKafkaæ¶ˆæ¯æ ¼å¼"""
        message = super().to_kafka_message()
        message.update({
            'topic': KafkaTopics.SYSTEM_HEALTH.value,
            'key': f"{self.component_name}_{int(self.timestamp.timestamp())}",
            'health_assessment': {
                'resource_stress': self._calculate_resource_stress(),
                'performance_impact': self._assess_performance_impact(),
                'recommendations': self._generate_recommendations()
            }
        })
        return message
    
    def _calculate_resource_stress(self) -> float:
        """è®¡ç®—èµ„æºå‹åŠ›å€¼ (0-100)"""
        stress_factors = [
            self.cpu_usage_percent,
            self.memory_usage_percent,
            self.disk_usage_percent,
            min(self.error_rate_percent * 10, 100)  # é”™è¯¯ç‡æƒé‡æ›´é«˜
        ]
        return sum(stress_factors) / len(stress_factors)
    
    def _assess_performance_impact(self) -> str:
        """è¯„ä¼°æ€§èƒ½å½±å“"""
        stress = self._calculate_resource_stress()
        if stress < 50:
            return "MINIMAL"
        elif stress < 75:
            return "MODERATE"
        else:
            return "SEVERE"
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if self.cpu_usage_percent > 80:
            recommendations.append("Optimize CPU-intensive operations")
        if self.memory_usage_percent > 80:
            recommendations.append("Investigate memory leaks or increase memory allocation")
        if self.disk_usage_percent > 85:
            recommendations.append("Clean up old log files or increase disk space")
        if self.api_response_time_ms > 1000:
            recommendations.append("Optimize API response time or check network connectivity")
        if self.error_rate_percent > 5:
            recommendations.append("Investigate and fix recurring errors")
        
        return recommendations


@dataclass
class TradeSignalEvent(BaseKafkaEvent):
    """äº¤æ˜“ä¿¡å·äº‹ä»¶ - Topic: trade.signals.v1"""
    signal_id: str = ""
    strategy: str = "dipmaster"
    symbol: str = ""
    signal_type: str = ""  # BUY, SELL
    confidence: float = 0.0
    price: float = 0.0
    expected_entry_price: float = 0.0
    expected_holding_minutes: int = 60
    technical_indicators: Dict[str, float] = field(default_factory=dict)
    market_conditions: Dict[str, Any] = field(default_factory=dict)
    risk_metrics: Dict[str, float] = field(default_factory=dict)
    
    def to_kafka_message(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºKafkaæ¶ˆæ¯æ ¼å¼"""
        message = super().to_kafka_message()
        message.update({
            'topic': KafkaTopics.TRADE_SIGNALS.value,
            'key': f"{self.symbol}_{self.signal_id}",
            'signal_quality': {
                'strength_score': self._calculate_signal_strength(),
                'quality_grade': self._get_quality_grade(),
                'dipmaster_compliance': self._check_dipmaster_compliance()
            }
        })
        return message
    
    def _calculate_signal_strength(self) -> float:
        """è®¡ç®—ä¿¡å·å¼ºåº¦ (0-100)"""
        # åŸºäºç½®ä¿¡åº¦å’ŒæŠ€æœ¯æŒ‡æ ‡
        confidence_score = self.confidence * 60  # ç½®ä¿¡åº¦æƒé‡60%
        
        # æŠ€æœ¯æŒ‡æ ‡è¯„åˆ†
        rsi = self.technical_indicators.get('rsi', 50)
        ma20_distance = self.technical_indicators.get('ma20_distance', 0)
        volume_ratio = self.technical_indicators.get('volume_ratio', 1)
        
        # DipMasterç‰¹å®šçš„æŠ€æœ¯æŒ‡æ ‡è¯„åˆ†
        tech_score = 0
        if 30 <= rsi <= 50:  # RSIåœ¨ç›®æ ‡åŒºé—´
            tech_score += 20
        if ma20_distance < 0:  # ä»·æ ¼ä½äºMA20
            tech_score += 10
        if volume_ratio > 1.5:  # æˆäº¤é‡æ”¾å¤§
            tech_score += 10
        
        return min(100, confidence_score + tech_score)
    
    def _get_quality_grade(self) -> str:
        """è·å–ä¿¡å·è´¨é‡ç­‰çº§"""
        strength = self._calculate_signal_strength()
        if strength >= 80:
            return "EXCELLENT"
        elif strength >= 60:
            return "GOOD"
        elif strength >= 40:
            return "AVERAGE"
        else:
            return "POOR"
    
    def _check_dipmaster_compliance(self) -> Dict[str, bool]:
        """æ£€æŸ¥DipMasterç­–ç•¥åˆè§„æ€§"""
        rsi = self.technical_indicators.get('rsi', 50)
        ma20_distance = self.technical_indicators.get('ma20_distance', 0)
        volume_ratio = self.technical_indicators.get('volume_ratio', 1)
        
        return {
            'rsi_in_range': 30 <= rsi <= 50,
            'price_below_ma20': ma20_distance < 0,
            'volume_confirmation': volume_ratio > 1.5,
            'dip_buying': self.signal_type == 'BUY' and self.price < self.expected_entry_price
        }


@dataclass
class PositionUpdateEvent(BaseKafkaEvent):
    """æŒä»“æ›´æ–°äº‹ä»¶ - Topic: position.updates.v1"""
    position_id: str = ""
    signal_id: str = ""
    symbol: str = ""
    side: str = ""  # BUY, SELL
    quantity: float = 0.0
    entry_price: float = 0.0
    current_price: float = 0.0
    unrealized_pnl: float = 0.0
    realized_pnl: float = 0.0
    holding_time_minutes: int = 0
    status: str = "OPEN"  # OPEN, CLOSED, PARTIAL
    strategy: str = "dipmaster"
    
    def to_kafka_message(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºKafkaæ¶ˆæ¯æ ¼å¼"""
        message = super().to_kafka_message()
        message.update({
            'topic': KafkaTopics.POSITION_UPDATES.value,
            'key': f"{self.symbol}_{self.position_id}",
            'position_analysis': {
                'pnl_percentage': self._calculate_pnl_percentage(),
                'holding_efficiency': self._calculate_holding_efficiency(),
                'exit_timing_score': self._assess_exit_timing()
            }
        })
        return message
    
    def _calculate_pnl_percentage(self) -> float:
        """è®¡ç®—P&Lç™¾åˆ†æ¯”"""
        if self.entry_price == 0:
            return 0.0
        
        if self.status == "CLOSED" and self.realized_pnl != 0:
            return (self.realized_pnl / (self.entry_price * self.quantity)) * 100
        else:
            return ((self.current_price - self.entry_price) / self.entry_price) * 100
    
    def _calculate_holding_efficiency(self) -> float:
        """è®¡ç®—æŒä»“æ•ˆç‡è¯„åˆ† (0-100)"""
        # DipMasterç›®æ ‡æŒä»“æ—¶é—´ä¸º60-120åˆ†é’Ÿ
        target_min, target_max = 60, 120
        
        if target_min <= self.holding_time_minutes <= target_max:
            return 100.0
        elif self.holding_time_minutes < target_min:
            # æŒä»“æ—¶é—´è¿‡çŸ­
            return max(0, 100 - (target_min - self.holding_time_minutes) * 2)
        else:
            # æŒä»“æ—¶é—´è¿‡é•¿
            return max(0, 100 - (self.holding_time_minutes - target_max) * 0.5)
    
    def _assess_exit_timing(self) -> float:
        """è¯„ä¼°å‡ºåœºæ—¶æœºè¯„åˆ† (0-100)"""
        pnl_pct = self._calculate_pnl_percentage()
        
        # DipMasterç›®æ ‡åˆ©æ¶¦ä¸º0.8%
        target_profit = 0.8
        
        if pnl_pct >= target_profit:
            return 100.0
        elif pnl_pct > 0:
            return (pnl_pct / target_profit) * 100
        else:
            # äºæŸæƒ…å†µä¸‹çš„æ—¶é—´æ­¢æŸè¯„åˆ†
            if self.holding_time_minutes >= 180:  # æœ€å¤§æŒä»“æ—¶é—´
                return 60.0  # åŠæ—¶æ­¢æŸ
            else:
                return max(0, 60 - abs(pnl_pct) * 10)


class KafkaEventProducer:
    """Kafkaäº‹ä»¶ç”Ÿäº§è€…"""
    
    def __init__(self, kafka_config: Dict[str, Any] = None):
        self.kafka_config = kafka_config or {}
        self.producer = None
        self.is_connected = False
        self.event_buffer: List[Dict[str, Any]] = []
        self.buffer_max_size = self.kafka_config.get('buffer_max_size', 1000)
        self.stats = {
            'events_published': 0,
            'events_failed': 0,
            'events_buffered': 0
        }
    
    async def connect(self) -> bool:
        """è¿æ¥åˆ°Kafka"""
        try:
            # è¿™é‡Œåº”è¯¥åˆå§‹åŒ–çœŸå®çš„Kafka producer
            # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡æ‹Ÿè¿æ¥
            self.is_connected = True
            logger.info("ğŸ“¤ Kafka producer connected")
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to connect to Kafka: {e}")
            return False
    
    async def disconnect(self) -> None:
        """æ–­å¼€Kafkaè¿æ¥"""
        try:
            if self.producer:
                # è¿™é‡Œåº”è¯¥å…³é—­çœŸå®çš„Kafka producer
                pass
            self.is_connected = False
            logger.info("ğŸ“¤ Kafka producer disconnected")
        except Exception as e:
            logger.error(f"âŒ Failed to disconnect from Kafka: {e}")
    
    async def publish_event(self, event: BaseKafkaEvent) -> bool:
        """å‘å¸ƒäº‹ä»¶åˆ°Kafka"""
        try:
            message = event.to_kafka_message()
            
            if self.is_connected:
                # è¿™é‡Œåº”è¯¥å‘é€åˆ°çœŸå®çš„Kafka
                # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åªæ˜¯è®°å½•æ—¥å¿—
                logger.debug(f"ğŸ“¤ Published event to {message.get('topic')}: {message.get('key')}")
                self.stats['events_published'] += 1
                return True
            else:
                # è¿æ¥æ–­å¼€æ—¶ç¼“å­˜äº‹ä»¶
                await self._buffer_event(message)
                return False
                
        except Exception as e:
            logger.error(f"âŒ Failed to publish event: {e}")
            self.stats['events_failed'] += 1
            return False
    
    async def _buffer_event(self, message: Dict[str, Any]) -> None:
        """ç¼“å­˜äº‹ä»¶åˆ°æœ¬åœ°ç¼“å†²åŒº"""
        if len(self.event_buffer) >= self.buffer_max_size:
            # ç§»é™¤æœ€è€çš„äº‹ä»¶
            self.event_buffer.pop(0)
        
        self.event_buffer.append(message)
        self.stats['events_buffered'] += 1
        logger.debug(f"ğŸ“¦ Buffered event, buffer size: {len(self.event_buffer)}")
    
    async def flush_buffer(self) -> int:
        """åˆ·æ–°ç¼“å†²åŒºä¸­çš„äº‹ä»¶"""
        if not self.is_connected or not self.event_buffer:
            return 0
        
        flushed_count = 0
        buffer_copy = self.event_buffer.copy()
        self.event_buffer.clear()
        
        for message in buffer_copy:
            try:
                # è¿™é‡Œåº”è¯¥å‘é€åˆ°çœŸå®çš„Kafka
                logger.debug(f"ğŸ“¤ Flushed buffered event to {message.get('topic')}")
                flushed_count += 1
                self.stats['events_published'] += 1
            except Exception as e:
                logger.error(f"âŒ Failed to flush buffered event: {e}")
                # é‡æ–°åŠ å…¥ç¼“å†²åŒº
                await self._buffer_event(message)
        
        logger.info(f"ğŸ“¤ Flushed {flushed_count} buffered events")
        return flushed_count
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç”Ÿäº§è€…ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'is_connected': self.is_connected,
            'buffer_size': len(self.event_buffer),
            'buffer_max_size': self.buffer_max_size,
            **self.stats
        }


class DipMasterKafkaStreamer:
    """DipMaster Kafkaäº‹ä»¶æµç”Ÿäº§å™¨"""
    
    def __init__(self, kafka_config: Dict[str, Any] = None):
        self.kafka_config = kafka_config or {
            'servers': ['localhost:9092'],
            'client_id': 'dipmaster-events'
        }
        self.producer = KafkaEventProducer(kafka_config)
        self.is_running = False
        self.event_stats = {topic.value: 0 for topic in KafkaTopics}
    
    async def start(self) -> None:
        """å¯åŠ¨äº‹ä»¶æµç”Ÿäº§å™¨"""
        if self.is_running:
            return
        
        self.is_running = True
        await self.producer.connect()
        logger.info("ğŸš€ DipMaster Kafka Streamer started")
    
    async def stop(self) -> None:
        """åœæ­¢äº‹ä»¶æµç”Ÿäº§å™¨"""
        self.is_running = False
        await self.producer.disconnect()
        logger.info("ğŸ›‘ DipMaster Kafka Streamer stopped")
    
    # ä¾¿æ·æ–¹æ³•ç”¨äºå‘å¸ƒç‰¹å®šç±»å‹çš„äº‹ä»¶
    
    async def publish_execution_report(
        self,
        execution_id: str,
        symbol: str,
        side: str,
        quantity: float,
        price: float,
        **kwargs
    ) -> bool:
        """å‘å¸ƒäº¤æ˜“æ‰§è¡ŒæŠ¥å‘Š"""
        event = ExecutionReportEvent(
            execution_id=execution_id,
            symbol=symbol,
            side=side,
            quantity=quantity,
            price=price,
            **kwargs
        )
        
        success = await self.producer.publish_event(event)
        if success:
            self.event_stats[KafkaTopics.EXECUTION_REPORTS.value] += 1
        return success
    
    async def publish_risk_metrics(
        self,
        portfolio_id: str = "main_portfolio",
        var_95: float = 0.0,
        var_99: float = 0.0,
        **kwargs
    ) -> bool:
        """å‘å¸ƒé£é™©æŒ‡æ ‡"""
        event = RiskMetricsEvent(
            portfolio_id=portfolio_id,
            var_95=var_95,
            var_99=var_99,
            **kwargs
        )
        
        success = await self.producer.publish_event(event)
        if success:
            self.event_stats[KafkaTopics.RISK_METRICS.value] += 1
        return success
    
    async def publish_alert(
        self,
        alert_id: str,
        severity: str,
        category: str,
        message: str,
        **kwargs
    ) -> bool:
        """å‘å¸ƒå‘Šè­¦äº‹ä»¶"""
        event = AlertEvent(
            alert_id=alert_id,
            severity=severity,
            category=category,
            message=message,
            **kwargs
        )
        
        success = await self.producer.publish_event(event)
        if success:
            self.event_stats[KafkaTopics.ALERTS.value] += 1
        return success
    
    async def publish_strategy_performance(
        self,
        strategy_name: str = "dipmaster",
        time_window: str = "1h",
        win_rate: float = 0.0,
        **kwargs
    ) -> bool:
        """å‘å¸ƒç­–ç•¥æ€§èƒ½æŒ‡æ ‡"""
        event = StrategyPerformanceEvent(
            strategy_name=strategy_name,
            time_window=time_window,
            win_rate=win_rate,
            **kwargs
        )
        
        success = await self.producer.publish_event(event)
        if success:
            self.event_stats[KafkaTopics.STRATEGY_PERFORMANCE.value] += 1
        return success
    
    async def publish_system_health(
        self,
        component_name: str,
        health_score: float,
        status: str = "healthy",
        **kwargs
    ) -> bool:
        """å‘å¸ƒç³»ç»Ÿå¥åº·æŒ‡æ ‡"""
        event = SystemHealthEvent(
            component_name=component_name,
            health_score=health_score,
            status=status,
            **kwargs
        )
        
        success = await self.producer.publish_event(event)
        if success:
            self.event_stats[KafkaTopics.SYSTEM_HEALTH.value] += 1
        return success
    
    async def publish_trade_signal(
        self,
        signal_id: str,
        symbol: str,
        signal_type: str,
        confidence: float,
        price: float,
        **kwargs
    ) -> bool:
        """å‘å¸ƒäº¤æ˜“ä¿¡å·"""
        event = TradeSignalEvent(
            signal_id=signal_id,
            symbol=symbol,
            signal_type=signal_type,
            confidence=confidence,
            price=price,
            **kwargs
        )
        
        success = await self.producer.publish_event(event)
        if success:
            self.event_stats[KafkaTopics.TRADE_SIGNALS.value] += 1
        return success
    
    async def publish_position_update(
        self,
        position_id: str,
        symbol: str,
        side: str,
        quantity: float,
        entry_price: float,
        current_price: float,
        **kwargs
    ) -> bool:
        """å‘å¸ƒæŒä»“æ›´æ–°"""
        event = PositionUpdateEvent(
            position_id=position_id,
            symbol=symbol,
            side=side,
            quantity=quantity,
            entry_price=entry_price,
            current_price=current_price,
            **kwargs
        )
        
        success = await self.producer.publish_event(event)
        if success:
            self.event_stats[KafkaTopics.POSITION_UPDATES.value] += 1
        return success
    
    def get_event_stats(self) -> Dict[str, Any]:
        """è·å–äº‹ä»¶ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'event_counts_by_topic': self.event_stats,
            'total_events': sum(self.event_stats.values()),
            'producer_stats': self.producer.get_stats(),
            'is_running': self.is_running
        }


# å·¥å‚å‡½æ•°
def create_kafka_streamer(config: Dict[str, Any] = None) -> DipMasterKafkaStreamer:
    """åˆ›å»ºKafkaäº‹ä»¶æµç”Ÿäº§å™¨"""
    return DipMasterKafkaStreamer(config)


# æ¼”ç¤ºå‡½æ•°
async def kafka_events_demo():
    """Kafkaäº‹ä»¶æµæ¼”ç¤º"""
    print("ğŸš€ DipMaster Kafka Event Streams Demo")
    
    # åˆ›å»ºKafkaæµç”Ÿäº§å™¨
    streamer = create_kafka_streamer({
        'servers': ['localhost:9092'],
        'client_id': 'dipmaster-demo'
    })
    
    try:
        # å¯åŠ¨æµç”Ÿäº§å™¨
        await streamer.start()
        print("âœ… Kafka streamer started")
        
        # å‘å¸ƒäº¤æ˜“ä¿¡å·äº‹ä»¶
        await streamer.publish_trade_signal(
            signal_id="sig_demo_001",
            symbol="BTCUSDT",
            signal_type="BUY",
            confidence=0.87,
            price=43250.50,
            technical_indicators={
                'rsi': 34.2,
                'ma20_distance': -0.008,
                'volume_ratio': 1.6
            }
        )
        print("ğŸ“Š Published trade signal event")
        
        # å‘å¸ƒæ‰§è¡ŒæŠ¥å‘Šäº‹ä»¶
        await streamer.publish_execution_report(
            execution_id="exec_demo_001",
            symbol="BTCUSDT",
            side="BUY",
            quantity=0.15,
            price=43245.00,
            slippage_bps=1.3,
            latency_ms=42,
            signal_id="sig_demo_001"
        )
        print("âš¡ Published execution report event")
        
        # å‘å¸ƒé£é™©æŒ‡æ ‡äº‹ä»¶
        await streamer.publish_risk_metrics(
            var_95=125000.50,
            var_99=187500.75,
            expected_shortfall=210000.00,
            sharpe_ratio=1.85,
            max_drawdown=0.082,
            leverage=1.5
        )
        print("ğŸ“ˆ Published risk metrics event")
        
        # å‘å¸ƒå‘Šè­¦äº‹ä»¶
        await streamer.publish_alert(
            alert_id="alert_demo_001",
            severity="WARNING",
            category="RISK_LIMIT",
            message="Portfolio exposure approaching 80% of limit",
            affected_systems=["portfolio_manager", "risk_engine"],
            recommended_action="Consider reducing position sizes"
        )
        print("ğŸš¨ Published alert event")
        
        # å‘å¸ƒç³»ç»Ÿå¥åº·äº‹ä»¶
        await streamer.publish_system_health(
            component_name="trading_engine",
            health_score=95.5,
            status="healthy",
            cpu_usage_percent=45.2,
            memory_usage_percent=62.8,
            api_response_time_ms=125
        )
        print("ğŸ’Š Published system health event")
        
        # å‘å¸ƒç­–ç•¥æ€§èƒ½äº‹ä»¶
        await streamer.publish_strategy_performance(
            strategy_name="dipmaster",
            time_window="1h",
            win_rate=0.823,
            profit_factor=2.1,
            sharpe_ratio=1.85,
            total_trades=15,
            winning_trades=12,
            total_pnl=287.50
        )
        print("ğŸ¯ Published strategy performance event")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = streamer.get_event_stats()
        print(f"ğŸ“Š Event statistics:")
        for topic, count in stats['event_counts_by_topic'].items():
            if count > 0:
                print(f"  {topic}: {count} events")
        print(f"ğŸ“Š Total events published: {stats['total_events']}")
        
        print("âœ… Demo completed successfully")
        
    except Exception as e:
        print(f"âŒ Demo failed: {e}")
    
    finally:
        await streamer.stop()
        print("ğŸ›‘ Kafka streamer stopped")


if __name__ == "__main__":
    asyncio.run(kafka_events_demo())